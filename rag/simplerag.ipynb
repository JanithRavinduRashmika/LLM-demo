{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 1\n",
      "Python-dotenv could not parse statement starting at line 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"What is Retrieval-Augmented Generation?\\nRetrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts.\", metadata={'source': 'speech.txt'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text based loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text_document = loader.load()\n",
    "text_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Web based loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nEmadeldeen Eldele1Mohamed Ragab1 2Zhenghua Chen1 2Min Wu2Xiaoli Li1 2\\nAbstract\\nTime series data, characterized by its intrin-\\nsic long and short-range dependencies, poses a\\nunique challenge across analytical applications.\\nWhile Transformer-based models excel at cap-\\nturing long-range dependencies, they face limita-\\ntions in noise sensitivity, computational efficiency,\\nand overfitting with smaller datasets. In response,\\nwe introduce a novel TimeSeries Lightweight\\nAdaptive Network ( TSLANet ), as a universal\\nconvolutional model for diverse time series tasks.\\nSpecifically, we propose an Adaptive Spectral\\nBlock, harnessing Fourier analysis to enhance\\nfeature representation and to capture both long-\\nterm and short-term interactions while mitigat-\\ning noise via adaptive thresholding. Additionally,\\nwe introduce an Interactive Convolution Block\\nand leverage self-supervised learning to refine\\nthe capacity of TSLANet for decoding complex\\ntemporal patterns and improve its robustness on\\ndifferent datasets. Our comprehensive experi-\\nments demonstrate that TSLANet outperforms\\nstate-of-the-art models in various tasks spanning\\nclassification, forecasting, and anomaly detection,\\nshowcasing its resilience and adaptability across\\na spectrum of noise levels and data sizes. The\\ncode is available at https://github.com/\\nemadeldeen24/TSLANet\\n1. Introduction\\nTime series data, known for its sequential nature and tempo-\\nral dependencies, is ubiquitous across numerous domains,\\nincluding finance, healthcare, and environmental monitor-\\ning. Recently, the Transformer model (Vaswani et al., 2017),\\noriginally renowned for its breakthroughs in natural lan-\\nguage processing, has been adapted as a potent tool for\\nanalyzing time series data. This was motivated by its abil-\\n1Centre for Frontier AI Research, Agency for Science, Tech-\\nnology and Research, Singapore2I2R, Agency for Science, Tech-\\nnology and Research, Singapore. Correspondence to: Emadeldeen\\nEldele <emad0002@ntu.edu.sg >.\\nClassifcation Accuracy\\n Forecasting MSE (Weather)\\n Forecasting MSE (ETTh1)\\n0.10.20.30.40.50.60.7CNN\\nFEDformerPatchTST\\nCrossformerAutoformerFigure 1: A comparison between CNN and Transformer-\\nbased architectures for classification and forecasting tasks.\\nClassification results are the average over 10 UEA datasets\\n(Wu et al., 2023), while forecasting results are the average\\nMSE results on lengths {96, 192, 336, 720 }.\\nity to capture long-range dependencies and interactions\\nwithin time series data, showing proficiency in forecasting\\ntasks (Wu et al., 2021b; Zhou et al., 2022; Liu et al., 2024).\\nDespite the initial success of Transformers in time series\\nforecasting, they encounter hurdles when deployed across\\ndiverse time series tasks, particularly those with smaller\\ndatasets. This can be attributed to its large parameter size,\\nwhich may lead to overfitting and computational inefficiency\\nproblems (Wen et al., 2023). In addition, their attention\\nmechanism often struggles with the inherent noise and re-\\ndundancy in time series data (Li et al., 2022). Moreover, re-\\ncent works have questioned their adaptability, as highlighted\\nby (Zeng et al., 2023; Li et al., 2023). They observed that the\\nself-attention within Transformers is inherently permutation-\\ninvariant, which compromises the preservation of temporal\\ninformation. Their experiments showed that a single linear\\nlayer surprisingly outperforms the complex Transformer\\narchitectures for time series forecasting. However, while\\nsuch linear models can perform well for small, clean data,\\nthey may not be able to handle complex, noisy time series.\\nIn this work, we pivot from the prevalent focus on Multi-\\nLayer Perceptrons (MLPs) and Transformers to tackle the\\npotential of convolutional operations for time series analysis.\\nConvolutional Neural Networks (CNNs) have traditionally\\nexcelled in capturing short-term patterns within time series\\ndue to their local receptive fields, which serve as a strength\\n1arXiv:2404.08472v1  [cs.LG]  12 Apr 2024', metadata={'source': 'rpaper.pdf', 'page': 0}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nin classification tasks. Indeed, as illustrated in Figure 1, a\\nstraightforward 3-layer CNN network demonstrates supe-\\nrior performance in classification compared to state-of-the-\\nart Transformer-based architectures. Yet, our experiment\\nshowed that the efficacy of CNNs in forecasting varies with\\nthe data frequency. For instance, the CNN shows compet-\\nitive performance to these Transformer-based models on\\nthe Weather dataset featuring a short 10-minute frequency\\nbut struggles with the longer hourly ETTh1 dataset, indicat-\\ning a difficulty with less frequent temporal changes. This\\ndiscrepancy highlights a critical question: How can we en-\\nhance CNNs to extend their robust performance across a\\nwider spectrum of time series tasks? It becomes obvious\\nthat expanding the capabilities of CNNs can be achieved by\\nlearning both short-term and long-term dependencies within\\ntime series data.\\nTo this end, we introduce TimeSeries Lightweight Adaptive\\nNetwork ( TSLANet ), a universal architecture for various\\ntime series tasks. TSLANet inherits the multi-block design\\nof the Transformer to allow scalability. However, we re-\\nplace the computationally expensive self-attention with a\\nlightweight Adaptive Spectral Block (ASB) featuring two\\nkey objectives. Firstly, ASB aims to encompass the entire\\nfrequency spectrum, thereby adeptly capturing both long-\\nterm and short-term interactions within the data. This is\\nachieved via Fourier-based multiplications by global and\\nlocal filters, akin to circular convolutions. Secondly, ASB se-\\nlectively attenuates high frequencies via an adaptive thresh-\\nolding approach, a strategy aimed at minimizing noise and\\nenhancing the clarity of the signal. In addition, we further\\nadvance our model by replacing the standard feed-forward\\nnetwork with an Interactive Convolutional Block, where\\nCNNs with different kernel sizes control each other to en-\\nrich the ability of the model to capture and interpret complex\\ntemporal patterns. Finally, we employ a per-dataset self-\\nsupervised pretraining to enhance the model capabilities,\\nespecially on large datasets.\\nThe proposed model is lightweight and enjoys the\\nO(NlogN)complexity of the Fast Fourier Transform\\n(FFT) operations, demonstrating superior efficiency and\\nspeed compared to self-attention (see Section 5.4). A\\nsummary comparison against CNN-based and Transformer-\\nbased models is also provided in Table 1. The contributions\\nof this paper can be summarized as follows:\\n•We propose a universal lightweight model ( TSLANet ),\\ndesigned to adapt seamlessly to a myriad of time series\\ntasks. Through computationally efficient convolution\\noperations, TSLANet learns both long- and short-term\\nrelationships within the data.\\n•We propose an Adaptive Spectral Block, which lever-\\nages the power of Fourier transform alongside global\\nand local filters to cover the whole frequency spectrum,Table 1: Comparison to different methods. ‘Local Depen-\\ndencies’ means the efficiency in capturing local features.\\nMethod Feature ExtractionLong-range\\nDependenciesLocal\\nDependenciesParameter\\nEfficiency\\nCNN Localized Convolution ✗ ✓ ✓\\nTransformer Self-Attention ✓ ✗ ✗\\nTSLANet Adaptive Spectral Convolution ✓ ✓ ✓\\nwhile adaptively removing high frequencies that tend\\nto introduce noises. In addition, we propose an Inter-\\nactive Convolution Block to learn intricate spatial and\\ntemporal features within data.\\n•TSLANet demonstrates superior performance against\\ndifferent state-of-the-art methods across various time\\nseries tasks.\\n2. Related Works\\nTransformer-based Networks. Since the advance of the\\nTransformer (Vaswani et al., 2017) for natural language\\nprocessing, numerous works have adopted it for time series\\nanalysis. For example, (Wu et al., 2021b; Zhou et al., 2022;\\nLi et al., 2021; Kitaev et al., 2020; Zhang & Yan, 2023) have\\nshowcased the Transformer capability to model interactions\\nwithin time series data, utilizing that for the forecasting task.\\nIn addition, Transformers with special design showed good\\nperformance in anomaly detection task (Xu et al., 2022).\\nYet, the efficacy of Transformers for time series has been\\ncontested. For instance, Zeng et al. (2023) argue that the\\npermutation-invariance property in Transformers may lead\\nto the loss of temporal information in time series. Following\\nthat, other MLP-based architectures showed efficacy in the\\ntime series forecasting task (Li et al., 2023; Ekambaram\\net al., 2023). Furthermore, Transformers demand extensive\\ncomputational resources in general, and they are prone to\\noverfitting when trained on smaller datasets (Wen et al.,\\n2023).\\nConvolution-based Networks. CNNs have showcased\\ntheir efficacy in time series analysis, particularly shining in\\nclassification tasks due to their adeptness at learning local\\npatterns (Dempster et al., 2020). CNNs also serve as the\\nbackbone for several time series representation learning\\nmethods, including TS-TCC (Eldele et al., 2021), TS2VEC\\n(Yue et al., 2022), and MHCCL (Meng et al., 2023).\\nDespite their promise, CNNs often face challenges in fore-\\ncasting and anomaly detection, primarily due to their lim-\\nited ability to capture long-range dependencies. Therefore,\\nrecent works attempt to enhance CNN capabilities in dif-\\nferent ways. For instance, T-WaveNet (LIU et al., 2022)\\nleverages frequency spectrum energy analysis for effective\\nsignal decomposition, SCINet (Liu et al., 2022) adopts a\\nrecursive downsample-convolve-interact strategy to model\\n2', metadata={'source': 'rpaper.pdf', 'page': 1}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nPatch \\n+ \\nPositional\\nEmbeddings\\nLayer \\nNorm\\nLayer \\nNorm\\nAdaptive \\nSpectral \\nBlock\\nInteractive\\nConvolutional\\nBlock\\nFFT\\nIFFT\\n1\\n2\\n3\\n4\\n5\\nxN\\nLocal \\nWeights\\nGlobal \\nWeights\\nConv \\n1D\\nConv \\n1D\\nConv \\n1D\\nLinear \\nLayer\\nAdaptive \\nMasking\\nGELU\\nGELU\\nFigure 2: The structure of our proposed TSLANet . The input time series is split into patches, and positional embeddings\\nare added. Next, the output embeddings pass through TSLANet layers, where each layer consists of two main components.\\nThe first is the Adaptive Spectral Block, which leverages frequency domain representations for robust feature extraction and\\nemploys adaptive thresholding to mitigate noise. The second is the Interactive Convolution Block, which captures complex\\ntemporal patterns through convolutional operations.\\ncomplex temporal dynamics, and WFTNet (Liu et al., 2023)\\nemploys a combination of Fourier and wavelet transforms\\nfor a thorough temporal-frequency analysis. Additionally,\\nTCE (Zhang et al., 2023) targets the improvement of 1D-\\nCNNs by addressing the disturbing convolution for better\\nlow-frequency component focus, and BTSF (Yang & Hong,\\n2022) introduces a bilinear temporal-spectral fusion tech-\\nnique for unsupervised learning, emphasizing the impor-\\ntance of maintaining the global context of time series data.\\nA noteworthy attempt to leverage CNNs for multiple time\\nseries tasks is the TimesNet model (Wu et al., 2023), which\\ncapitalizes on multi-periodicity to merge intraperiod and\\ninterperiod variations within a 2D space, enhancing the\\nrepresentation of temporal patterns. However, TimesNet\\nmay not fully address the challenges presented by non-\\nstationary datasets lacking clear periodicity. Some recent\\nworks have explored combining CNNs with Transformers to\\nharness both their strengths (Li et al., 2022; Wu et al., 2021a;\\nD’Ascoli et al., 2021), though such hybrid approaches re-\\nmain underexplored in time series analysis compared to\\ntheir applications in computer vision.\\nOur work takes a distinct path by proposing a universal\\nconvolutional-based architecture, adept at handling various\\ntime series tasks through adaptive spectral feature extrac-\\ntion. This approach not only utilizes the strong local featurelearning capabilities of CNNs but also efficiently captures\\nglobal temporal patterns, offering a balanced solution for\\nboth local and long-range dependencies in time series data.\\n3. Method\\n3.1. Preliminaries: Discrete Fourier Transform\\nWe first explore the Discrete Fourier Transform (DFT) as\\nit is a cornerstone in our framework. Consider a series of\\nNcomplex numbers x[n], where 0≤n≤N−1. The\\n1D DFT transforms this series into a frequency domain\\nrepresentation:\\nX[k] =N−1X\\nn=0x[n]e−j(2π/N )kn:=N−1X\\nn=0x[n]Wkn\\nN,(1)\\nwhere jdenotes the imaginary unit, with WN=e−j(2π/N ).\\nThis formulation is derived from the continuous Fourier\\ntransform by discretizing in both time and frequency do-\\nmains. The spectrum of the sequence x[n]at frequency\\nωk= 2πk/N is represented by X[k], which is periodic\\nwith an interval of length N, thus only the first Npoints are\\nconsidered.\\nDue to the bijective nature of DFT, the original sequence\\n3', metadata={'source': 'rpaper.pdf', 'page': 2}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nx[n]is retrievable via the Inverse DFT (IDFT):\\nx[n] =1\\nNN−1X\\nk=0X[k]ej(2π/N )kn. (2)\\nFor real-valued x[n], DFT exhibits conjugate symmetry,\\ni.e.,X[N−k] =X∗[k]. This symmetry is pivotal, as\\nperforming IDFT on a conjugate symmetric X[k]results in\\na real discrete signal. Half of the DFT spectrum, specifically\\nX[k] : 0≤k≤ ⌈N/2⌉, sufficiently describes the frequency\\ncharacteristics of x[n].\\nThe choice of DFT in TSLANet is motivated by two factors:\\nits discrete nature aligns well with digital processing and\\nthe existence of efficient computation methods. The Fast\\nFourier Transform (FFT), leveraging the symmetry and pe-\\nriodicity of Wkn\\nN, optimizes DFT computation from O(N2)\\ntoO(NlogN). The IDFT, paralleling DFT’s form, benefits\\nsimilarly from the Inverse FFT (IFFT).\\n3.2. Overall Architecture\\nOur model integrates two novel components, i.e., the Adap-\\ntive Spectral Block (ASB) and the Interactive Convolution\\nBlock (ICB), as depicted in Figure 2. These two components\\nform a single layer that could be extended to multiple layers.\\nThe ASB employs Fourier analysis to transform time series\\ndata into the frequency domain, in which we apply adaptive\\nthresholding to attenuate high-frequency noise and highlight\\nrelevant spectral features. After processing, the IFFT recon-\\nstructs the time-domain features, now with reduced noise\\nand enhanced representations. The ICB is a streamlined\\nconvolutional block that interactively refines features using\\ndifferent kernel sizes, improving adaptability to temporal\\ndynamics in time series. Together, these components form a\\ncohesive structure that balances local and global temporal\\nfeature extraction for time series analysis.\\n3.3. Embedding Layer\\nGiven an input time series S, with each signal S∈RC×L\\nhaving Cchannels and a sequence length L. First, the sig-\\nnalSis divided into a set of Mpatches {P1, P2, ..., P M},\\nwhere each patch Picaptures a segment of S. The dimen-\\nsion of each patch is determined by the predefined patch\\nsizep, such that each patch Pi∈RC×p.\\nEach patch is then mapped into another dimension p′, i.e.,\\nPi→P′\\ni∈RC×p′. Next, the positional embeddings are\\nadded to each patch to retain the temporal ordering dis-\\nrupted during the segmentation process. The positional\\nembedding for the i-th patch is denoted as Ei, a vector that\\naligns dimensionally with the patch. The augmented patch\\nresults from adding both inputs, i.e., SPEi=P′\\ni+Ei, and\\nSPE={SPE1, SPE2, . . . S PEM}. Notably, the positional\\nembeddings are learnable parameters, allowing the modelto capture the temporal relationships within the time series\\ndata effectively.\\n3.4. Adaptive Spectral Block\\nWe propose the Adaptive Spectral Block (ASB) that em-\\nploys the Fourier-domain processing, as inspired by (Rao\\net al., 2021). This block aims to learn spatial informa-\\ntion with the global circular convolution operations. More-\\nover, it provides adaptive local filters to isolate noisy high-\\nfrequency components for any time series data.\\nFast Fourier Transformations. Given a discrete time\\nseries x[n], we obtain its frequency domain representation\\nX[k], by performing FFT along the spatial dimensions as\\nin Equation 1. Similarly, given SPE, its representation is\\ncalculated as:\\nF=F[SPE]∈CC×L′, (3)\\nwhereF[·]denotes the 1D FFT operation, and L′is the trans-\\nformed sequence length in the frequency domain, which\\nmay differ from Ldepending on the FFT implementation\\nand the nature of the time series data. Each channel of\\nthe time series is independently transformed, resulting in\\na comprehensive frequency domain representation Fthat\\nencapsulates the spectral characteristics of the original time\\nseries across all channels.\\nAdaptive Removal of High-Frequency Noise. High-\\nfrequency components often represent rapid fluctuations\\nthat deviate from the underlying trend or signal of interest,\\nmaking them appear more random and difficult to interpret\\n(Rhif et al., 2019). Therefore, we propose an adaptive lo-\\ncal filter that allows the model to dynamically adjust the\\nlevel of filtering according to the dataset characteristics and\\nremove these high-frequency noisy components. This is\\ncrucial when dealing with non-stationary data, where the\\nfrequency spectrum may change over time. The proposed\\nfilter adaptively sets the appropriate frequency threshold for\\neach specific time series data.\\nGiven the frequency domain representation Fobtained from\\nthe FFT operation, we first calculate the power spectrum of\\nF, which helps in identifying dominant frequency compo-\\nnents. The power spectrum Pis computed as the square\\nof the magnitude of the frequency components: P=|F|2,\\nwhich gives us a measure of the strength of different fre-\\nquencies in the time series data.\\nThe key to effective noise reduction lies in adaptively filter-\\ning high-frequency components from the power spectrum\\nP. We achieve this with a trainable threshold θ, which ad-\\njusts based on the spectral characteristics of the data. This\\nthreshold θis set as a learnable parameter optimized during\\ntraining through backpropagation, specifically∂L\\n∂θ, enabling\\n4', metadata={'source': 'rpaper.pdf', 'page': 3}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nθto discern between essential signal frequencies and noise.\\nWe formulate this adaptive thresholding as follows:\\nFfiltered =F⊙(P> θ), (4)\\nwhere⊙represents element-wise multiplication, and (P>\\nθ)is a binary mask where frequencies with power above the\\nthreshold θare retained, and others are filtered out.\\nThe adaptability of the threshold θensures that the ASB\\ncan efficiently remove high frequencies while preserving\\ncrucial information. By adaptively selecting the frequency\\nthreshold, the ASB tailors its filtering process to each spe-\\ncific time series dataset, enhancing the overall effectiveness\\nof the model in handling a wide range of data scenarios.\\nLearnable Filters. After adaptively filtering the frequency\\ndomain data, the model employs two sets of learnable filters;\\na global filter to learn from the original frequency domain\\ndataFand a local filter to learn from the adaptively filtered\\ndataFfiltered . LetWGandWLbe the learnable global and\\nlocal filters, respectively. The application of these filters is\\nrepresented as:\\nFG=WG⊙F, (5)\\nFL=WL⊙Ffiltered. (6)\\nNext, we integrate these filtered features to capture a com-\\nprehensive spectral detail, i.e., Fintegrated =FG+FL.\\nNotably, the multiplication operations in Equations 5 and\\n6 are equivalent to the circular convolution process (see\\nAppendix A). Circular convolution, with its larger recep-\\ntive field over the entire sequence, is particularly adept at\\ncapturing periodic patterns in time series data.\\nInverse Fourier Transform. To convert the integrated\\nfrequency domain data back to the time domain, we apply\\nthe Inverse Fast Fourier Transform (IFFT). The resulting\\ntime-domain signal S′is given by:\\nS′=F−1[Fintegrated ]∈RC×p′. (7)\\nThe IFFT ensures that the enhanced features align with the\\noriginal data structure of the input time series. The full\\noperation of the ASB is described in Algorithm 1 in the\\nAppendix.\\n3.5. Interactive Convolution Block\\nAfter enhancing feature representation by the ASB, we pro-\\npose the Interactive Convolution Block (ICB), which utilizes\\na dual-layer convolutional structure, as shown in Figure 2.\\nThe design of the ICB includes parallel convolutions with\\ndifferent kernel sizes to capture local features and longer-\\nrange dependencies. Specifically, the first convolutionallayer is designed to capture fine-grained, localized patterns\\nin the data with a smaller kernel. In contrast, the second\\nlayer aims to identify broader, longer-range dependencies\\nwith a larger kernel. We design the ICB such that the output\\nof each layer modulates the feature extraction of the other.\\nThe element-wise multiplication encourages interactions\\nbetween features extracted at different scales, potentially\\nleading to better modeling of complex relationships.\\nGiven the output of the IFFT operation S′, it serves as the\\ninput to the ICB. The process within the ICB is as follows:\\nA1=ϕ(Conv1 (S′))⊙Conv2 (S′), (8)\\nA2=ϕ(Conv2 (S′))⊙Conv1 (S′), (9)\\nwhere Conv1 (·)andConv2 (·)are two 1D-convolutional\\nlayers and ϕis the GELU activation function.\\nThe activated features are then added and passed through a\\nfinal convolutional layer Conv3 (·):\\nOICB=Conv3 (A1+A2). (10)\\nThe output OICBrepresents the enhanced features ready for\\nthe final layer in the network, represented by a customizable\\nlinear layer according to the task.\\n3.6. Self-Supervised Pretraining\\nExpanding the capabilities of TSLANet , we incorporate a\\nphase of self-supervised pretraining, which has garnered\\nsignificant attention for its efficacy in learning high-level\\nrepresentations from unlabeled data (Nie et al., 2023). Draw-\\ning inspiration from methodologies applied in natural lan-\\nguage processing and computer vision, we adopt a masked\\nautoencoder paradigm for time series data (He et al., 2022).\\nOur implementation involves selective masking of input\\nsequence patches, followed by training TSLANet to recon-\\nstruct these masked segments accurately. The masked data\\nthen serves as the training input, compelling the model to\\nlearn and infer the underlying patterns and dependencies in\\nthe data. Unlike methods that apply masking at individual\\ntime steps, our approach focuses on larger patches. This\\ndesign choice avoids simplistic interpolation from adjacent\\ntime points and encourages the model to understand the\\nentire sequence deeply. The reconstruction of these patches\\nis achieved by optimizing the mean squared error (MSE)\\nloss function.\\n4. Experiments\\nIn this section, we evaluate the efficacy of TSLANet on\\ntime series classification, forecasting, and anomaly detection\\ntasks. We show that our TSLANet can serve as a foundation\\nmodel with competitive performance on these tasks. The\\n5', metadata={'source': 'rpaper.pdf', 'page': 4}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 2: Classification results in different datasets. Results are averaged across each subset of datasets. Results are in terms\\nof accuracy (as %). Blue : best results, Purple : second best. Full results are listed in Tables 10, 11, and 12 in the Appendix.\\nMethodsTSLANet GPT4TS TimesNet ROCKET Crossformer PatchTST MLP TS-TCC TS2VEC\\n(Ours) (2023) (2023) (2020) (2023) (2023) (2023) (2021) (2022)\\nUCR repository (85 datasets) 83.18 61.58 65.27 81.42 73.47 71.84 69.68 75.07 81.42\\nUEA repository (26 datasets) 72.73 58.51 66.55 68.79 66.84 69.13 65.81 69.38 59.62\\nBiomedical signals (2 datasets) 90.24 87.04 87.10 87.20 70.82 83.87 70.63 92.25 86.31\\nHuman activity recognition (3 datasets) 97.46 92.71 91.51 96.44 77.55 94.87 56.69 97.16 95.70\\nAverage 85.90 74.96 77.61 83.46 72.17 79.93 65.70 83.55 80.76\\nTable 3: Multivariate forecasting results with prediction lengths ∈ {96,192,336,720}. Results are averaged from all\\nprediction lengths. Avg means further averaged by subsets .Blue : best results, Purple : second best. Full results are listed in\\nTable 13 in the Appendix.\\nModelsTSLANet Time-LLM iTransformer PatchTST Crossformer FEDformer Autoformer RLinear Dlinear TimesNet GPT4TS SCINet\\n(Ours) (2024) (2024) (2023) (2023) (2022) (2021b) (2023) (2023) (2023) (2023) (2022)\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\nECL 0.165 0.257 0.158 0.252 0.178 0.270 0.167 0.259 0.244 0.334 0.214 0.327 0.227 0.338 0.219 0.298 0.166 0.263 0.192 0.295 0.167 0.263 0.268 0.365\\nETT (Avg) 0.337 0.377 0.330 0.372 0.383 0.399 0.347 0.378 0.685 0.578 0.408 0.428 0.465 0.459 0.380 0.392 0.369 0.398 0.391 0.404 0.350 0.382 0.689 0.597\\nExchange 0.369 0.404 - -0.360 0.403 0.367 0.404 0.940 0.707 0.519 0.429 0.613 0.539 0.378 0.417 0.297 0.378 0.416 0.443 0.370 0.406 0.750 0.626\\nTraffic 0.396 0.271 0.388 0.264 0.428 0.282 0.420 0.277 0.550 0.304 0.610 0.376 0.628 0.379 0.626 0.378 0.433 0.295 0.620 0.336 0.414 0.294 0.804 0.509\\nWeather 0.228 0.264 0.225 0.257 0.258 0.279 0.238 0.268 0.259 0.315 0.309 0.360 0.338 0.382 0.272 0.291 0.246 0.300 0.259 0.287 0.237 0.270 0.292 0.363\\ndetailed experimental setup is described in Section D, while\\nthe detailed experimental results are presented in Section F\\nin the Appendix.\\n4.1. Classification\\nDatasets. We examine the classification ability of\\nTSLANet on a total of 116 datasets, including 85 uni-\\nvariate UCR datasets (Dau et al., 2019), 26 multi-variate\\nUEA datasets (Bagnall et al., 2018). We also include another\\n5 datasets, i.e., two biomedical datasets, namely, Sleep-EDF\\ndataset (Goldberger et al., 2000) for EEG-based sleep stage\\nclassification and MIT-BIH dataset (Moody & Mark, 2001)\\nfor ECG-based arrhythmia classification, and three human\\nactivity recognition (HAR) datasets, namely, UCIHAR (An-\\nguita et al., 2013), WISDM (Kwapisz et al., 2011), and\\nHHAR (Stisen et al., 2015). These datasets have different\\ncharacteristics and they span a wide range of time series\\napplications. More details about these datasets are included\\nin Appendix E.2.\\nBaselines and Experimental Settings. We select eight\\nstate-of-the-art baselines, i.e., GPT4TS (Zhou et al.,\\n2023), TimesNet (Wu et al., 2023), ROCKET (Demp-\\nster et al., 2020), TS-TCC (Eldele et al., 2021), TS2Vec\\n(Yue et al., 2022), Crossformer (Zhang & Yan, 2023) and\\nPatchTST (Nie et al., 2023) as they showed the best classifi-\\ncation accuracy over other Transformer-based architectures.\\nLast, we experiment with a simple single-layer MLP.Results. Table 2 reports the classification results, where\\nour proposed TSLANet demonstrates superior performance\\nover state-of-the-art baselines. Notably, convolution-based\\nmethods, including ROCKET, TS-TCC, and our approach,\\noutperform Transformer-based models, highlighting their\\nsuperiority in classification tasks. For example, in the\\nUCR repository, TSLANet achieves an impressive accu-\\nracy of 83.18%, outperforming other models including the\\nROCKET, which scores 81.42%. The UEA repository re-\\nsults further reinforce our efficacy, with a 72.73% accuracy,\\ncompared to the next best model, PatchTST, at 69.38%.\\nIn more specialized datasets like biomedical signals and\\nHAR, our advantage is even more pronounced, achieving\\nan overall accuracy of 90.24% and 97.46%, respectively.\\nThese results highlight the robustness and adaptability of\\nTSLANet in diverse time series contexts.\\nIn our comparative analysis, Transformer models generally\\nface challenges across various datasets, reflecting inherent\\nlimitations in handling time series data. MLP models per-\\nform well on simpler UCR datasets but falter in complex,\\nnoisy environments. TimesNet excels in datasets rich in\\nfrequency information but struggles with simpler ones. Last,\\nthe GPT4TS model shows promise in larger datasets due to\\nthe high capacity of the GPT model, yet underperforms in\\nsmaller datasets due to probable overfitting.\\n4.2. Forecasting\\nDatasets. To assess the efficacy of TSLANet in forecast-\\ning, we conduct comprehensive evaluations on eight bench-\\n6', metadata={'source': 'rpaper.pdf', 'page': 5}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 4: Anomaly detection task. We calculate the F1-score (as %) for each dataset. ∗. in the Transformers indicates the\\nname of ∗former. Blue : best, Purple : second best. Table 14 in the Appendix shows the full results.\\nMethodsTSLANetGPT4TS TimesNet PatchTST ETS. FED. LightTS DLinear Stationary Auto. Pyra. Anomaly. In. Re. LogTrans. Trans.(Ours)\\nSMD 87.91 86.89 84.61 84.62 83.13 85.08 82.53 77.10 84.72 85.11 83.04 85.49 81.65 75.32 76.21 79.56\\nMSL 83.32 82.45 81.84 78.70 85.03 78.57 78.95 84.88 77.50 79.05 84.86 83.31 84.06 84.40 79.57 78.68\\nSMAP 75.96 72.88 69.39 68.82 69.50 70.76 69.21 69.26 71.09 71.12 71.09 71.18 69.92 70.40 69.97 69.70\\nSWaT 92.80 94.23 93.02 85.72 84.91 93.19 93.33 87.52 79.88 92.74 91.78 83.10 81.43 82.80 80.52 80.37\\nPSM 97.73 97.13 97.34 96.08 91.76 97.23 97.15 93.55 97.29 93.29 82.08 79.40 77.10 73.61 76.74 76.07\\nAverage 87.54 86.72 85.24 82.79 82.87 84.97 84.23 82.46 82.08 84.26 82.57 80.50 78.83 77.31 76.60 76.88\\nmark datasets. i.e., Electricity ( ECL) featuring electric-\\nity consumption data, four ETT datasets ( ETTh1, ETTh2,\\nETTm1, ETTm2 ) that encompass a range of scenarios in en-\\nergy transfer technology, Exchange that encompasses fluctu-\\nating currency exchange rates, Traffic that comprises traffic\\nflow information, and Weather that offers insights into vari-\\nous meteorological variables over time. We include more\\ndetails about their characteristics in Appendix E.3.\\nBaselines and Experimental Settings. We compare\\nTSLANet against a variety of state-of-the-art baselines.\\nFor Transformer architectures, we compare against iTrans-\\nformer (Liu et al., 2024), PatchTST, Crossformer, FED-\\nformer (Zhou et al., 2022), and Autoformer (Wu et al.,\\n2021b). For MLP-based models, we compare against RLin-\\near (Li et al., 2023) and DLinear (Zeng et al., 2023) models.\\nFor general-purpose time series models, we compare our\\nmodel against TimesNet and GPT4TS. For a convolutional-\\nbased forecasting model, we compare with SCINet (Liu\\net al., 2022). Last, we include Time-LLM (Jin et al., 2024),\\nwhich is based on Large-Language Models. Similar to\\n(Zhou et al., 2023) settings, we set the look-back window to\\n336 for the ETT dataset, 96 for Exchange, 512 for the Traf-\\nfic and Weather datasets, and 96 for the ECL dataset. We\\nalso incorporate the data normalization block, and reverse\\ninstance norm in the forecasting task (Kim et al., 2021).\\nFor the baselines, we report the best results in their original\\nworks if they are consistent with our settings, otherwise, we\\nre-run their codes again.\\nResults. In our forecasting experiments presented in Ta-\\nble 3, we notice the superiority of Time-LLM due to its re-\\nliance on the large Llama-7B model (Touvron et al., 2023),\\nwhich enables it to capture complex patterns and depen-\\ndencies in data. Other than Time-LLM, TSLANet consis-\\ntently outperforms baseline models across various datasets.\\nSpecifically, it achieves the second lowest MSE and MAE in\\nseven out of eight datasets, showing 3% and 3.8% MSE im-\\nprovement over the state-of-the-art PatchTST in ETT(avg)\\nand Weather datasets respectively. This indicates the ef-\\nfectiveness of our model in handling datasets with diverse\\ncharacteristics and complexities. In addition, it shows the\\neffect of the added capability of the ASB module in learninglong-range dependencies.\\nThe results also suggest the superiority of our model\\nover specialized Transformer-based architectures and MLP-\\nbased models. These models, e.g., iTransformer and Dlinear\\nshow competitive performance in certain datasets but fall\\nbehind in others. In addition, GPT4TS shows the power\\nof the GPT models in the forecasting task by scoring the\\nsecond-best performance in some datasets.\\nWhile Time-LLM offers slightly better performance, its\\ncomputational cost is significantly higher than TSLANet .\\nTo illustrate, TSLANet demonstrates a nearly equivalent\\nperformance to Time-LLM on the ETTh1 dataset with\\nan MSE of 0.413 compared to Time-LLM’s 0.408, yet\\nTSLANet does so with significantly lower computational\\ncost of 6.9e+10 FLOPS against 7.3e+12 for Time-LLM.\\nThis showcases the effective balance between performance\\nand computational efficiency in our TSLANet .\\n4.3. Anomaly Detection\\nDatasets. In this study, we focus on detecting anomalies\\nin unsupervised time series data. We use five benchmark\\ndatasets for our experiments: SMD (Su et al., 2019) for\\nserver monitoring, MSL (Hundman et al., 2018) for space\\ntelemetry, SMAP (Hundman et al., 2018) for earth observa-\\ntions, SWaT (Mathur & Tippenhauer, 2016) for water treat-\\nment security, and PSM (Abdulaal et al., 2021) for industrial\\npump sensors. We discuss their details in Appendix E.4.\\nBaselines and Experimental Settings. We followed the\\nsame experimental settings and adopted the same baselines\\nin GPT4TS (Zhou et al., 2023). These are GPT4TS, Times-\\nNet, PatchTST, ETSformer (Woo et al., 2022), FEDformer,\\nLightTS (Zhang et al., 2022), DLinear, Stationary (Liu et al.,\\n2022), Autoformer, Pyraformer (Liu et al., 2021), Anoma-\\nlyformer (Xu et al., 2022), Informer, Reformer, LogTrans-\\nformer (Li et al., 2019), and the vanilla Transformer. For\\ndata preparation, we segmented each dataset with a slid-\\ning window, following (Xu et al., 2022). We adopted the\\nreconstruction error as our evaluation metric, common in\\nunsupervised learning for spotting anomalies.\\n7', metadata={'source': 'rpaper.pdf', 'page': 6}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 5: Ablation study to the effect of each component.\\nASB-L refers to the local filters in the ASB. UWaveGL is\\nthe UWaveGestureLibrary dataset from the UEA repository.\\nVariantClassification (ACC %) Forecasting (MSE)\\nFordA UWaveGL ETTh1 Exchange\\nw/o ASB 87.3 77.5 0.421 0.380\\nw/o ASB (L) 92.7 88.9 0.417 0.373\\nw/o ICB 91.3 86.2 0.419 0.376\\nw/o pretraining 92.5 90.6 0.415 0.372\\nTSLANet 93.1 91.3 0.413 0.369\\nResults. Table 4 presents the results, where TSLANet\\nperforms best in most of the datasets with an overall F1-\\nscore of 87.54%. It outperforms advanced models like FED-\\nformer and Autoformer, especially in the SMD and PSM\\ndatasets with F1-scores of 87.91% and 97.73% respectively.\\nGPT4TS model follows closely, ranking second with an\\noverall average of 86.72%. Its high capacity makes it effec-\\ntive in detecting anomalies, though it slightly trails behind.\\nNotably, Transformer-based models exhibit lower efficacy\\nin anomaly detection in general. This could be regarded\\nto the attention mechanism focusing on dominant normal\\npoints, thus missing rare anomalies. Models that consider\\nperiodicity, like TimesNet and FEDformer, perform well,\\nindicating the value of periodic analysis in highlighting\\nunusual patterns.\\n5. Model Analysis\\n5.1. Ablation Study\\nIn Table 5, we assess the contribution of the different com-\\nponents in our model, where we report the performance of\\nthe model when removing each component individually. No-\\ntably, removing the Adaptive Spectral Block (i.e., w/o ASB)\\nyields a notable decline in performance. For classification\\ntasks on FordA and UWaveGestureLibrary datasets, the ac-\\ncuracy drops to 87.3% and 77.5%, respectively. Similarly,\\nits absence results in higher MSE values in the forecasting\\ntask of 0.421 and 0.380 for ETTh1 and Exchange datasets.\\nThis underscores the ASB’s critical role in feature extraction\\nand noise reduction. Similarly, excluding the local adap-\\ntive part of the ASB (i.e., w/o ASB-L) affects the noisy\\ndatasets more than less noisy ones, highlighting the local\\ncomponent’s value in handling noise.\\nThe effect of the ICB was less than the ASB, with less\\nperformance degradation in the two tasks. However, its re-\\nmoval shows reduced classification accuracy and increased\\nforecasting MSE indicating its importance. The role of\\npretraining is similarly validated, as its absence slightly\\ndiminishes the model’s performance across both tasks.\\n0 0.5 1 1.5 2 2.5\\nNoise level406080100Accuracy (%)\\nUCIHAR\\nTSLANet\\nTSLANet w/o Adaptive Filter\\nTransformer(a) Robustness against noise lev-\\nels on UCIHAR dataset.\\n0 0.5 1 1.5 2 2.5\\nNoise level708090100Accuracy (%)\\nNATOPS\\nTSLANet\\nTSLANet w/o Adaptive Filter\\nTransformer(b) Robustness against noise\\nlevels on NATOPS dataset.\\n(c) The features before and after the local adaptive filter on the\\nUEA Handwriting dataset.\\nFigure 3: Effectiveness of the Adaptive Filter in noise re-\\nduction.\\n5.2. Efficacy of Adaptive Filtering in Noise Reduction\\nWe delve into the effectiveness of the Adaptive Filter in\\nmitigating noise and enhancing model robustness by ex-\\namining Figure 3. Specifically, Figures 3a and 3b present\\nthe performance of TSLANet , both with and without the\\nAdaptive Filter, against the Transformer model by adding\\ndifferent Gaussian noise levels to the time series. The per-\\nformance of the Transformer deteriorates rapidly as noise\\nincreases. In contrast, TSLANet maintains a relatively sta-\\nble performance, with the variant using the Adaptive Filter\\nshowing the most resilience to noise. This is particularly\\nnoteworthy at higher noise levels, where the accuracy of the\\nstandard Transformer falls steeply, while TSLANet with\\nthe Adaptive Filter experiences a much less pronounced\\ndecline.\\nIn Figure 3c, we observe the frequency spectra before and\\nafter applying the Adaptive Filter. The left plot shows a\\nnoisy spectrum with high amplitude spikes across various\\nfrequencies. However, after applying the Adaptive Filter,\\na markedly cleaner spectrum where the amplitude of noise\\nspikes is significantly reduced, particularly in the higher\\nfrequency range. This demonstrates the filter’s ability to at-\\ntenuate unwanted noise while preserving the relevant signal.\\n5.3. Scaling Efficiency\\nWe compare the scalability of our TSLANet with one of the\\nbest-performing Transformer models in the classification\\ntask, i.e., PatchTST (Nie et al., 2023), by observing their\\nperformance across various dataset sizes and layer counts.\\nSpecifically, we experiment with variable data sizes from\\n8', metadata={'source': 'rpaper.pdf', 'page': 7}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\n1 5 10 15 20\\nNumber of Layers5060708090100Accuracy (%)\\nTSLANet\\nPatchTSTData Percentage\\n1%\\n10%\\n50%\\n100%\\nFigure 4: A comparison between TSLANet vs. PatchTST\\nin terms of accuracy with varying the number of layers in\\nboth for different data percentages from the uWaveGesture-\\nLibraryAll dataset.\\nthe uWaveGestureLibraryAll dataset, as shown in Figure 4.\\nNotably, in smaller data sizes, TSLANet demonstrates a\\nconsistent accuracy level, subtly decreasing as the number of\\nlayers increases. In contrast, the PatchTST shows a marked\\ndecline in accuracy with additional layers, suggesting a\\npotential overfitting issue or inefficiency in handling limited\\ndata with increased model complexity.\\nAs dataset sizes grow, TSLANet performance remains ro-\\nbust, showing slight variations in accuracy with more layers.\\nThis stability contrasts with the PatchTST performance,\\nwhich tends to decrease notably at higher layer counts. This\\ntrend in PatchTST could be attributed to their inherent de-\\nsign, which might lead to diminishing returns or optimiza-\\ntion challenges as the model depth increases. Lastly, we\\nnotice that TSLANet effectively leverages larger dataset\\nsamples, as its performance improves with an increase in\\nthe number of layers, highlighting its capacity to capitalize\\non more extensive data for enhanced accuracy.\\n5.4. Complexity Analysis\\nWe compare the complexity of our TSLANet with Times-\\nNet and Transformer-based models, e.g., PatchTST, FED-\\nFormer, AutoFormer, Informer, and Reformer in terms of\\nthe number of parameters, FLOPs, and accuracy on the\\nUEA Heartbeat dataset, as shown in Figure 5. TSLANet\\ndemonstrates superior efficiency and accuracy in time series\\nanalysis, achieving the highest accuracy of 77.56% with the\\nlowest computational and parameter footprint among the\\ncompared models. It requires 93% fewer FLOPs and 84%\\nfewer parameters than the PatchTST, yet outperforms it by\\nover 8% in accuracy. Compared to TimesNet, TSLANet\\noperates with more than 99% fewer FLOPs and parameters\\nwhile still delivering a 3% higher accuracy.\\nThis considerable reduction in computational demand con-\\nfirms the lightweight nature of TSLANet compared to\\nTransformer-based alternatives, underscoring its capacity to\\n0 1 2 3 4 5 6 7\\nNumber of Parameters (Millions)50556065707580Accuracy (%) TSLANet\\n PatchTST\\n FEDformer\\n Autoformer TimesNet\\n Informer\\n Reformer\\n1G FLOPs 10G FLOPsFigure 5: TSLANet vs. baselines in terms of the number\\nof parameters and FLOPS count against the classification\\naccuracy of the UEA Heartbeat dataset.\\nmake time series analysis more efficient.\\n6. Conclusions\\nIn this paper, we introduced TSLANet , a novel lightweight\\nmodel for time series analysis that revisits the convolution\\napproach as a potent replacement to Transformers, with\\nan innovative combination of convolution operations and\\nadaptive spectral analysis. Our comprehensive experiments\\nacross various datasets in classification, forecasting, and\\nanomaly detection have demonstrated its superior perfor-\\nmance over traditional Transformer models, particularly\\nin its ability to maintain high accuracy levels in noisy\\nconditions and across different data sizes. Furthermore,\\nour in-depth layer-wise performance analysis revealed that\\nTSLANet not only outperforms Transformers in smaller\\ndatasets but also exhibits improved scalability with increas-\\ning layers, particularly in larger datasets. TSLANet is a\\nstep towards a foundation model for time series analysis.\\nImpact Statement\\nOur proposed work TSLANet aims to advance the field of\\nMachine Learning by providing a more efficient, scalable,\\nand robust foundation model for analyzing time series data\\nacross various applications. It has the potential to impact\\nvarious sectors, including healthcare, finance, and environ-\\nmental monitoring, by enhancing forecasting accuracy and\\nanomaly detection capabilities. Such improvements could\\nlead to better patient outcomes, more informed financial\\ndecisions, and greater preparedness for natural disasters.\\nReferences\\nAbdulaal, A., Liu, Z., and Lancewicki, T. Practical ap-\\nproach to asynchronous multivariate time series anomaly\\ndetection and localization. In SIGKDD , 2021.\\n9', metadata={'source': 'rpaper.pdf', 'page': 8}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nAnguita, D., Ghio, A., Oneto, L., Parra, X., and Reyes-\\nOrtiz, J. L. A public domain dataset for human activity\\nrecognition using smartphones. In European Symposium\\non Artificial Neural Networks , 2013.\\nBagnall, A., Dau, H. A., Lines, J., Flynn, M., Large, J.,\\nBostrom, A., Southam, P., and Keogh, E. The uea multi-\\nvariate time series classification archive, 2018, 2018.\\nD’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S.,\\nBiroli, G., and Sagun, L. Convit: Improving vision trans-\\nformers with soft convolutional inductive biases. In ICML ,\\npp. 2286–2296, 2021.\\nDau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu,\\nY ., Gharghabi, S., Ratanamahatana, C. A., and Keogh, E.\\nThe ucr time series archive, 2019.\\nDempster, A., Petitjean, F., and Webb, G. I. ROCKET:\\nExceptionally fast and accurate time series classification\\nusing random convolutional kernels. Data Mining and\\nKnowledge Discovery , 34(5):1454–1495, 2020.\\nEkambaram, V ., Jati, A., Nguyen, N., Sinthong, P., and\\nKalagnanam, J. Tsmixer: Lightweight mlp-mixer model\\nfor multivariate time series forecasting. KDD , 2023.\\nEldele, E., Ragab, M., Chen, Z., Wu, M., Kwoh, C. K., Li,\\nX., and Guan, C. Time-series representation learning\\nvia temporal and contextual contrasting. In IJCAI , pp.\\n2352–2359, 2021.\\nGoldberger, A. L., Amaral, L. A. N., Glass, L., Hausdorff,\\nJ. M., Ivanov, P. C., Mark, R. G., Mietus, J. E., Moody,\\nG. B., Peng, C.-K., and Stanley, H. E. Physiobank, phys-\\niotoolkit, and physionet components of a new research\\nresource for complex physiologic signals. Circulation ,\\n2000.\\nHe, K., Chen, X., Xie, S., Li, Y ., Doll ´ar, P., and Girshick,\\nR. Masked autoencoders are scalable vision learners. In\\nCVPR , 2022.\\nHundman, K., Constantinou, V ., Laporte, C., Colwell, I.,\\nand Soderstrom, T. Detecting spacecraft anomalies us-\\ning lstms and nonparametric dynamic thresholding. In\\nSIGKDD , 2018.\\nJin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y ., Shi, X.,\\nChen, P.-Y ., Liang, Y ., Li, Y .-F., Pan, S., and Wen, Q.\\nTime-LLM: Time series forecasting by reprogramming\\nlarge language models. In International Conference on\\nLearning Representations (ICLR) , 2024.\\nKim, T., Kim, J., Tae, Y ., Park, C., Choi, J.-H., and Choo,\\nJ. Reversible instance normalization for accurate time-\\nseries forecasting against distribution shift. ICLR , 2021.Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The\\nefficient transformer. In ICLR , 2020.\\nKwapisz, J. R., Weiss, G. M., and Moore, S. A. Activ-\\nity recognition using cell phone accelerometers. Sigkdd\\nExplorations , 2011.\\nLi, J., Hui, X., and Zhang, W. Informer: Beyond efficient\\ntransformer for long sequence time-series forecasting. In\\nAAAI , 2021.\\nLi, K., Wang, Y ., Peng, G., Song, G., Liu, Y ., Li, H., and\\nQiao, Y . Uniformer: Unified transformer for efficient\\nspatial-temporal representation learning. In ICLR , 2022.\\nLi, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .-X.,\\nand Yan, X. Enhancing the locality and breaking the mem-\\nory bottleneck of transformer on time series forecasting.\\nInNeurIPS , volume 32, 2019.\\nLi, Z., Qi, S., Li, Y ., and Xu, Z. Revisiting long-term time\\nseries forecasting: An investigation on linear mapping.\\narXiv preprint arXiv:2305.10721 , 2023.\\nLiu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L., and\\nXu, Q. Scinet: time series modeling and forecasting with\\nsample convolution and interaction. NeurIPS , 2022.\\nLIU, M., Zeng, A., LAI, Q., Gao, R., Li, M., Qin, J., and Xu,\\nQ. T-wavenet: A tree-structured wavelet neural network\\nfor time series signal analysis. In ICLR , 2022.\\nLiu, P., Wu, B., Li, N., Dai, T., Lei, F., Bao, J., Jiang, Y ., and\\nXia, S.-T. Wftnet: Exploiting global and local periodicity\\nin long-term time series forecasting. ICASSP , 2023.\\nLiu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and\\nDustdar, S. Pyraformer: Low-complexity pyramidal atten-\\ntion for long-range time series modeling and forecasting.\\nICLR , 2021.\\nLiu, Y ., Wu, H., Wang, J., and Long, M. Non-stationary\\ntransformers: Rethinking the stationarity in time series\\nforecasting. NeurIPS , 2022.\\nLiu, Y ., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L.,\\nand Long, M. itransformer: Inverted transformers are\\neffective for time series forecasting. In ICLR , 2024.\\nMathur, A. P. and Tippenhauer, N. O. Swat: A water treat-\\nment testbed for research and training on ics security.\\nIninternational workshop on cyber-physical systems for\\nsmart water networks (CySWater) , 2016.\\nMeng, Q., Qian, H., Liu, Y ., Cui, L., Xu, Y ., and Shen, Z.\\nMHCCL: masked hierarchical cluster-wise contrastive\\nlearning for multivariate time series. In AAAI , 2023.\\n10', metadata={'source': 'rpaper.pdf', 'page': 9}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nMoody, G. and Mark, R. The impact of the mit-bih arrhyth-\\nmia database. IEEE Engineering in Medicine and Biology\\nMagazine , 2001. doi: 10.1109/51.932724.\\nNie, Y ., Nguyen, N. H., Sinthong, P., and Kalagnanam, J.\\nA time series is worth 64 words: Long-term forecasting\\nwith transformers. ICLR , 2023.\\nRao, Y ., Zhao, W., Zhu, Z., Lu, J., and Zhou, J. Global\\nfilter networks for image classification. In Beygelzimer,\\nA., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.),\\nNeurIPS , 2021.\\nRhif, M., Ben Abbes, A., Farah, I. R., Mart ´ınez, B., and\\nSang, Y . Wavelet transform application for/in non-\\nstationary time-series analysis: A review. Applied Sci-\\nences , 2019.\\nStisen, A., Blunck, H., Bhattacharya, S., Prentow, T. S.,\\nKjærgaard, M. B., Dey, A., Sonne, T., and Jensen, M. M.\\nSmart devices are different: Assessing and mitigatingmo-\\nbile sensing heterogeneities for activity recognition. In\\nProceedings of the 13th ACM Conference on Embedded\\nNetworked Sensor Systems , 2015.\\nSu, Y ., Zhao, Y ., Niu, C., Liu, R., Sun, W., and Pei, D.\\nRobust anomaly detection for multivariate time series\\nthrough stochastic recurrent neural network. In SIGKDD ,\\n2019.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels. arXiv preprint arXiv:2302.13971 , 2023.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\\nis all you need. NeurIPS , 2017.\\nWen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J.,\\nand Sun, L. Transformers in time series: A survey. In\\nIJCAI , pp. 6778–6786, 8 2023.\\nWoo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S. Ets-\\nformer: Exponential smoothing transformers for time-\\nseries forecasting. arXiv preprint arXiv:2202.01381 ,\\n2022.\\nWu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L.,\\nand Zhang, L. Cvt: Introducing convolutions to vision\\ntransformers. In ICCV , 2021a.\\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: De-\\ncomposition transformers with Auto-Correlation for long-\\nterm series forecasting. NeurIPS , 2021b.Wu, H., Hu, T., Liu, Y ., Zhou, H., Wang, J., and Long, M.\\nTimesnet: Temporal 2d-variation modeling for general\\ntime series analysis. ICLR , 2023.\\nXu, J., Wu, H., Wang, J., and Long, M. Anomaly trans-\\nformer: Time series anomaly detection with association\\ndiscrepancy. In ICLR , 2022.\\nYang, L. and Hong, S. Unsupervised time-series represen-\\ntation learning with iterative bilinear temporal-spectral\\nfusion. In ICML , 2022.\\nYue, Z., Wang, Y ., Duan, J., Yang, T., Huang, C., Tong, Y .,\\nand Xu, B. Ts2vec: Towards universal representation of\\ntime series. In AAAI , 2022.\\nZeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers\\neffective for time series forecasting? AAAI , 2023.\\nZhang, J., Feng, L., He, Y ., Wu, Y ., and Dong, Y . Temporal\\nconvolutional explorer helps understand 1d-cnn’s learn-\\ning behavior in time series classification from frequency\\ndomain. In CIKM , 2023.\\nZhang, T., Zhang, Y ., Cao, W., Bian, J., Yi, X., Zheng, S.,\\nand Li, J. Less is more: Fast multivariate time series\\nforecasting with light sampling-oriented mlp structures.\\narXiv preprint arXiv:2207.01186 , 2022.\\nZhang, Y . and Yan, J. Crossformer: Transformer utilizing\\ncross-dimension dependency for multivariate time series\\nforecasting. ICLR , 2023.\\nZhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin,\\nR. FEDformer: Frequency enhanced decomposed trans-\\nformer for long-term series forecasting. ICML , 2022.\\nZhou, T., Niu, P., Wang, X., Sun, L., and Jin, R. One fits all:\\nPower general time series analysis by pretrained LM. In\\nNeurIPS , 2023.\\n11', metadata={'source': 'rpaper.pdf', 'page': 10}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nA. Circular Convolutions\\nThe convolution theorem suggests that the multiplication in the frequency domain is equivalent to the circular convolution\\nprocess.\\nLetx[n]andh[n]be two length Nsequences. Their DFTs are X[k]andH[k], respectively. Consider the circular convolution\\ny[n] = (x⊛h)[n]. The DFT of y[n]isY[k].\\nFirst, the DFT of the Convolution can be formulated as:\\nY[k] =N−1X\\nn=0 N−1X\\nm=0x[m]·h[(n−m) mod N]!\\n·e−i2πkn/N\\nHowever, if we changed the order of summation, it becomes:\\nY[k] =N−1X\\nm=0x[m]·N−1X\\nn=0h[(n−m) mod N]·e−i2πkn/N\\nBy substituting n−mwithr:\\nY[k] =N−1X\\nm=0x[m]·e−i2πkm/N·N−1X\\nr=0h[r]·e−i2πkr/N\\nTherefore, we recognize the DFTs of x[n]andh[n]:\\nY[k] = N−1X\\nm=0x[m]·e−i2πkm/N!\\n· N−1X\\nr=0h[r]·e−i2πkr/N!\\nY[k] =X[k]·H[k]\\nThus, we have shown that the DFT of the circular convolution of two sequences x[n]andh[n]is the product of their\\nindividual DFTs, i.e., Y[k] =X[k]·H[k].\\nB. Frequency Domain Processing Role to Learn Long-Range Dependencies\\nFourier transforms, used in our Adaptive Spectral Block (ASB), can learn long-range and short-range dependencies in time\\nseries. The Fourier Transform (FT) of a time series x(t)is given by:\\nX(f) =Z∞\\n−∞x(t)e−j2πftdt\\nwhere X(f)represents the signal in the frequency domain, fis the frequency, and trepresents time.\\nThe FT decomposes x(t)into its constituent frequencies, where each frequency component represents a pattern in the time\\nseries. Low-frequency components correspond to long-range dependencies (slowly changing trends), and high-frequency\\ncomponents correspond to short-range dependencies (rapid fluctuations). Let’s consider a simplified model where the ASB\\napplies a filter H(f)to the Fourier transform X(f)of the input signal, enhancing certain frequencies while attenuating\\nothers:\\nY(f) =H(f)·X(f)\\nwhere Y(f)is the output signal in the frequency domain.\\nThe adaptiveness comes from adjusting H(f)based on the data, which can be modeled as a learning process where\\nH(f)is updated to minimize a loss function Lthat measures the discrepancy between the model output and the true data\\ncharacteristics:\\nmin\\nH(f)L(Y(f),True Data )\\n12', metadata={'source': 'rpaper.pdf', 'page': 11}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nThrough this process, H(f)learns to emphasize the frequency components that are most relevant for predicting the target,\\nwhether they capture long-range or short-range dependencies.\\nAfter filtering in the frequency domain, the inverse Fourier transform (IFT) is applied to convert Y(f)back into the time\\ndomain, yielding the modified signal y(t):\\ny(t) =Z∞\\n−∞Y(f)ej2πftd f\\nThis signal now encapsulates the learned dependencies, ready for further processing or as an input to subsequent model\\nlayers.\\nC. Algorithm of Adaptive Spectral Block\\nAlgorithm 1 Pseudocode of the Adaptive Spectral Block.\\ndef adaptive_high_freq_mask(x, threshold):\\n# Calculate energy\\nenergy = torch.abs(x_fft).pow(2).sum(dim=-1)\\n# Compute the adaptive threshold\\nthreshold = torch.quantile(energy, threshold)\\n# Identify the dominant frequencies\\ndominant_freq = normalized_energy > threshold\\n# Set adaptive mask values\\nadaptive_mask[dominant_freq] = 1\\nreturn adaptive_mask\\n# Transform input x_in to frequency domain\\nX_fft = fft(x_in)\\n# Create an adaptive mask for high-freq. components\\nfreq_mask = adaptive_high_freq_mask(X_fft, threshold)\\n# Apply adaptive high-frequency mask\\nX_masked = X_fft *freq_mask\\n# Apply global and local learnable weights\\nX_L = X_masked *local_weight\\nX_G = X_fft *global_weight\\n# Transform data back into the time domain\\nx_out = ifft(X_L + X_G)\\nD. Experimental Setup\\nD.1. Training Protocol\\nTo train the classification experiments, we optimized TSLANet using AdamW with a learning rate of 1e-3 and a weight\\ndecay of 1e-4, applied during both training and pretraining phases. The experiments ran for 50 epochs for pretraining and\\n100 epochs for fine-tuning. For the forecasting and anomaly detection experiments, we utilized a learning rate of 1e-4 and a\\nweight decay of 1e-6, with both phases running for 10 epochs.\\nFor all experiments, the patch size was set to 8 and the stride to 4. Each experiment was repeated three times, with the\\naverage performance reported. TSLANet was implemented using PyTorch and conducted on NVIDIA RTX A6000 GPUs.\\nD.2. Objective Functions\\nFor the classification task, we employ a categorical cross-entropy loss function with label smoothing, defined as Lclf=\\n−PC\\ni=1ysmooth\\ni·log(ˆyi). Here, ysmooth\\ni is the true class label in one-hot encoded form adjusted via label smoothing, ˆyiis the\\npredicted probability for each class, and Cis the total number of classes. Label smoothing reduces model confidence by\\nadjusting the true labels with a smoothing parameter ϵ, making the distribution more uniform, where each yiis transformed\\ntoysmooth\\ni = (1−ϵ)·yi+ϵ\\nC.\\nIn forecasting and anomaly detection, we use the Mean Squared Error (MSE) to measure discrepancies between predicted\\nvalues and actual observations, expressed as LMSE=1\\nNPN\\ni=1(yi−ˆyi)2. Here, yirepresents the actual value at time i,ˆyi\\n13', metadata={'source': 'rpaper.pdf', 'page': 12}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\ndenotes the forecasted value, and Nis the number of predictions. This MSE loss is also utilized in self-supervised learning\\ntasks to reconstruct masked patches.\\nD.3. Evaluation Metrics\\nModel performance was evaluated using standard metrics appropriate to each task. For classification, we reported accuracy;\\nfor forecasting, Mean Squared Error (MSE) and Mean Absolute Error (MAE) were used; for anomaly detection, the F1-score\\nwas our primary metric due to the imbalanced nature of the datasets.\\nE. Datasets Details\\nE.1. Data Preprocessing\\nFor the classification task, the UCR and UEA datasets are already split into train/test splits. A validation set was picked\\nfrom each dataset in the training set with a ratio of 80/20. The selection of the hyperparameters was based on the average\\nresults on the validation sets across each collection of datasets, i.e., UCR and UEA. For biomedical and human activity\\nrecognition datasets, which are not split by default, we split the data into a 60/20/20 ratio for train/validation/test splits.\\nFor forecasting and anomaly detection datasets, these are split into a ratio of 70/10/20 following a line of previous works,\\ntowards a fair comparison with these works (Zhou et al., 2022; Kitaev et al., 2020; Li et al., 2021; Wu et al., 2023). All\\ndatasets are normalized during training.\\nFor the self-supervised task, we deploy the unlabeled version of the training set in each dataset for pretraining, then use the\\nsame set again with labels for fine-tuning.\\nE.2. Classification\\nIn our evaluation, we extensively utilize four categories of datasets:\\n•UCR datasets: The UCR Time Series Classification Archive is one of the most comprehensive collections of univariate\\ndatasets tailored for time series analysis. This archive encompasses 85 diverse datasets, each presenting unique\\nchallenges and characteristics that span a wide array of domains, from healthcare and finance to environmental\\nmonitoring and beyond. The variety within the UCR archive allows for a robust assessment of TSLANet across\\ndifferent contexts, showcasing its versatility and performance.\\n•UEA datasets: We also incorporate datasets from the University of East Anglia (UEA) Time Series Classification\\nrepository, which is renowned for its rich collection of multivariate time series datasets. We were able to preprocess 26\\ndatasets, each offering a multidimensional perspective on time series analysis across various real-world scenarios, such\\nas human activity recognition, sensor data interpretation, and complex system monitoring. More details about the UCR\\nand UEA datasets can be found in https://www.timeseriesclassification.com/ .\\n•Biomedical datasets: The biomedical domain presents unique challenges and opportunities for time series analysis. In\\nthis context, we utilized two pivotal datasets for our evaluation: the Sleep-EDF dataset and the MIT-BIH Arrhythmia\\ndataset.\\n–Sleep-EDF Dataset: This dataset consists of polysomnography recordings intended for sleep stage classification.\\nIt is part of the PhysioNet database and includes polysomnographic sleep recordings that have been widely used\\nto analyze sleep patterns and stages. For our analysis, we extracted the brain EEG signals.\\n–MIT-BIH Arrhythmia Dataset: Another significant dataset from PhysioNet, the MIT-BIH Arrhythmia Dataset, is\\ncomposed of electrocardiogram (ECG) recordings used primarily for arrhythmia detection and classification. It is\\none of the most extensively used datasets for validating arrhythmia detection algorithms, offering a comprehensive\\ncollection of annotated heartbeats and arrhythmia examples.\\nA summary of the characteristics of these two datasets is presented in Table 6.\\n•Human Activity Recognition datasets: Human activity recognition (HAR) using sensor data is a vital application of\\ntime series analysis, with implications for health monitoring, elder care, and fitness tracking. In this study, we evaluate\\nour model using three prominent HAR datasets: UCIHAR, WISDM, and HHAR.\\n14', metadata={'source': 'rpaper.pdf', 'page': 13}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\n–UCI Human Activity Recognition Using Smartphones (UCIHAR): This dataset is collected from experiments\\nthat were carried out with a group of 30 volunteers performing six activities (walking, walking upstairs, walking\\ndownstairs, sitting, standing, and laying) while wearing a smartphone on the waist. The smartphone’s embedded\\naccelerometer and gyroscope captured 3-axial linear acceleration and 3-axial angular velocity, respectively.\\n–Wireless Sensor Data Mining (WISDM): The WISDM dataset includes time series data from smartphone\\nsensors and wearable devices, capturing various human activities such as walking, jogging, sitting, and standing.\\nIt provides a diverse set of user-generated activity data, making it suitable for testing the robustness of HAR\\nmodels across different motion patterns and sensor placements.\\n–Heterogeneity Human Activity Recognition (HHAR): HHAR dataset stands out due to its collection from\\nmultiple device types, including smartphones and smartwatches, across different individuals performing activities\\nlike biking, sitting, standing, walking, stair climbing, and more. Its heterogeneity in terms of device types and\\npositions offers a challenging benchmark for assessing a model’s ability to generalize across various sensor\\nconfigurations and activity types. Here, we utilized the data from the Samsung devices.\\nA summary of the characteristics of these three datasets is presented in Table 7.\\nTable 6: A description of characteristics of the biomedical datasets used in our experiments.\\nDataset # Train # Test Length # Channel # Class\\nSleep EEG 25,612 8,910 3,000 2 5\\nArrhythmia ECG 70,043 21,892 187 1 2\\nTable 7: A description of characteristics of the Human Activity Recognition datasets used in our experiments.\\nDataset # Train # Test Length # Channel # Class\\nUCIHAR 7,352 2,947 128 9 6\\nWISDM 4,731 2,561 128 3 6\\nHHAR 10,336 4,436 128 3 6\\nE.3. Forecasting\\nOur study leverages a diverse set of forecasting datasets to evaluate the effectiveness of our model across various domains:\\n•Electricity: This dataset contains electricity consumption records from 321 clients, offering insights into usage patterns\\nand enabling demand forecasting, crucial for optimizing power generation and distribution.\\n•ETT (Electricity Transformer Temperature) datasets: The ETTh1, ETTh2, ETTm1, and ETTm2 datasets provide\\ndata on the temperature of electricity transformers and the load, facilitating the prediction of future temperatures and\\nloads based on past patterns. These datasets vary in granularity, with ”h” indicating hourly data and ”m” indicating\\n15-minute intervals, offering a range of temporal resolutions for forecasting challenges.\\n•Exchange Rate: Featuring daily exchange rates of different currencies against the US dollar, this dataset is vital for\\nfinancial forecasting, enabling models to anticipate currency fluctuations based on historical data.\\n•Traffic: Traffic dataset consists of hourly interstate 94 Westbound traffic volume for the Twin Cities (Minneapolis-St.\\nPaul) metropolitan area, allowing for the prediction of traffic flow patterns, essential for urban planning and congestion\\nmanagement.\\n•Weather: This dataset includes hourly weather conditions and atmospheric measurements from a weather station,\\nsupporting forecasts of various weather phenomena, crucial for agriculture, transportation, and daily life planning.\\nWe describe the characteristics of these datasets in Table 8.\\n15', metadata={'source': 'rpaper.pdf', 'page': 14}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 8: Descriptions of the forecasting datasets. Dim shows the variate number of each dataset. Dataset Size indicates the\\nsize of the (Train, Validation, Test) split respectively. Frequency denotes the sampling interval of time points.\\nDataset Dim Dataset Size Frequency Information\\nECL 321 (18317, 2633, 5261) Hourly Electricity\\nETTh1, ETTh2 7 (8545, 2881, 2881) Hourly Electricity\\nETTm1, ETTm2 7 (34465, 11521, 11521) 15min Electricity\\nExchange 8 (5120, 665, 1422) Daily Economy\\nTraffic 862 (12185, 1757, 3509) Hourly Transportation\\nWeather 21 (36792, 5271, 10540) 10min Weather\\nE.4. Anomaly Detection\\nAnomaly detection plays a pivotal role across various domains, enabling the identification of unusual patterns that may\\nindicate critical incidents, such as system failures, security breaches, or environmental changes. In our study, we assess the\\nperformance of our model using five benchmark datasets, each representing a distinct application area, to demonstrate its\\neffectiveness in detecting anomalies in diverse settings:\\n•SMD (Server Machine Dataset): Utilized for server monitoring, the SMD dataset comprises multivariate time series\\ndata collected from servers and aims to identify unusual server behaviors that could indicate failures or security issues.\\n•MSL (Mars Science Laboratory): This dataset contains telemetry data from the Mars Science Laboratory rover,\\nfocusing on space exploration applications. Anomaly detection in this context is crucial for identifying potential issues\\nwith spacecraft systems based on their operational data.\\n•SMAP (Soil Moisture Active Passive): Related to earth observations, the SMAP dataset includes soil moisture\\nmeasurements intended for environmental monitoring. Detecting anomalies in soil moisture can provide insights into\\nenvironmental conditions and potential agricultural impacts.\\n•SWaT (Secure Water Treatment): In the domain of water treatment security, the SWaT dataset consists of data from\\na water treatment testbed, simulating the operational data of water treatment plants. Anomaly detection here is vital for\\nensuring the safety and security of water treatment processes.\\n•PSM (Pump Sensor Monitoring): Focused on industrial pump sensors, the PSM dataset gathers sensor data from\\npumps in industrial settings. Anomalies in this dataset can indicate equipment malfunctions or the need for maintenance,\\ncritical for preventing industrial accidents.\\nThe detailed characteristics of these datasets is presented in Table 9.\\nTable 9: Descriptions of the Anomaly detection datasets. Dim shows the variate number of each dataset. Dataset Size\\nindicates the size of the (Train, Validation, Test) split respectively. Frequency denotes the sampling interval of time points.\\nDataset Dim Length Dataset Size Information\\nSMD 38 100 (566724, 141681, 708420) Server Machine\\nMSL 55 100 (44653, 11664, 73729) Spacecraft\\nSMAP 25 100 (108146, 27037, 427617) Spacecraft\\nSWaT 51 100 (396000, 99000, 449919) Infrastructure\\nPSM 25 100 (105984, 26497, 87841) Server Machine\\nF. Full Results\\nF.1. Classification\\n16', metadata={'source': 'rpaper.pdf', 'page': 15}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nDataset TSLANet GPT4TS TimesNet ROCKET CrossF. Pat.TST MLP TS-TCC TS2VEC\\nAdiac 80.56 52.69 24.04 78.52 58.31 34.78 61.38 76.57 72.89\\nArrowHead 80.57 66.29 49.71 77.31 73.71 72.57 75.43 62.20 77.71\\nBeef 90.00 66.67 60.00 67.33 73.33 76.67 73.33 47.32 76.67\\nBeetleFly 90.00 85.00 80.00 88.00 85.00 80.00 80.00 31.25 85.00\\nBirdChicken 100.00 85.00 60.00 84.50 85.00 80.00 75.00 75.00 80.00\\nCBF 97.56 92.00 92.22 89.67 89.56 85.11 83.44 90.79 88.33\\nCar 88.33 76.67 30.00 99.52 86.67 75.00 86.67 71.88 99.22\\nChlorineConcentration 85.94 61.25 55.21 69.40 61.72 56.56 61.72 57.40 71.85\\nCinC ECG torso 85.51 23.99 51.74 84.96 84.93 66.88 46.67 95.55 79.28\\nCoffee 100.00 100.00 53.57 100.00 100.00 100.00 100.00 95.83 100.00\\nComputers 68.40 52.00 62.40 66.48 63.20 69.60 58.00 61.95 60.40\\nCricket X 76.15 6.41 55.64 77.31 41.79 45.38 32.56 77.25 76.15\\nCricket Y 78.72 49.74 55.90 79.15 47.69 43.59 42.31 75.75 73.08\\nCricket Z 80.00 8.21 57.44 79.33 41.79 47.95 32.56 75.83 76.92\\nDiatomSizeReduction 92.16 88.89 48.69 97.68 95.75 91.18 93.14 95.94 97.71\\nDistalPhalanxOutlineAgeGroup 86.50 86.50 80.25 76.12 80.75 82.00 80.50 85.25 81.25\\nDistalPhalanxOutlineCorrect 80.67 75.67 73.67 75.68 76.17 78.50 75.50 80.76 81.17\\nDistalPhalanxTW 80.50 78.25 77.25 70.07 79.25 79.00 79.50 79.50 78.00\\nEarthquakes 82.30 38.82 23.60 75.32 82.30 80.75 59.94 75.89 72.36\\nECG200 88.00 85.00 90.00 84.90 86.00 89.00 84.00 87.50 87.00\\nECG5000 94.62 93.40 93.47 94.72 94.36 93.87 94.18 94.19 93.33\\nECGFiveDays 99.30 94.77 83.74 100.00 98.49 86.41 96.63 90.71 100.00\\nElectricDevices 68.28 56.36 68.58 66.84 61.87 74.66 48.22 69.31 68.10\\nFaceAll 82.31 37.22 73.61 93.33 90.53 79.94 78.64 76.99 79.17\\nFaceFour 94.32 7.95 52.27 77.39 93.18 86.36 82.95 85.42 94.32\\nFacesUCR 92.39 82.88 46.00 94.81 83.07 77.46 74.39 92.93 94.24\\nFiftyWords 80.00 36.48 61.32 76.92 62.86 55.16 58.90 77.62 79.12\\nFISH 94.29 71.43 59.43 96.86 84.57 71.43 87.43 61.29 93.14\\nFordA 93.06 50.49 66.20 90.61 70.62 50.90 51.32 92.35 89.28\\nFordB 91.39 61.99 54.43 77.53 52.70 52.20 51.16 91.72 83.50\\nGun Point 99.33 90.00 87.33 99.33 89.33 94.00 85.33 93.33 98.00\\nHam 80.00 51.43 65.71 69.43 78.10 73.33 77.14 75.00 72.38\\nHandOutlines 88.90 36.20 86.30 94.35 86.00 85.20 86.40 85.81 85.70\\nHaptics 47.73 26.95 37.01 50.84 43.83 41.23 46.10 44.06 43.51\\nHerring 67.19 40.63 59.38 64.38 68.75 64.06 70.31 60.94 64.06\\nInlineSkate 36.73 18.91 25.82 39.64 30.91 29.45 27.82 29.76 38.55\\nInsectWingbeatSound 66.36 63.23 60.00 63.92 64.29 57.83 64.75 66.52 63.79\\nItalyPowerDemand 97.08 96.89 97.08 97.17 97.28 96.60 96.89 96.44 95.63\\nLargeKitchenAppliances 81.87 33.33 47.20 81.47 53.87 63.20 42.13 76.08 86.40\\nLighting2 83.61 54.10 72.13 73.61 75.41 75.41 67.21 73.56 86.89\\nLighting7 83.56 53.42 72.60 68.63 72.60 67.12 64.38 81.53 83.56\\nMALLAT 94.71 91.86 54.50 94.12 93.48 84.01 95.05 91.11 89.13\\nMeat 93.33 50.00 33.33 93.33 88.33 91.67 80.00 31.25 91.67\\nMedicalImages 72.76 61.18 58.95 75.42 65.79 63.03 59.61 74.35 75.79\\nMiddlePhalanxOutlineAgeGroup 81.25 74.50 78.75 83.64 80.75 79.75 80.75 78.25 75.25\\nMiddlePhalanxOutlineCorrect 84.00 64.67 64.67 61.36 64.50 64.83 64.50 52.47 71.67\\nMiddlePhalanxTW 65.91 64.91 64.66 53.77 64.66 64.16 65.16 56.10 61.65\\nMoteStrain 93.13 87.14 88.34 83.49 87.22 89.54 86.74 85.28 87.86\\nNonInvasiveFatalECG Thorax1 93.44 72.98 81.58 95.65 86.97 78.73 92.98 84.58 90.48\\nNonInvasiveFatalECG Thorax2 93.74 88.04 84.38 95.59 90.53 85.24 93.49 82.50 93.74\\nOliveOil 40.00 40.00 40.00 80.33 60.00 83.33 70.00 42.86 90.00\\nOSULeaf 74.79 9.50 43.39 82.89 49.59 42.15 45.04 63.28 76.86\\nPhalangesOutlinesCorrect 82.40 77.04 68.30 83.11 69.35 65.97 66.90 78.73 80.77\\nPhoneme 27.27 3.22 9.70 20.92 11.23 9.12 8.60 30.04 26.79\\nPlane 100.00 97.14 98.10 100.00 98.10 99.05 97.14 96.43 100.00\\nProximalPhalanxOutlineAgeGroup 88.29 83.90 86.34 90.17 86.34 86.34 85.85 73.34 81.95\\nProximalPhalanxOutlineCorrect 91.75 81.79 77.66 86.59 84.54 78.01 81.79 87.17 87.29\\nProximalPhalanxTW 83.00 81.50 81.75 78.98 80.00 80.25 82.75 72.75 79.00\\nRefrigerationDevices 55.47 33.60 33.60 50.40 42.40 45.87 38.67 49.74 51.20\\nScreenType 44.80 37.07 44.00 41.55 45.07 44.80 40.27 39.99 40.00\\nShapeletSim 90.00 49.44 50.00 65.72 57.22 56.67 56.67 61.98 87.78\\nShapesAll 85.17 61.17 64.33 86.63 68.17 61.00 61.83 79.11 88.00\\n17', metadata={'source': 'rpaper.pdf', 'page': 16}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nSmallKitchenAppliances 76.27 33.33 45.60 62.13 55.47 61.60 41.33 74.74 71.20\\nSonyAIBORobotSurface 85.86 42.93 70.55 93.16 81.03 83.69 70.38 68.46 89.18\\nSonyAIBORobotSurfaceII 92.44 70.30 85.62 91.26 85.73 85.73 85.10 86.15 90.66\\nStarLightCurves 97.41 92.70 89.22 97.63 92.74 86.09 91.96 96.80 96.28\\nStrawberry 98.37 94.94 93.15 97.84 94.45 93.15 95.76 93.59 96.57\\nSwedishLeaf 96.16 88.32 83.40 96.10 82.08 76.64 80.96 92.31 93.60\\nSymbols 94.07 16.98 86.43 96.71 86.13 82.21 84.72 86.08 96.58\\nSynthetic control 100.00 97.67 99.67 99.53 93.67 99.67 87.00 99.67 99.67\\nToeSegmentation1 87.72 52.63 61.40 94.21 62.28 66.23 60.09 78.75 92.11\\nToeSegmentation2 90.00 75.38 86.15 91.00 81.54 76.92 58.46 59.72 87.69\\nTrace 100.00 68.00 66.00 100.00 74.00 100.00 67.00 97.32 100.00\\nTwoLeadECG 93.85 76.56 68.74 100.00 86.65 84.55 91.48 81.63 99.78\\nTwo Patterns 100.00 99.58 98.00 100.00 79.90 93.23 84.13 100.00 99.21\\nuWaveGestureLibrary X 82.80 69.65 69.37 82.64 66.78 65.02 64.77 80.97 77.89\\nuWaveGestureLibrary Y 73.53 54.22 62.67 73.83 61.33 55.11 60.19 71.22 67.87\\nuWaveGestureLibrary Z 75.15 59.27 60.44 75.05 59.35 55.05 56.98 72.92 72.67\\nuWaveGestureLibraryAll 97.57 85.54 90.28 97.20 88.02 87.58 88.22 96.54 91.99\\nwafer 99.81 99.58 99.71 99.84 98.39 99.63 94.78 99.69 99.85\\nWine 66.67 53.70 50.00 71.30 68.52 77.78 72.22 57.81 85.19\\nWordsSynonyms 69.28 7.37 50.16 71.30 56.90 49.06 44.83 66.12 68.81\\nWorms 60.77 17.68 43.65 65.97 34.25 34.81 31.49 51.98 55.80\\nWormsTwoClass 77.35 58.01 62.98 76.62 62.43 60.77 58.01 64.48 69.61\\nyoga 85.83 71.83 67.77 90.49 73.87 68.43 65.13 77.46 84.23\\nAverage 83.18 61.58 65.27 81.42 73.47 71.84 69.68 75.07 81.42\\n1st count 38 0 1 27 2 2 2 4 9\\nTable 10: Full classification results on the UCR datasets in terms of accuracy (as %).\\nTable 11: Full classification results on the UEA datasets in terms of accuracy (as %).\\nDataset TSLANet GPT4TS TimesNet ROCKET CrossF. PatchTST MLP TS-TCC TS2VEC\\nArticularyWordRecognition 99.00 93.33 96.18 99.33 98.00 97.67 97.33 98.00 87.33\\nAtrialFibrillation 40.00 33.33 33.33 20.00 46.66 53.33 46.66 33.33 53.33\\nBasicMotions 100.00 92.50 100.00 100.00 90.00 92.50 85.00 100.00 92.50\\nCricket 98.61 8.33 87.50 98.61 84.72 84.72 91.67 93.06 65.28\\nEpilepsy 98.55 85.51 78.13 98.55 73.19 65.94 60.14 97.10 62.32\\nEthanolConcentration 30.42 25.48 27.73 42.58 34.98 28.90 33.46 32.32 40.68\\nFaceDetection 66.77 65.58 67.47 64.70 66.17 68.96 67.42 63.05 50.96\\nFingerMovements 61.00 57.00 59.38 61.00 64.00 62.00 64.00 44.00 51.00\\nHandMovementDirection 52.70 18.92 50.00 50.00 58.11 58.11 58.11 64.86 32.43\\nHandwriting 57.88 3.76 26.18 48.47 26.24 26.00 22.47 47.76 15.53\\nHeartbeat 77.56 36.59 74.48 69.76 76.59 76.59 73.17 77.07 69.76\\nInsectWingbeat 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00\\nJapaneseV owels 99.19 98.11 97.83 95.68 98.92 98.65 97.84 97.30 90.00\\nLibras 92.78 79.44 77.84 83.89 76.11 81.11 73.33 86.67 85.56\\nLSST 66.34 46.39 59.21 54.10 42.82 67.80 35.77 49.23 39.01\\nMotorImagery 62.00 50.00 51.04 53.00 61.00 61.00 61.00 47.00 47.00\\nNATOPS 95.56 91.67 81.82 83.33 88.33 96.67 93.89 96.11 82.22\\nPEMS-SF 83.82 87.28 88.13 75.10 82.08 88.44 82.08 86.71 72.25\\nPenDigits 98.94 97.74 98.19 97.34 93.65 99.23 92.94 98.51 97.40\\nPhonemeSpectra 17.75 3.01 18.24 17.60 7.55 11.69 7.10 25.92 8.23\\nRacketSports 90.79 76.97 82.64 86.18 81.58 84.21 78.95 84.87 74.34\\nSelfRegulationSCP1 91.81 91.47 77.43 84.64 92.49 89.76 88.40 91.13 77.13\\nSelfRegulationSCP2 61.67 51.67 52.84 54.44 53.33 54.44 51.67 53.89 51.11\\nSpokenArabicDigits 99.91 99.36 98.36 99.20 96.41 99.68 96.68 99.77 85.27\\nStandWalkJump 46.67 33.33 53.33 46.67 53.33 60.00 60.00 40.00 46.67\\nUWaveGestureLibrary 91.25 84.38 83.13 94.40 81.56 80.00 81.88 86.25 62.81\\nAverage 72.73 58.51 66.55 68.79 66.84 69.13 65.81 69.38 59.62\\n1st count 12 0 0 3 2 7 0 2 0\\n18', metadata={'source': 'rpaper.pdf', 'page': 17}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 12: Full classification results on the human activity recognition and biomedical signal datasets in terms of accuracy\\n(as %).\\nDataset TSLANet GPT4TS TimesNet ROCKET CrossF. PatchTST MLP TS-TCC TS2VEC\\nUCIHAR 96.06 91.24 91.34 94.37 76.59 92.70 63.49 95.95 96.19\\nWISDM 97.77 89.49 89.61 97.03 77.31 95.94 58.88 97.05 93.87\\nHHAR 98.53 97.40 93.59 97.93 78.74 95.96 47.70 98.49 97.05\\nAverage 97.46 92.71 91.51 96.44 77.55 94.87 56.69 97.16 95.70\\nEEG 82.10 76.37 75.86 76.69 53.30 69.69 49.70 86.06 75.13\\nECG 98.37 97.70 98.33 97.72 88.33 98.06 91.57 98.44 97.48\\nAverage 90.24 87.04 87.10 87.20 70.82 83.87 70.63 92.25 86.31\\nF.2. Forecasting\\nTable 13: Full forecasting results on different prediction lengths ∈ {96,192,336,720}. Lower MSE indicates better\\nperformance.\\nMethods TSLANet Time-LLM iTransformer PatchTST Crossformer FEDformer Autoformer RLinear Dlinear TimesNet GPT4TS SCINet\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEECL96 0.136 0.229 0.131 0.224 0.148 0.240 0.138 0.230 0.219 0.314 0.193 0.308 0.201 0.317 0.201 0.281 0.140 0.237 0.168 0.272 0.139 0.238 0.247 0.345\\n192 0.152 0.244 0.152 0.241 0.162 0.253 0.149 0.243 0.231 0.322 0.201 0.315 0.222 0.334 0.201 0.283 0.153 0.249 0.184 0.289 0.153 0.251 0.257 0.355\\n336 0.168 0.262 0.160 0.248 0.178 0.269 0.169 0.262 0.246 0.337 0.214 0.329 0.231 0.338 0.215 0.298 0.169 0.267 0.198 0.300 0.169 0.266 0.269 0.369\\n720 0.205 0.293 0.192 0.298 0.225 0.317 0.211 0.299 0.280 0.363 0.246 0.355 0.254 0.361 0.257 0.331 0.203 0.301 0.220 0.320 0.206 0.297 0.299 0.390\\nAvg 0.165 0.257 0.158 0.252 0.178 0.270 0.167 0.259 0.244 0.334 0.214 0.327 0.227 0.338 0.219 0.298 0.166 0.264 0.193 0.295 0.167 0.263 0.268 0.365ETTh 196 0.370 0.394 0.362 0.392 0.386 0.405 0.382 0.401 0.423 0.448 0.376 0.419 0.449 0.459 0.386 0.395 0.375 0.399 0.384 0.402 0.376 0.397 0.654 0.599\\n192 0.412 0.417 0.398 0.418 0.441 0.436 0.428 0.425 0.471 0.474 0.420 0.448 0.500 0.482 0.437 0.424 0.405 0.416 0.436 0.429 0.416 0.418 0.719 0.631\\n336 0.399 0.416 0.430 0.427 0.487 0.458 0.451 0.436 0.570 0.546 0.459 0.465 0.521 0.496 0.479 0.446 0.439 0.443 0.491 0.469 0.442 0.433 0.778 0.659\\n720 0.472 0.475 0.442 0.457 0.503 0.491 0.452 0.459 0.653 0.621 0.506 0.507 0.514 0.512 0.481 0.470 0.472 0.490 0.521 0.500 0.477 0.456 0.836 0.699\\nAvg 0.413 0.426 0.408 0.423 0.454 0.448 0.428 0.430 0.529 0.522 0.440 0.460 0.496 0.487 0.446 0.434 0.423 0.437 0.458 0.450 0.428 0.426 0.747 0.647ETTh 296 0.280 0.341 0.268 0.328 0.297 0.349 0.285 0.340 0.745 0.584 0.358 0.397 0.346 0.388 0.288 0.338 0.289 0.353 0.340 0.374 0.285 0.342 0.707 0.621\\n192 0.330 0.375 0.329 0.375 0.380 0.400 0.356 0.386 0.877 0.656 0.429 0.439 0.456 0.452 0.374 0.390 0.383 0.418 0.402 0.414 0.354 0.389 0.860 0.689\\n336 0.317 0.374 0.368 0.409 0.428 0.432 0.350 0.395 1.043 0.731 0.496 0.487 0.482 0.486 0.415 0.426 0.448 0.465 0.452 0.452 0.373 0.407 1.000 0.744\\n720 0.404 0.440 0.372 0.420 0.427 0.445 0.395 0.427 1.104 0.763 0.463 0.474 0.515 0.511 0.420 0.440 0.605 0.551 0.462 0.468 0.406 0.441 1.249 0.838\\nAvg 0.333 0.383 0.334 0.383 0.383 0.407 0.347 0.387 0.942 0.684 0.437 0.449 0.450 0.459 0.374 0.399 0.431 0.447 0.414 0.427 0.355 0.395 0.954 0.723ETTm 196 0.289 0.349 0.272 0.334 0.334 0.368 0.291 0.340 0.404 0.426 0.379 0.419 0.505 0.475 0.355 0.376 0.299 0.343 0.338 0.375 0.292 0.346 0.418 0.438\\n192 0.328 0.370 0.310 0.358 0.377 0.391 0.328 0.365 0.450 0.451 0.426 0.441 0.553 0.496 0.391 0.392 0.335 0.365 0.374 0.387 0.332 0.372 0.439 0.450\\n336 0.355 0.389 0.352 0.384 0.426 0.420 0.365 0.389 0.532 0.515 0.445 0.459 0.621 0.537 0.424 0.415 0.369 0.386 0.410 0.411 0.366 0.394 0.490 0.485\\n720 0.421 0.425 0.383 0.411 0.491 0.459 0.422 0.423 0.666 0.589 0.543 0.490 0.671 0.561 0.487 0.450 0.425 0.421 0.478 0.450 0.417 0.421 0.595 0.550\\nAvg 0.348 0.383 0.329 0.372 0.407 0.410 0.352 0.379 0.513 0.495 0.448 0.452 0.588 0.517 0.414 0.408 0.357 0.379 0.400 0.406 0.352 0.383 0.486 0.481ETTm 296 0.169 0.259 0.161 0.253 0.180 0.264 0.169 0.254 0.287 0.366 0.203 0.287 0.255 0.339 0.182 0.265 0.167 0.260 0.187 0.267 0.173 0.262 0.286 0.377\\n192 0.224 0.297 0.219 0.293 0.250 0.309 0.230 0.294 0.414 0.492 0.269 0.328 0.281 0.340 0.246 0.304 0.224 0.303 0.249 0.309 0.229 0.301 0.399 0.445\\n336 0.275 0.329 0.271 0.329 0.311 0.348 0.280 0.329 0.597 0.542 0.325 0.366 0.339 0.372 0.307 0.342 0.281 0.342 0.321 0.351 0.286 0.341 0.637 0.591\\n720 0.354 0.380 0.352 0.379 0.412 0.407 0.378 0.386 1.730 1.042 0.421 0.415 0.433 0.432 0.407 0.398 0.397 0.421 0.408 0.403 0.378 0.401 0.960 0.735\\nAvg 0.256 0.316 0.251 0.313 0.288 0.332 0.264 0.316 0.757 0.611 0.305 0.349 0.327 0.371 0.286 0.327 0.267 0.332 0.291 0.333 0.267 0.326 0.571 0.537Exchange96 0.083 0.201 - - 0.086 0.206 0.088 0.205 0.256 0.367 0.148 0.278 0.197 0.323 0.093 0.217 0.081 0.203 0.107 0.234 0.082 0.199 0.267 0.396\\n192 0.177 0.299 - - 0.177 0.299 0.176 0.299 0.470 0.509 0.271 0.315 0.300 0.369 0.184 0.307 0.157 0.293 0.226 0.344 0.171 0.293 0.351 0.459\\n336 0.331 0.417 - - 0.331 0.417 0.301 0.397 1.268 0.883 0.460 0.427 0.509 0.524 0.351 0.432 0.305 0.414 0.367 0.448 0.354 0.428 1.324 0.853\\n720 0.888 0.739 - - 0.847 0.691 0.901 0.714 1.767 1.068 1.195 0.695 1.447 0.941 0.886 0.714 0.643 0.601 0.964 0.746 0.877 0.704 1.058 0.797\\nAvg 0.370 0.414 - - 0.360 0.403 0.367 0.404 0.940 0.707 0.519 0.429 0.613 0.539 0.379 0.418 0.297 0.378 0.416 0.443 0.371 0.406 0.750 0.626Traffic96 0.372 0.261 0.362 0.248 0.395 0.268 0.401 0.267 0.522 0.290 0.587 0.366 0.613 0.388 0.649 0.389 0.410 0.282 0.593 0.321 0.388 0.282 0.788 0.499\\n192 0.388 0.266 0.374 0.247 0.417 0.276 0.406 0.268 0.530 0.293 0.604 0.373 0.616 0.382 0.601 0.366 0.423 0.287 0.617 0.336 0.407 0.290 0.789 0.505\\n336 0.394 0.269 0.385 0.271 0.433 0.283 0.421 0.277 0.558 0.305 0.621 0.383 0.622 0.337 0.609 0.369 0.436 0.296 0.629 0.336 0.412 0.294 0.797 0.508\\n720 0.430 0.289 0.43 0.288 0.467 0.302 0.452 0.297 0.589 0.328 0.626 0.382 0.660 0.408 0.647 0.387 0.466 0.315 0.640 0.350 0.450 0.312 0.841 0.523\\nAvg 0.396 0.271 0.388 0.264 0.428 0.282 0.420 0.277 0.550 0.304 0.610 0.376 0.628 0.379 0.627 0.378 0.434 0.295 0.620 0.336 0.414 0.295 0.804 0.509Weather96 0.148 0.197 0.147 0.201 0.174 0.214 0.160 0.204 0.158 0.230 0.217 0.296 0.266 0.336 0.192 0.232 0.176 0.237 0.172 0.220 0.162 0.212 0.221 0.306\\n192 0.193 0.241 0.189 0.234 0.221 0.254 0.204 0.245 0.206 0.277 0.276 0.336 0.307 0.367 0.240 0.271 0.220 0.282 0.219 0.261 0.204 0.248 0.261 0.340\\n336 0.245 0.282 0.262 0.279 0.278 0.296 0.257 0.285 0.272 0.335 0.339 0.380 0.359 0.395 0.292 0.307 0.265 0.319 0.280 0.306 0.254 0.286 0.309 0.378\\n720 0.325 0.337 0.304 0.316 0.358 0.349 0.329 0.338 0.398 0.418 0.403 0.428 0.419 0.428 0.364 0.353 0.323 0.362 0.365 0.359 0.326 0.337 0.377 0.427\\nAvg 0.228 0.264 0.225 0.257 0.258 0.278 0.238 0.268 0.259 0.315 0.309 0.360 0.338 0.382 0.272 0.291 0.246 0.300 0.259 0.287 0.237 0.271 0.292 0.363\\n19', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 14: Full results for the anomaly detection.\\nMethods SMD MSL SMAP SWaT PSM Avg F1\\nMetrics P R F1 P R F1 P R F1 P R F1 P R F1 %\\nTSLANet (Ours) 85.58 90.37 87.91 77.46 90.12 83.32 92.45 64.47 75.96 91.50 94.14 92.80 98.36 98.55 97.73 87.54\\nGPT4TS 88.89 84.98 86.89 82.00 82.91 82.45 90.60 60.95 72.88 92.20 96.34 94.23 98.62 95.68 97.13 86.72\\nTimesNet 87.91 81.54 84.61 89.54 75.36 81.84 90.14 56.40 69.39 90.75 95.40 93.02 98.51 96.20 97.34 85.24\\nPatchTST 87.26 82.14 84.62 88.34 70.96 78.70 90.64 55.46 68.82 91.10 80.94 85.72 98.84 93.47 96.08 82.79\\nETSformer 87.44 79.23 83.13 85.13 84.93 85.03 92.25 55.75 69.50 90.02 80.36 84.91 99.31 85.28 91.76 82.87\\nFEDformer 87.95 82.39 85.08 77.14 80.07 78.57 90.47 58.10 70.76 90.17 96.42 93.19 97.31 97.16 97.23 84.97\\nLightTS 87.10 78.42 82.53 82.40 75.78 78.95 92.58 55.27 69.21 91.98 94.72 93.33 98.37 95.97 97.15 84.23\\nDLinear 83.62 71.52 77.10 84.34 85.42 84.88 92.32 55.41 69.26 80.91 95.30 87.52 98.28 89.26 93.55 82.46\\nStationary 88.33 81.21 84.62 68.55 89.14 77.50 89.37 59.02 71.09 68.03 96.75 79.88 97.82 96.76 97.29 82.08\\nAutoformer 88.06 82.35 85.11 77.27 80.92 79.05 90.40 58.62 71.12 89.85 95.81 92.74 99.08 88.15 93.29 84.26\\nPyraformer 85.61 80.61 83.04 83.81 85.93 84.86 92.34 57.71 71.09 87.92 96.00 91.78 71.67 96.02 82.08 82.57\\nAnomaly Transformer 88.91 82.23 85.49 79.61 87.37 83.31 91.85 58.11 71.18 72.51 97.32 83.10 68.35 94.72 79.40 80.50\\nInformer 86.60 77.23 81.65 81.77 86.48 84.06 90.11 57.13 69.92 70.29 96.75 81.43 64.27 96.33 77.10 78.83\\nReformer 82.58 69.24 75.32 85.51 83.31 84.40 90.91 57.44 70.40 72.50 96.53 82.80 59.93 95.38 73.61 77.31\\nLogTransformer 83.46 70.13 76.21 73.05 87.37 79.57 89.15 57.59 69.97 68.67 97.31 80.52 63.06 98.00 76.74 76.60\\nTransformer 83.58 76.13 79.56 71.57 87.37 78.68 89.37 57.12 69.70 68.84 96.53 80.37 62.75 96.56 76.07 76.88\\nF.3. Anomaly Detection\\nG. Future Work\\nTSLANet is aimed to be a foundation model for time series analysis. Therefore, we have some future directions toward\\nachieving this goal. These are summarized as follows.\\nLarge-Scale Pretraining We aim to explore the potential of TSLANet when pretrained on a diverse and large cohort\\nof datasets. This would enable us to assess the model’s generalization capabilities and its performance on few-shot and\\nzero-shot learning tasks. In addition, this would give our model an advantage in competing against LLM-pretrained models\\nin time series analysis.\\nBetter Pretraining Task We aim to develop other pretraining tasks beyond the current masking approach, which, while\\nstraightforward and effective for initial learning, presents limitations in fully capturing the complexity of time series data.\\nMasking may not adequately challenge the model to learn the intricate temporal dependencies and patterns essential for\\nadvanced classification and forecasting. This exploration will contribute to evolving TSLANet into a more refined and\\ncapable foundation model for time series analysis.\\nEnhanced Noise Reduction Techniques Building upon the adaptive spectral filtering capabilities of TSLANet, future\\nwork could explore more sophisticated noise reduction techniques that can adapt to a wider variety of noise patterns and\\ndistributions, as well as be adept to the quick fluctuations in short-term forecasting problems.\\n20', metadata={'source': 'rpaper.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF based loader\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('rpaper.pdf')\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nEmadeldeen Eldele1Mohamed Ragab1 2Zhenghua Chen1 2Min Wu2Xiaoli Li1 2\\nAbstract\\nTime series data, characterized by its intrin-\\nsic long and short-range dependencies, poses a\\nunique challenge across analytical applications.\\nWhile Transformer-based models excel at cap-\\nturing long-range dependencies, they face limita-\\ntions in noise sensitivity, computational efficiency,\\nand overfitting with smaller datasets. In response,\\nwe introduce a novel TimeSeries Lightweight\\nAdaptive Network ( TSLANet ), as a universal\\nconvolutional model for diverse time series tasks.\\nSpecifically, we propose an Adaptive Spectral\\nBlock, harnessing Fourier analysis to enhance\\nfeature representation and to capture both long-\\nterm and short-term interactions while mitigat-\\ning noise via adaptive thresholding. Additionally,\\nwe introduce an Interactive Convolution Block\\nand leverage self-supervised learning to refine', metadata={'source': 'rpaper.pdf', 'page': 0}),\n",
       " Document(page_content='term and short-term interactions while mitigat-\\ning noise via adaptive thresholding. Additionally,\\nwe introduce an Interactive Convolution Block\\nand leverage self-supervised learning to refine\\nthe capacity of TSLANet for decoding complex\\ntemporal patterns and improve its robustness on\\ndifferent datasets. Our comprehensive experi-\\nments demonstrate that TSLANet outperforms\\nstate-of-the-art models in various tasks spanning\\nclassification, forecasting, and anomaly detection,\\nshowcasing its resilience and adaptability across\\na spectrum of noise levels and data sizes. The\\ncode is available at https://github.com/\\nemadeldeen24/TSLANet\\n1. Introduction\\nTime series data, known for its sequential nature and tempo-\\nral dependencies, is ubiquitous across numerous domains,\\nincluding finance, healthcare, and environmental monitor-\\ning. Recently, the Transformer model (Vaswani et al., 2017),\\noriginally renowned for its breakthroughs in natural lan-', metadata={'source': 'rpaper.pdf', 'page': 0}),\n",
       " Document(page_content='including finance, healthcare, and environmental monitor-\\ning. Recently, the Transformer model (Vaswani et al., 2017),\\noriginally renowned for its breakthroughs in natural lan-\\nguage processing, has been adapted as a potent tool for\\nanalyzing time series data. This was motivated by its abil-\\n1Centre for Frontier AI Research, Agency for Science, Tech-\\nnology and Research, Singapore2I2R, Agency for Science, Tech-\\nnology and Research, Singapore. Correspondence to: Emadeldeen\\nEldele <emad0002@ntu.edu.sg >.\\nClassifcation Accuracy\\n Forecasting MSE (Weather)\\n Forecasting MSE (ETTh1)\\n0.10.20.30.40.50.60.7CNN\\nFEDformerPatchTST\\nCrossformerAutoformerFigure 1: A comparison between CNN and Transformer-\\nbased architectures for classification and forecasting tasks.\\nClassification results are the average over 10 UEA datasets\\n(Wu et al., 2023), while forecasting results are the average\\nMSE results on lengths {96, 192, 336, 720 }.\\nity to capture long-range dependencies and interactions', metadata={'source': 'rpaper.pdf', 'page': 0}),\n",
       " Document(page_content='(Wu et al., 2023), while forecasting results are the average\\nMSE results on lengths {96, 192, 336, 720 }.\\nity to capture long-range dependencies and interactions\\nwithin time series data, showing proficiency in forecasting\\ntasks (Wu et al., 2021b; Zhou et al., 2022; Liu et al., 2024).\\nDespite the initial success of Transformers in time series\\nforecasting, they encounter hurdles when deployed across\\ndiverse time series tasks, particularly those with smaller\\ndatasets. This can be attributed to its large parameter size,\\nwhich may lead to overfitting and computational inefficiency\\nproblems (Wen et al., 2023). In addition, their attention\\nmechanism often struggles with the inherent noise and re-\\ndundancy in time series data (Li et al., 2022). Moreover, re-\\ncent works have questioned their adaptability, as highlighted\\nby (Zeng et al., 2023; Li et al., 2023). They observed that the\\nself-attention within Transformers is inherently permutation-', metadata={'source': 'rpaper.pdf', 'page': 0}),\n",
       " Document(page_content='cent works have questioned their adaptability, as highlighted\\nby (Zeng et al., 2023; Li et al., 2023). They observed that the\\nself-attention within Transformers is inherently permutation-\\ninvariant, which compromises the preservation of temporal\\ninformation. Their experiments showed that a single linear\\nlayer surprisingly outperforms the complex Transformer\\narchitectures for time series forecasting. However, while\\nsuch linear models can perform well for small, clean data,\\nthey may not be able to handle complex, noisy time series.\\nIn this work, we pivot from the prevalent focus on Multi-\\nLayer Perceptrons (MLPs) and Transformers to tackle the\\npotential of convolutional operations for time series analysis.\\nConvolutional Neural Networks (CNNs) have traditionally\\nexcelled in capturing short-term patterns within time series\\ndue to their local receptive fields, which serve as a strength\\n1arXiv:2404.08472v1  [cs.LG]  12 Apr 2024', metadata={'source': 'rpaper.pdf', 'page': 0}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nin classification tasks. Indeed, as illustrated in Figure 1, a\\nstraightforward 3-layer CNN network demonstrates supe-\\nrior performance in classification compared to state-of-the-\\nart Transformer-based architectures. Yet, our experiment\\nshowed that the efficacy of CNNs in forecasting varies with\\nthe data frequency. For instance, the CNN shows compet-\\nitive performance to these Transformer-based models on\\nthe Weather dataset featuring a short 10-minute frequency\\nbut struggles with the longer hourly ETTh1 dataset, indicat-\\ning a difficulty with less frequent temporal changes. This\\ndiscrepancy highlights a critical question: How can we en-\\nhance CNNs to extend their robust performance across a\\nwider spectrum of time series tasks? It becomes obvious\\nthat expanding the capabilities of CNNs can be achieved by\\nlearning both short-term and long-term dependencies within\\ntime series data.', metadata={'source': 'rpaper.pdf', 'page': 1}),\n",
       " Document(page_content='wider spectrum of time series tasks? It becomes obvious\\nthat expanding the capabilities of CNNs can be achieved by\\nlearning both short-term and long-term dependencies within\\ntime series data.\\nTo this end, we introduce TimeSeries Lightweight Adaptive\\nNetwork ( TSLANet ), a universal architecture for various\\ntime series tasks. TSLANet inherits the multi-block design\\nof the Transformer to allow scalability. However, we re-\\nplace the computationally expensive self-attention with a\\nlightweight Adaptive Spectral Block (ASB) featuring two\\nkey objectives. Firstly, ASB aims to encompass the entire\\nfrequency spectrum, thereby adeptly capturing both long-\\nterm and short-term interactions within the data. This is\\nachieved via Fourier-based multiplications by global and\\nlocal filters, akin to circular convolutions. Secondly, ASB se-\\nlectively attenuates high frequencies via an adaptive thresh-\\nolding approach, a strategy aimed at minimizing noise and', metadata={'source': 'rpaper.pdf', 'page': 1}),\n",
       " Document(page_content='local filters, akin to circular convolutions. Secondly, ASB se-\\nlectively attenuates high frequencies via an adaptive thresh-\\nolding approach, a strategy aimed at minimizing noise and\\nenhancing the clarity of the signal. In addition, we further\\nadvance our model by replacing the standard feed-forward\\nnetwork with an Interactive Convolutional Block, where\\nCNNs with different kernel sizes control each other to en-\\nrich the ability of the model to capture and interpret complex\\ntemporal patterns. Finally, we employ a per-dataset self-\\nsupervised pretraining to enhance the model capabilities,\\nespecially on large datasets.\\nThe proposed model is lightweight and enjoys the\\nO(NlogN)complexity of the Fast Fourier Transform\\n(FFT) operations, demonstrating superior efficiency and\\nspeed compared to self-attention (see Section 5.4). A\\nsummary comparison against CNN-based and Transformer-\\nbased models is also provided in Table 1. The contributions\\nof this paper can be summarized as follows:', metadata={'source': 'rpaper.pdf', 'page': 1}),\n",
       " Document(page_content='summary comparison against CNN-based and Transformer-\\nbased models is also provided in Table 1. The contributions\\nof this paper can be summarized as follows:\\n•We propose a universal lightweight model ( TSLANet ),\\ndesigned to adapt seamlessly to a myriad of time series\\ntasks. Through computationally efficient convolution\\noperations, TSLANet learns both long- and short-term\\nrelationships within the data.\\n•We propose an Adaptive Spectral Block, which lever-\\nages the power of Fourier transform alongside global\\nand local filters to cover the whole frequency spectrum,Table 1: Comparison to different methods. ‘Local Depen-\\ndencies’ means the efficiency in capturing local features.\\nMethod Feature ExtractionLong-range\\nDependenciesLocal\\nDependenciesParameter\\nEfficiency\\nCNN Localized Convolution ✗ ✓ ✓\\nTransformer Self-Attention ✓ ✗ ✗\\nTSLANet Adaptive Spectral Convolution ✓ ✓ ✓\\nwhile adaptively removing high frequencies that tend\\nto introduce noises. In addition, we propose an Inter-', metadata={'source': 'rpaper.pdf', 'page': 1}),\n",
       " Document(page_content='Transformer Self-Attention ✓ ✗ ✗\\nTSLANet Adaptive Spectral Convolution ✓ ✓ ✓\\nwhile adaptively removing high frequencies that tend\\nto introduce noises. In addition, we propose an Inter-\\nactive Convolution Block to learn intricate spatial and\\ntemporal features within data.\\n•TSLANet demonstrates superior performance against\\ndifferent state-of-the-art methods across various time\\nseries tasks.\\n2. Related Works\\nTransformer-based Networks. Since the advance of the\\nTransformer (Vaswani et al., 2017) for natural language\\nprocessing, numerous works have adopted it for time series\\nanalysis. For example, (Wu et al., 2021b; Zhou et al., 2022;\\nLi et al., 2021; Kitaev et al., 2020; Zhang & Yan, 2023) have\\nshowcased the Transformer capability to model interactions\\nwithin time series data, utilizing that for the forecasting task.\\nIn addition, Transformers with special design showed good\\nperformance in anomaly detection task (Xu et al., 2022).\\nYet, the efficacy of Transformers for time series has been', metadata={'source': 'rpaper.pdf', 'page': 1}),\n",
       " Document(page_content='In addition, Transformers with special design showed good\\nperformance in anomaly detection task (Xu et al., 2022).\\nYet, the efficacy of Transformers for time series has been\\ncontested. For instance, Zeng et al. (2023) argue that the\\npermutation-invariance property in Transformers may lead\\nto the loss of temporal information in time series. Following\\nthat, other MLP-based architectures showed efficacy in the\\ntime series forecasting task (Li et al., 2023; Ekambaram\\net al., 2023). Furthermore, Transformers demand extensive\\ncomputational resources in general, and they are prone to\\noverfitting when trained on smaller datasets (Wen et al.,\\n2023).\\nConvolution-based Networks. CNNs have showcased\\ntheir efficacy in time series analysis, particularly shining in\\nclassification tasks due to their adeptness at learning local\\npatterns (Dempster et al., 2020). CNNs also serve as the\\nbackbone for several time series representation learning\\nmethods, including TS-TCC (Eldele et al., 2021), TS2VEC', metadata={'source': 'rpaper.pdf', 'page': 1}),\n",
       " Document(page_content='patterns (Dempster et al., 2020). CNNs also serve as the\\nbackbone for several time series representation learning\\nmethods, including TS-TCC (Eldele et al., 2021), TS2VEC\\n(Yue et al., 2022), and MHCCL (Meng et al., 2023).\\nDespite their promise, CNNs often face challenges in fore-\\ncasting and anomaly detection, primarily due to their lim-\\nited ability to capture long-range dependencies. Therefore,\\nrecent works attempt to enhance CNN capabilities in dif-\\nferent ways. For instance, T-WaveNet (LIU et al., 2022)\\nleverages frequency spectrum energy analysis for effective\\nsignal decomposition, SCINet (Liu et al., 2022) adopts a\\nrecursive downsample-convolve-interact strategy to model\\n2', metadata={'source': 'rpaper.pdf', 'page': 1}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nPatch \\n+ \\nPositional\\nEmbeddings\\nLayer \\nNorm\\nLayer \\nNorm\\nAdaptive \\nSpectral \\nBlock\\nInteractive\\nConvolutional\\nBlock\\nFFT\\nIFFT\\n1\\n2\\n3\\n4\\n5\\nxN\\nLocal \\nWeights\\nGlobal \\nWeights\\nConv \\n1D\\nConv \\n1D\\nConv \\n1D\\nLinear \\nLayer\\nAdaptive \\nMasking\\nGELU\\nGELU\\nFigure 2: The structure of our proposed TSLANet . The input time series is split into patches, and positional embeddings\\nare added. Next, the output embeddings pass through TSLANet layers, where each layer consists of two main components.\\nThe first is the Adaptive Spectral Block, which leverages frequency domain representations for robust feature extraction and\\nemploys adaptive thresholding to mitigate noise. The second is the Interactive Convolution Block, which captures complex\\ntemporal patterns through convolutional operations.\\ncomplex temporal dynamics, and WFTNet (Liu et al., 2023)\\nemploys a combination of Fourier and wavelet transforms', metadata={'source': 'rpaper.pdf', 'page': 2}),\n",
       " Document(page_content='temporal patterns through convolutional operations.\\ncomplex temporal dynamics, and WFTNet (Liu et al., 2023)\\nemploys a combination of Fourier and wavelet transforms\\nfor a thorough temporal-frequency analysis. Additionally,\\nTCE (Zhang et al., 2023) targets the improvement of 1D-\\nCNNs by addressing the disturbing convolution for better\\nlow-frequency component focus, and BTSF (Yang & Hong,\\n2022) introduces a bilinear temporal-spectral fusion tech-\\nnique for unsupervised learning, emphasizing the impor-\\ntance of maintaining the global context of time series data.\\nA noteworthy attempt to leverage CNNs for multiple time\\nseries tasks is the TimesNet model (Wu et al., 2023), which\\ncapitalizes on multi-periodicity to merge intraperiod and\\ninterperiod variations within a 2D space, enhancing the\\nrepresentation of temporal patterns. However, TimesNet\\nmay not fully address the challenges presented by non-\\nstationary datasets lacking clear periodicity. Some recent', metadata={'source': 'rpaper.pdf', 'page': 2}),\n",
       " Document(page_content='representation of temporal patterns. However, TimesNet\\nmay not fully address the challenges presented by non-\\nstationary datasets lacking clear periodicity. Some recent\\nworks have explored combining CNNs with Transformers to\\nharness both their strengths (Li et al., 2022; Wu et al., 2021a;\\nD’Ascoli et al., 2021), though such hybrid approaches re-\\nmain underexplored in time series analysis compared to\\ntheir applications in computer vision.\\nOur work takes a distinct path by proposing a universal\\nconvolutional-based architecture, adept at handling various\\ntime series tasks through adaptive spectral feature extrac-\\ntion. This approach not only utilizes the strong local featurelearning capabilities of CNNs but also efficiently captures\\nglobal temporal patterns, offering a balanced solution for\\nboth local and long-range dependencies in time series data.\\n3. Method\\n3.1. Preliminaries: Discrete Fourier Transform\\nWe first explore the Discrete Fourier Transform (DFT) as', metadata={'source': 'rpaper.pdf', 'page': 2}),\n",
       " Document(page_content='both local and long-range dependencies in time series data.\\n3. Method\\n3.1. Preliminaries: Discrete Fourier Transform\\nWe first explore the Discrete Fourier Transform (DFT) as\\nit is a cornerstone in our framework. Consider a series of\\nNcomplex numbers x[n], where 0≤n≤N−1. The\\n1D DFT transforms this series into a frequency domain\\nrepresentation:\\nX[k] =N−1X\\nn=0x[n]e−j(2π/N )kn:=N−1X\\nn=0x[n]Wkn\\nN,(1)\\nwhere jdenotes the imaginary unit, with WN=e−j(2π/N ).\\nThis formulation is derived from the continuous Fourier\\ntransform by discretizing in both time and frequency do-\\nmains. The spectrum of the sequence x[n]at frequency\\nωk= 2πk/N is represented by X[k], which is periodic\\nwith an interval of length N, thus only the first Npoints are\\nconsidered.\\nDue to the bijective nature of DFT, the original sequence\\n3', metadata={'source': 'rpaper.pdf', 'page': 2}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nx[n]is retrievable via the Inverse DFT (IDFT):\\nx[n] =1\\nNN−1X\\nk=0X[k]ej(2π/N )kn. (2)\\nFor real-valued x[n], DFT exhibits conjugate symmetry,\\ni.e.,X[N−k] =X∗[k]. This symmetry is pivotal, as\\nperforming IDFT on a conjugate symmetric X[k]results in\\na real discrete signal. Half of the DFT spectrum, specifically\\nX[k] : 0≤k≤ ⌈N/2⌉, sufficiently describes the frequency\\ncharacteristics of x[n].\\nThe choice of DFT in TSLANet is motivated by two factors:\\nits discrete nature aligns well with digital processing and\\nthe existence of efficient computation methods. The Fast\\nFourier Transform (FFT), leveraging the symmetry and pe-\\nriodicity of Wkn\\nN, optimizes DFT computation from O(N2)\\ntoO(NlogN). The IDFT, paralleling DFT’s form, benefits\\nsimilarly from the Inverse FFT (IFFT).\\n3.2. Overall Architecture\\nOur model integrates two novel components, i.e., the Adap-\\ntive Spectral Block (ASB) and the Interactive Convolution', metadata={'source': 'rpaper.pdf', 'page': 3}),\n",
       " Document(page_content='similarly from the Inverse FFT (IFFT).\\n3.2. Overall Architecture\\nOur model integrates two novel components, i.e., the Adap-\\ntive Spectral Block (ASB) and the Interactive Convolution\\nBlock (ICB), as depicted in Figure 2. These two components\\nform a single layer that could be extended to multiple layers.\\nThe ASB employs Fourier analysis to transform time series\\ndata into the frequency domain, in which we apply adaptive\\nthresholding to attenuate high-frequency noise and highlight\\nrelevant spectral features. After processing, the IFFT recon-\\nstructs the time-domain features, now with reduced noise\\nand enhanced representations. The ICB is a streamlined\\nconvolutional block that interactively refines features using\\ndifferent kernel sizes, improving adaptability to temporal\\ndynamics in time series. Together, these components form a\\ncohesive structure that balances local and global temporal\\nfeature extraction for time series analysis.\\n3.3. Embedding Layer', metadata={'source': 'rpaper.pdf', 'page': 3}),\n",
       " Document(page_content='dynamics in time series. Together, these components form a\\ncohesive structure that balances local and global temporal\\nfeature extraction for time series analysis.\\n3.3. Embedding Layer\\nGiven an input time series S, with each signal S∈RC×L\\nhaving Cchannels and a sequence length L. First, the sig-\\nnalSis divided into a set of Mpatches {P1, P2, ..., P M},\\nwhere each patch Picaptures a segment of S. The dimen-\\nsion of each patch is determined by the predefined patch\\nsizep, such that each patch Pi∈RC×p.\\nEach patch is then mapped into another dimension p′, i.e.,\\nPi→P′\\ni∈RC×p′. Next, the positional embeddings are\\nadded to each patch to retain the temporal ordering dis-\\nrupted during the segmentation process. The positional\\nembedding for the i-th patch is denoted as Ei, a vector that\\naligns dimensionally with the patch. The augmented patch\\nresults from adding both inputs, i.e., SPEi=P′\\ni+Ei, and\\nSPE={SPE1, SPE2, . . . S PEM}. Notably, the positional', metadata={'source': 'rpaper.pdf', 'page': 3}),\n",
       " Document(page_content='aligns dimensionally with the patch. The augmented patch\\nresults from adding both inputs, i.e., SPEi=P′\\ni+Ei, and\\nSPE={SPE1, SPE2, . . . S PEM}. Notably, the positional\\nembeddings are learnable parameters, allowing the modelto capture the temporal relationships within the time series\\ndata effectively.\\n3.4. Adaptive Spectral Block\\nWe propose the Adaptive Spectral Block (ASB) that em-\\nploys the Fourier-domain processing, as inspired by (Rao\\net al., 2021). This block aims to learn spatial informa-\\ntion with the global circular convolution operations. More-\\nover, it provides adaptive local filters to isolate noisy high-\\nfrequency components for any time series data.\\nFast Fourier Transformations. Given a discrete time\\nseries x[n], we obtain its frequency domain representation\\nX[k], by performing FFT along the spatial dimensions as\\nin Equation 1. Similarly, given SPE, its representation is\\ncalculated as:\\nF=F[SPE]∈CC×L′, (3)\\nwhereF[·]denotes the 1D FFT operation, and L′is the trans-', metadata={'source': 'rpaper.pdf', 'page': 3}),\n",
       " Document(page_content='in Equation 1. Similarly, given SPE, its representation is\\ncalculated as:\\nF=F[SPE]∈CC×L′, (3)\\nwhereF[·]denotes the 1D FFT operation, and L′is the trans-\\nformed sequence length in the frequency domain, which\\nmay differ from Ldepending on the FFT implementation\\nand the nature of the time series data. Each channel of\\nthe time series is independently transformed, resulting in\\na comprehensive frequency domain representation Fthat\\nencapsulates the spectral characteristics of the original time\\nseries across all channels.\\nAdaptive Removal of High-Frequency Noise. High-\\nfrequency components often represent rapid fluctuations\\nthat deviate from the underlying trend or signal of interest,\\nmaking them appear more random and difficult to interpret\\n(Rhif et al., 2019). Therefore, we propose an adaptive lo-\\ncal filter that allows the model to dynamically adjust the\\nlevel of filtering according to the dataset characteristics and\\nremove these high-frequency noisy components. This is', metadata={'source': 'rpaper.pdf', 'page': 3}),\n",
       " Document(page_content='cal filter that allows the model to dynamically adjust the\\nlevel of filtering according to the dataset characteristics and\\nremove these high-frequency noisy components. This is\\ncrucial when dealing with non-stationary data, where the\\nfrequency spectrum may change over time. The proposed\\nfilter adaptively sets the appropriate frequency threshold for\\neach specific time series data.\\nGiven the frequency domain representation Fobtained from\\nthe FFT operation, we first calculate the power spectrum of\\nF, which helps in identifying dominant frequency compo-\\nnents. The power spectrum Pis computed as the square\\nof the magnitude of the frequency components: P=|F|2,\\nwhich gives us a measure of the strength of different fre-\\nquencies in the time series data.\\nThe key to effective noise reduction lies in adaptively filter-\\ning high-frequency components from the power spectrum\\nP. We achieve this with a trainable threshold θ, which ad-\\njusts based on the spectral characteristics of the data. This', metadata={'source': 'rpaper.pdf', 'page': 3}),\n",
       " Document(page_content='ing high-frequency components from the power spectrum\\nP. We achieve this with a trainable threshold θ, which ad-\\njusts based on the spectral characteristics of the data. This\\nthreshold θis set as a learnable parameter optimized during\\ntraining through backpropagation, specifically∂L\\n∂θ, enabling\\n4', metadata={'source': 'rpaper.pdf', 'page': 3}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nθto discern between essential signal frequencies and noise.\\nWe formulate this adaptive thresholding as follows:\\nFfiltered =F⊙(P> θ), (4)\\nwhere⊙represents element-wise multiplication, and (P>\\nθ)is a binary mask where frequencies with power above the\\nthreshold θare retained, and others are filtered out.\\nThe adaptability of the threshold θensures that the ASB\\ncan efficiently remove high frequencies while preserving\\ncrucial information. By adaptively selecting the frequency\\nthreshold, the ASB tailors its filtering process to each spe-\\ncific time series dataset, enhancing the overall effectiveness\\nof the model in handling a wide range of data scenarios.\\nLearnable Filters. After adaptively filtering the frequency\\ndomain data, the model employs two sets of learnable filters;\\na global filter to learn from the original frequency domain\\ndataFand a local filter to learn from the adaptively filtered', metadata={'source': 'rpaper.pdf', 'page': 4}),\n",
       " Document(page_content='domain data, the model employs two sets of learnable filters;\\na global filter to learn from the original frequency domain\\ndataFand a local filter to learn from the adaptively filtered\\ndataFfiltered . LetWGandWLbe the learnable global and\\nlocal filters, respectively. The application of these filters is\\nrepresented as:\\nFG=WG⊙F, (5)\\nFL=WL⊙Ffiltered. (6)\\nNext, we integrate these filtered features to capture a com-\\nprehensive spectral detail, i.e., Fintegrated =FG+FL.\\nNotably, the multiplication operations in Equations 5 and\\n6 are equivalent to the circular convolution process (see\\nAppendix A). Circular convolution, with its larger recep-\\ntive field over the entire sequence, is particularly adept at\\ncapturing periodic patterns in time series data.\\nInverse Fourier Transform. To convert the integrated\\nfrequency domain data back to the time domain, we apply\\nthe Inverse Fast Fourier Transform (IFFT). The resulting\\ntime-domain signal S′is given by:\\nS′=F−1[Fintegrated ]∈RC×p′. (7)', metadata={'source': 'rpaper.pdf', 'page': 4}),\n",
       " Document(page_content='frequency domain data back to the time domain, we apply\\nthe Inverse Fast Fourier Transform (IFFT). The resulting\\ntime-domain signal S′is given by:\\nS′=F−1[Fintegrated ]∈RC×p′. (7)\\nThe IFFT ensures that the enhanced features align with the\\noriginal data structure of the input time series. The full\\noperation of the ASB is described in Algorithm 1 in the\\nAppendix.\\n3.5. Interactive Convolution Block\\nAfter enhancing feature representation by the ASB, we pro-\\npose the Interactive Convolution Block (ICB), which utilizes\\na dual-layer convolutional structure, as shown in Figure 2.\\nThe design of the ICB includes parallel convolutions with\\ndifferent kernel sizes to capture local features and longer-\\nrange dependencies. Specifically, the first convolutionallayer is designed to capture fine-grained, localized patterns\\nin the data with a smaller kernel. In contrast, the second\\nlayer aims to identify broader, longer-range dependencies\\nwith a larger kernel. We design the ICB such that the output', metadata={'source': 'rpaper.pdf', 'page': 4}),\n",
       " Document(page_content='in the data with a smaller kernel. In contrast, the second\\nlayer aims to identify broader, longer-range dependencies\\nwith a larger kernel. We design the ICB such that the output\\nof each layer modulates the feature extraction of the other.\\nThe element-wise multiplication encourages interactions\\nbetween features extracted at different scales, potentially\\nleading to better modeling of complex relationships.\\nGiven the output of the IFFT operation S′, it serves as the\\ninput to the ICB. The process within the ICB is as follows:\\nA1=ϕ(Conv1 (S′))⊙Conv2 (S′), (8)\\nA2=ϕ(Conv2 (S′))⊙Conv1 (S′), (9)\\nwhere Conv1 (·)andConv2 (·)are two 1D-convolutional\\nlayers and ϕis the GELU activation function.\\nThe activated features are then added and passed through a\\nfinal convolutional layer Conv3 (·):\\nOICB=Conv3 (A1+A2). (10)\\nThe output OICBrepresents the enhanced features ready for\\nthe final layer in the network, represented by a customizable\\nlinear layer according to the task.', metadata={'source': 'rpaper.pdf', 'page': 4}),\n",
       " Document(page_content='OICB=Conv3 (A1+A2). (10)\\nThe output OICBrepresents the enhanced features ready for\\nthe final layer in the network, represented by a customizable\\nlinear layer according to the task.\\n3.6. Self-Supervised Pretraining\\nExpanding the capabilities of TSLANet , we incorporate a\\nphase of self-supervised pretraining, which has garnered\\nsignificant attention for its efficacy in learning high-level\\nrepresentations from unlabeled data (Nie et al., 2023). Draw-\\ning inspiration from methodologies applied in natural lan-\\nguage processing and computer vision, we adopt a masked\\nautoencoder paradigm for time series data (He et al., 2022).\\nOur implementation involves selective masking of input\\nsequence patches, followed by training TSLANet to recon-\\nstruct these masked segments accurately. The masked data\\nthen serves as the training input, compelling the model to\\nlearn and infer the underlying patterns and dependencies in\\nthe data. Unlike methods that apply masking at individual', metadata={'source': 'rpaper.pdf', 'page': 4}),\n",
       " Document(page_content='then serves as the training input, compelling the model to\\nlearn and infer the underlying patterns and dependencies in\\nthe data. Unlike methods that apply masking at individual\\ntime steps, our approach focuses on larger patches. This\\ndesign choice avoids simplistic interpolation from adjacent\\ntime points and encourages the model to understand the\\nentire sequence deeply. The reconstruction of these patches\\nis achieved by optimizing the mean squared error (MSE)\\nloss function.\\n4. Experiments\\nIn this section, we evaluate the efficacy of TSLANet on\\ntime series classification, forecasting, and anomaly detection\\ntasks. We show that our TSLANet can serve as a foundation\\nmodel with competitive performance on these tasks. The\\n5', metadata={'source': 'rpaper.pdf', 'page': 4}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 2: Classification results in different datasets. Results are averaged across each subset of datasets. Results are in terms\\nof accuracy (as %). Blue : best results, Purple : second best. Full results are listed in Tables 10, 11, and 12 in the Appendix.\\nMethodsTSLANet GPT4TS TimesNet ROCKET Crossformer PatchTST MLP TS-TCC TS2VEC\\n(Ours) (2023) (2023) (2020) (2023) (2023) (2023) (2021) (2022)\\nUCR repository (85 datasets) 83.18 61.58 65.27 81.42 73.47 71.84 69.68 75.07 81.42\\nUEA repository (26 datasets) 72.73 58.51 66.55 68.79 66.84 69.13 65.81 69.38 59.62\\nBiomedical signals (2 datasets) 90.24 87.04 87.10 87.20 70.82 83.87 70.63 92.25 86.31\\nHuman activity recognition (3 datasets) 97.46 92.71 91.51 96.44 77.55 94.87 56.69 97.16 95.70\\nAverage 85.90 74.96 77.61 83.46 72.17 79.93 65.70 83.55 80.76\\nTable 3: Multivariate forecasting results with prediction lengths ∈ {96,192,336,720}. Results are averaged from all', metadata={'source': 'rpaper.pdf', 'page': 5}),\n",
       " Document(page_content='Average 85.90 74.96 77.61 83.46 72.17 79.93 65.70 83.55 80.76\\nTable 3: Multivariate forecasting results with prediction lengths ∈ {96,192,336,720}. Results are averaged from all\\nprediction lengths. Avg means further averaged by subsets .Blue : best results, Purple : second best. Full results are listed in\\nTable 13 in the Appendix.\\nModelsTSLANet Time-LLM iTransformer PatchTST Crossformer FEDformer Autoformer RLinear Dlinear TimesNet GPT4TS SCINet\\n(Ours) (2024) (2024) (2023) (2023) (2022) (2021b) (2023) (2023) (2023) (2023) (2022)\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\\nECL 0.165 0.257 0.158 0.252 0.178 0.270 0.167 0.259 0.244 0.334 0.214 0.327 0.227 0.338 0.219 0.298 0.166 0.263 0.192 0.295 0.167 0.263 0.268 0.365\\nETT (Avg) 0.337 0.377 0.330 0.372 0.383 0.399 0.347 0.378 0.685 0.578 0.408 0.428 0.465 0.459 0.380 0.392 0.369 0.398 0.391 0.404 0.350 0.382 0.689 0.597', metadata={'source': 'rpaper.pdf', 'page': 5}),\n",
       " Document(page_content='ETT (Avg) 0.337 0.377 0.330 0.372 0.383 0.399 0.347 0.378 0.685 0.578 0.408 0.428 0.465 0.459 0.380 0.392 0.369 0.398 0.391 0.404 0.350 0.382 0.689 0.597\\nExchange 0.369 0.404 - -0.360 0.403 0.367 0.404 0.940 0.707 0.519 0.429 0.613 0.539 0.378 0.417 0.297 0.378 0.416 0.443 0.370 0.406 0.750 0.626\\nTraffic 0.396 0.271 0.388 0.264 0.428 0.282 0.420 0.277 0.550 0.304 0.610 0.376 0.628 0.379 0.626 0.378 0.433 0.295 0.620 0.336 0.414 0.294 0.804 0.509\\nWeather 0.228 0.264 0.225 0.257 0.258 0.279 0.238 0.268 0.259 0.315 0.309 0.360 0.338 0.382 0.272 0.291 0.246 0.300 0.259 0.287 0.237 0.270 0.292 0.363\\ndetailed experimental setup is described in Section D, while\\nthe detailed experimental results are presented in Section F\\nin the Appendix.\\n4.1. Classification\\nDatasets. We examine the classification ability of\\nTSLANet on a total of 116 datasets, including 85 uni-\\nvariate UCR datasets (Dau et al., 2019), 26 multi-variate\\nUEA datasets (Bagnall et al., 2018). We also include another', metadata={'source': 'rpaper.pdf', 'page': 5}),\n",
       " Document(page_content='TSLANet on a total of 116 datasets, including 85 uni-\\nvariate UCR datasets (Dau et al., 2019), 26 multi-variate\\nUEA datasets (Bagnall et al., 2018). We also include another\\n5 datasets, i.e., two biomedical datasets, namely, Sleep-EDF\\ndataset (Goldberger et al., 2000) for EEG-based sleep stage\\nclassification and MIT-BIH dataset (Moody & Mark, 2001)\\nfor ECG-based arrhythmia classification, and three human\\nactivity recognition (HAR) datasets, namely, UCIHAR (An-\\nguita et al., 2013), WISDM (Kwapisz et al., 2011), and\\nHHAR (Stisen et al., 2015). These datasets have different\\ncharacteristics and they span a wide range of time series\\napplications. More details about these datasets are included\\nin Appendix E.2.\\nBaselines and Experimental Settings. We select eight\\nstate-of-the-art baselines, i.e., GPT4TS (Zhou et al.,\\n2023), TimesNet (Wu et al., 2023), ROCKET (Demp-\\nster et al., 2020), TS-TCC (Eldele et al., 2021), TS2Vec\\n(Yue et al., 2022), Crossformer (Zhang & Yan, 2023) and', metadata={'source': 'rpaper.pdf', 'page': 5}),\n",
       " Document(page_content='2023), TimesNet (Wu et al., 2023), ROCKET (Demp-\\nster et al., 2020), TS-TCC (Eldele et al., 2021), TS2Vec\\n(Yue et al., 2022), Crossformer (Zhang & Yan, 2023) and\\nPatchTST (Nie et al., 2023) as they showed the best classifi-\\ncation accuracy over other Transformer-based architectures.\\nLast, we experiment with a simple single-layer MLP.Results. Table 2 reports the classification results, where\\nour proposed TSLANet demonstrates superior performance\\nover state-of-the-art baselines. Notably, convolution-based\\nmethods, including ROCKET, TS-TCC, and our approach,\\noutperform Transformer-based models, highlighting their\\nsuperiority in classification tasks. For example, in the\\nUCR repository, TSLANet achieves an impressive accu-\\nracy of 83.18%, outperforming other models including the\\nROCKET, which scores 81.42%. The UEA repository re-\\nsults further reinforce our efficacy, with a 72.73% accuracy,\\ncompared to the next best model, PatchTST, at 69.38%.', metadata={'source': 'rpaper.pdf', 'page': 5}),\n",
       " Document(page_content='ROCKET, which scores 81.42%. The UEA repository re-\\nsults further reinforce our efficacy, with a 72.73% accuracy,\\ncompared to the next best model, PatchTST, at 69.38%.\\nIn more specialized datasets like biomedical signals and\\nHAR, our advantage is even more pronounced, achieving\\nan overall accuracy of 90.24% and 97.46%, respectively.\\nThese results highlight the robustness and adaptability of\\nTSLANet in diverse time series contexts.\\nIn our comparative analysis, Transformer models generally\\nface challenges across various datasets, reflecting inherent\\nlimitations in handling time series data. MLP models per-\\nform well on simpler UCR datasets but falter in complex,\\nnoisy environments. TimesNet excels in datasets rich in\\nfrequency information but struggles with simpler ones. Last,\\nthe GPT4TS model shows promise in larger datasets due to\\nthe high capacity of the GPT model, yet underperforms in\\nsmaller datasets due to probable overfitting.\\n4.2. Forecasting', metadata={'source': 'rpaper.pdf', 'page': 5}),\n",
       " Document(page_content='the GPT4TS model shows promise in larger datasets due to\\nthe high capacity of the GPT model, yet underperforms in\\nsmaller datasets due to probable overfitting.\\n4.2. Forecasting\\nDatasets. To assess the efficacy of TSLANet in forecast-\\ning, we conduct comprehensive evaluations on eight bench-\\n6', metadata={'source': 'rpaper.pdf', 'page': 5}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 4: Anomaly detection task. We calculate the F1-score (as %) for each dataset. ∗. in the Transformers indicates the\\nname of ∗former. Blue : best, Purple : second best. Table 14 in the Appendix shows the full results.\\nMethodsTSLANetGPT4TS TimesNet PatchTST ETS. FED. LightTS DLinear Stationary Auto. Pyra. Anomaly. In. Re. LogTrans. Trans.(Ours)\\nSMD 87.91 86.89 84.61 84.62 83.13 85.08 82.53 77.10 84.72 85.11 83.04 85.49 81.65 75.32 76.21 79.56\\nMSL 83.32 82.45 81.84 78.70 85.03 78.57 78.95 84.88 77.50 79.05 84.86 83.31 84.06 84.40 79.57 78.68\\nSMAP 75.96 72.88 69.39 68.82 69.50 70.76 69.21 69.26 71.09 71.12 71.09 71.18 69.92 70.40 69.97 69.70\\nSWaT 92.80 94.23 93.02 85.72 84.91 93.19 93.33 87.52 79.88 92.74 91.78 83.10 81.43 82.80 80.52 80.37\\nPSM 97.73 97.13 97.34 96.08 91.76 97.23 97.15 93.55 97.29 93.29 82.08 79.40 77.10 73.61 76.74 76.07', metadata={'source': 'rpaper.pdf', 'page': 6}),\n",
       " Document(page_content='PSM 97.73 97.13 97.34 96.08 91.76 97.23 97.15 93.55 97.29 93.29 82.08 79.40 77.10 73.61 76.74 76.07\\nAverage 87.54 86.72 85.24 82.79 82.87 84.97 84.23 82.46 82.08 84.26 82.57 80.50 78.83 77.31 76.60 76.88\\nmark datasets. i.e., Electricity ( ECL) featuring electric-\\nity consumption data, four ETT datasets ( ETTh1, ETTh2,\\nETTm1, ETTm2 ) that encompass a range of scenarios in en-\\nergy transfer technology, Exchange that encompasses fluctu-\\nating currency exchange rates, Traffic that comprises traffic\\nflow information, and Weather that offers insights into vari-\\nous meteorological variables over time. We include more\\ndetails about their characteristics in Appendix E.3.\\nBaselines and Experimental Settings. We compare\\nTSLANet against a variety of state-of-the-art baselines.\\nFor Transformer architectures, we compare against iTrans-\\nformer (Liu et al., 2024), PatchTST, Crossformer, FED-\\nformer (Zhou et al., 2022), and Autoformer (Wu et al.,\\n2021b). For MLP-based models, we compare against RLin-', metadata={'source': 'rpaper.pdf', 'page': 6}),\n",
       " Document(page_content='former (Liu et al., 2024), PatchTST, Crossformer, FED-\\nformer (Zhou et al., 2022), and Autoformer (Wu et al.,\\n2021b). For MLP-based models, we compare against RLin-\\near (Li et al., 2023) and DLinear (Zeng et al., 2023) models.\\nFor general-purpose time series models, we compare our\\nmodel against TimesNet and GPT4TS. For a convolutional-\\nbased forecasting model, we compare with SCINet (Liu\\net al., 2022). Last, we include Time-LLM (Jin et al., 2024),\\nwhich is based on Large-Language Models. Similar to\\n(Zhou et al., 2023) settings, we set the look-back window to\\n336 for the ETT dataset, 96 for Exchange, 512 for the Traf-\\nfic and Weather datasets, and 96 for the ECL dataset. We\\nalso incorporate the data normalization block, and reverse\\ninstance norm in the forecasting task (Kim et al., 2021).\\nFor the baselines, we report the best results in their original\\nworks if they are consistent with our settings, otherwise, we\\nre-run their codes again.', metadata={'source': 'rpaper.pdf', 'page': 6}),\n",
       " Document(page_content='For the baselines, we report the best results in their original\\nworks if they are consistent with our settings, otherwise, we\\nre-run their codes again.\\nResults. In our forecasting experiments presented in Ta-\\nble 3, we notice the superiority of Time-LLM due to its re-\\nliance on the large Llama-7B model (Touvron et al., 2023),\\nwhich enables it to capture complex patterns and depen-\\ndencies in data. Other than Time-LLM, TSLANet consis-\\ntently outperforms baseline models across various datasets.\\nSpecifically, it achieves the second lowest MSE and MAE in\\nseven out of eight datasets, showing 3% and 3.8% MSE im-\\nprovement over the state-of-the-art PatchTST in ETT(avg)\\nand Weather datasets respectively. This indicates the ef-\\nfectiveness of our model in handling datasets with diverse\\ncharacteristics and complexities. In addition, it shows the\\neffect of the added capability of the ASB module in learninglong-range dependencies.\\nThe results also suggest the superiority of our model', metadata={'source': 'rpaper.pdf', 'page': 6}),\n",
       " Document(page_content='characteristics and complexities. In addition, it shows the\\neffect of the added capability of the ASB module in learninglong-range dependencies.\\nThe results also suggest the superiority of our model\\nover specialized Transformer-based architectures and MLP-\\nbased models. These models, e.g., iTransformer and Dlinear\\nshow competitive performance in certain datasets but fall\\nbehind in others. In addition, GPT4TS shows the power\\nof the GPT models in the forecasting task by scoring the\\nsecond-best performance in some datasets.\\nWhile Time-LLM offers slightly better performance, its\\ncomputational cost is significantly higher than TSLANet .\\nTo illustrate, TSLANet demonstrates a nearly equivalent\\nperformance to Time-LLM on the ETTh1 dataset with\\nan MSE of 0.413 compared to Time-LLM’s 0.408, yet\\nTSLANet does so with significantly lower computational\\ncost of 6.9e+10 FLOPS against 7.3e+12 for Time-LLM.\\nThis showcases the effective balance between performance', metadata={'source': 'rpaper.pdf', 'page': 6}),\n",
       " Document(page_content='TSLANet does so with significantly lower computational\\ncost of 6.9e+10 FLOPS against 7.3e+12 for Time-LLM.\\nThis showcases the effective balance between performance\\nand computational efficiency in our TSLANet .\\n4.3. Anomaly Detection\\nDatasets. In this study, we focus on detecting anomalies\\nin unsupervised time series data. We use five benchmark\\ndatasets for our experiments: SMD (Su et al., 2019) for\\nserver monitoring, MSL (Hundman et al., 2018) for space\\ntelemetry, SMAP (Hundman et al., 2018) for earth observa-\\ntions, SWaT (Mathur & Tippenhauer, 2016) for water treat-\\nment security, and PSM (Abdulaal et al., 2021) for industrial\\npump sensors. We discuss their details in Appendix E.4.\\nBaselines and Experimental Settings. We followed the\\nsame experimental settings and adopted the same baselines\\nin GPT4TS (Zhou et al., 2023). These are GPT4TS, Times-\\nNet, PatchTST, ETSformer (Woo et al., 2022), FEDformer,\\nLightTS (Zhang et al., 2022), DLinear, Stationary (Liu et al.,', metadata={'source': 'rpaper.pdf', 'page': 6}),\n",
       " Document(page_content='in GPT4TS (Zhou et al., 2023). These are GPT4TS, Times-\\nNet, PatchTST, ETSformer (Woo et al., 2022), FEDformer,\\nLightTS (Zhang et al., 2022), DLinear, Stationary (Liu et al.,\\n2022), Autoformer, Pyraformer (Liu et al., 2021), Anoma-\\nlyformer (Xu et al., 2022), Informer, Reformer, LogTrans-\\nformer (Li et al., 2019), and the vanilla Transformer. For\\ndata preparation, we segmented each dataset with a slid-\\ning window, following (Xu et al., 2022). We adopted the\\nreconstruction error as our evaluation metric, common in\\nunsupervised learning for spotting anomalies.\\n7', metadata={'source': 'rpaper.pdf', 'page': 6}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 5: Ablation study to the effect of each component.\\nASB-L refers to the local filters in the ASB. UWaveGL is\\nthe UWaveGestureLibrary dataset from the UEA repository.\\nVariantClassification (ACC %) Forecasting (MSE)\\nFordA UWaveGL ETTh1 Exchange\\nw/o ASB 87.3 77.5 0.421 0.380\\nw/o ASB (L) 92.7 88.9 0.417 0.373\\nw/o ICB 91.3 86.2 0.419 0.376\\nw/o pretraining 92.5 90.6 0.415 0.372\\nTSLANet 93.1 91.3 0.413 0.369\\nResults. Table 4 presents the results, where TSLANet\\nperforms best in most of the datasets with an overall F1-\\nscore of 87.54%. It outperforms advanced models like FED-\\nformer and Autoformer, especially in the SMD and PSM\\ndatasets with F1-scores of 87.91% and 97.73% respectively.\\nGPT4TS model follows closely, ranking second with an\\noverall average of 86.72%. Its high capacity makes it effec-\\ntive in detecting anomalies, though it slightly trails behind.\\nNotably, Transformer-based models exhibit lower efficacy', metadata={'source': 'rpaper.pdf', 'page': 7}),\n",
       " Document(page_content='overall average of 86.72%. Its high capacity makes it effec-\\ntive in detecting anomalies, though it slightly trails behind.\\nNotably, Transformer-based models exhibit lower efficacy\\nin anomaly detection in general. This could be regarded\\nto the attention mechanism focusing on dominant normal\\npoints, thus missing rare anomalies. Models that consider\\nperiodicity, like TimesNet and FEDformer, perform well,\\nindicating the value of periodic analysis in highlighting\\nunusual patterns.\\n5. Model Analysis\\n5.1. Ablation Study\\nIn Table 5, we assess the contribution of the different com-\\nponents in our model, where we report the performance of\\nthe model when removing each component individually. No-\\ntably, removing the Adaptive Spectral Block (i.e., w/o ASB)\\nyields a notable decline in performance. For classification\\ntasks on FordA and UWaveGestureLibrary datasets, the ac-\\ncuracy drops to 87.3% and 77.5%, respectively. Similarly,\\nits absence results in higher MSE values in the forecasting', metadata={'source': 'rpaper.pdf', 'page': 7}),\n",
       " Document(page_content='tasks on FordA and UWaveGestureLibrary datasets, the ac-\\ncuracy drops to 87.3% and 77.5%, respectively. Similarly,\\nits absence results in higher MSE values in the forecasting\\ntask of 0.421 and 0.380 for ETTh1 and Exchange datasets.\\nThis underscores the ASB’s critical role in feature extraction\\nand noise reduction. Similarly, excluding the local adap-\\ntive part of the ASB (i.e., w/o ASB-L) affects the noisy\\ndatasets more than less noisy ones, highlighting the local\\ncomponent’s value in handling noise.\\nThe effect of the ICB was less than the ASB, with less\\nperformance degradation in the two tasks. However, its re-\\nmoval shows reduced classification accuracy and increased\\nforecasting MSE indicating its importance. The role of\\npretraining is similarly validated, as its absence slightly\\ndiminishes the model’s performance across both tasks.\\n0 0.5 1 1.5 2 2.5\\nNoise level406080100Accuracy (%)\\nUCIHAR\\nTSLANet\\nTSLANet w/o Adaptive Filter\\nTransformer(a) Robustness against noise lev-', metadata={'source': 'rpaper.pdf', 'page': 7}),\n",
       " Document(page_content='diminishes the model’s performance across both tasks.\\n0 0.5 1 1.5 2 2.5\\nNoise level406080100Accuracy (%)\\nUCIHAR\\nTSLANet\\nTSLANet w/o Adaptive Filter\\nTransformer(a) Robustness against noise lev-\\nels on UCIHAR dataset.\\n0 0.5 1 1.5 2 2.5\\nNoise level708090100Accuracy (%)\\nNATOPS\\nTSLANet\\nTSLANet w/o Adaptive Filter\\nTransformer(b) Robustness against noise\\nlevels on NATOPS dataset.\\n(c) The features before and after the local adaptive filter on the\\nUEA Handwriting dataset.\\nFigure 3: Effectiveness of the Adaptive Filter in noise re-\\nduction.\\n5.2. Efficacy of Adaptive Filtering in Noise Reduction\\nWe delve into the effectiveness of the Adaptive Filter in\\nmitigating noise and enhancing model robustness by ex-\\namining Figure 3. Specifically, Figures 3a and 3b present\\nthe performance of TSLANet , both with and without the\\nAdaptive Filter, against the Transformer model by adding\\ndifferent Gaussian noise levels to the time series. The per-\\nformance of the Transformer deteriorates rapidly as noise', metadata={'source': 'rpaper.pdf', 'page': 7}),\n",
       " Document(page_content='Adaptive Filter, against the Transformer model by adding\\ndifferent Gaussian noise levels to the time series. The per-\\nformance of the Transformer deteriorates rapidly as noise\\nincreases. In contrast, TSLANet maintains a relatively sta-\\nble performance, with the variant using the Adaptive Filter\\nshowing the most resilience to noise. This is particularly\\nnoteworthy at higher noise levels, where the accuracy of the\\nstandard Transformer falls steeply, while TSLANet with\\nthe Adaptive Filter experiences a much less pronounced\\ndecline.\\nIn Figure 3c, we observe the frequency spectra before and\\nafter applying the Adaptive Filter. The left plot shows a\\nnoisy spectrum with high amplitude spikes across various\\nfrequencies. However, after applying the Adaptive Filter,\\na markedly cleaner spectrum where the amplitude of noise\\nspikes is significantly reduced, particularly in the higher\\nfrequency range. This demonstrates the filter’s ability to at-', metadata={'source': 'rpaper.pdf', 'page': 7}),\n",
       " Document(page_content='a markedly cleaner spectrum where the amplitude of noise\\nspikes is significantly reduced, particularly in the higher\\nfrequency range. This demonstrates the filter’s ability to at-\\ntenuate unwanted noise while preserving the relevant signal.\\n5.3. Scaling Efficiency\\nWe compare the scalability of our TSLANet with one of the\\nbest-performing Transformer models in the classification\\ntask, i.e., PatchTST (Nie et al., 2023), by observing their\\nperformance across various dataset sizes and layer counts.\\nSpecifically, we experiment with variable data sizes from\\n8', metadata={'source': 'rpaper.pdf', 'page': 7}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\n1 5 10 15 20\\nNumber of Layers5060708090100Accuracy (%)\\nTSLANet\\nPatchTSTData Percentage\\n1%\\n10%\\n50%\\n100%\\nFigure 4: A comparison between TSLANet vs. PatchTST\\nin terms of accuracy with varying the number of layers in\\nboth for different data percentages from the uWaveGesture-\\nLibraryAll dataset.\\nthe uWaveGestureLibraryAll dataset, as shown in Figure 4.\\nNotably, in smaller data sizes, TSLANet demonstrates a\\nconsistent accuracy level, subtly decreasing as the number of\\nlayers increases. In contrast, the PatchTST shows a marked\\ndecline in accuracy with additional layers, suggesting a\\npotential overfitting issue or inefficiency in handling limited\\ndata with increased model complexity.\\nAs dataset sizes grow, TSLANet performance remains ro-\\nbust, showing slight variations in accuracy with more layers.\\nThis stability contrasts with the PatchTST performance,\\nwhich tends to decrease notably at higher layer counts. This', metadata={'source': 'rpaper.pdf', 'page': 8}),\n",
       " Document(page_content='bust, showing slight variations in accuracy with more layers.\\nThis stability contrasts with the PatchTST performance,\\nwhich tends to decrease notably at higher layer counts. This\\ntrend in PatchTST could be attributed to their inherent de-\\nsign, which might lead to diminishing returns or optimiza-\\ntion challenges as the model depth increases. Lastly, we\\nnotice that TSLANet effectively leverages larger dataset\\nsamples, as its performance improves with an increase in\\nthe number of layers, highlighting its capacity to capitalize\\non more extensive data for enhanced accuracy.\\n5.4. Complexity Analysis\\nWe compare the complexity of our TSLANet with Times-\\nNet and Transformer-based models, e.g., PatchTST, FED-\\nFormer, AutoFormer, Informer, and Reformer in terms of\\nthe number of parameters, FLOPs, and accuracy on the\\nUEA Heartbeat dataset, as shown in Figure 5. TSLANet\\ndemonstrates superior efficiency and accuracy in time series\\nanalysis, achieving the highest accuracy of 77.56% with the', metadata={'source': 'rpaper.pdf', 'page': 8}),\n",
       " Document(page_content='UEA Heartbeat dataset, as shown in Figure 5. TSLANet\\ndemonstrates superior efficiency and accuracy in time series\\nanalysis, achieving the highest accuracy of 77.56% with the\\nlowest computational and parameter footprint among the\\ncompared models. It requires 93% fewer FLOPs and 84%\\nfewer parameters than the PatchTST, yet outperforms it by\\nover 8% in accuracy. Compared to TimesNet, TSLANet\\noperates with more than 99% fewer FLOPs and parameters\\nwhile still delivering a 3% higher accuracy.\\nThis considerable reduction in computational demand con-\\nfirms the lightweight nature of TSLANet compared to\\nTransformer-based alternatives, underscoring its capacity to\\n0 1 2 3 4 5 6 7\\nNumber of Parameters (Millions)50556065707580Accuracy (%) TSLANet\\n PatchTST\\n FEDformer\\n Autoformer TimesNet\\n Informer\\n Reformer\\n1G FLOPs 10G FLOPsFigure 5: TSLANet vs. baselines in terms of the number\\nof parameters and FLOPS count against the classification\\naccuracy of the UEA Heartbeat dataset.', metadata={'source': 'rpaper.pdf', 'page': 8}),\n",
       " Document(page_content='Informer\\n Reformer\\n1G FLOPs 10G FLOPsFigure 5: TSLANet vs. baselines in terms of the number\\nof parameters and FLOPS count against the classification\\naccuracy of the UEA Heartbeat dataset.\\nmake time series analysis more efficient.\\n6. Conclusions\\nIn this paper, we introduced TSLANet , a novel lightweight\\nmodel for time series analysis that revisits the convolution\\napproach as a potent replacement to Transformers, with\\nan innovative combination of convolution operations and\\nadaptive spectral analysis. Our comprehensive experiments\\nacross various datasets in classification, forecasting, and\\nanomaly detection have demonstrated its superior perfor-\\nmance over traditional Transformer models, particularly\\nin its ability to maintain high accuracy levels in noisy\\nconditions and across different data sizes. Furthermore,\\nour in-depth layer-wise performance analysis revealed that\\nTSLANet not only outperforms Transformers in smaller\\ndatasets but also exhibits improved scalability with increas-', metadata={'source': 'rpaper.pdf', 'page': 8}),\n",
       " Document(page_content='our in-depth layer-wise performance analysis revealed that\\nTSLANet not only outperforms Transformers in smaller\\ndatasets but also exhibits improved scalability with increas-\\ning layers, particularly in larger datasets. TSLANet is a\\nstep towards a foundation model for time series analysis.\\nImpact Statement\\nOur proposed work TSLANet aims to advance the field of\\nMachine Learning by providing a more efficient, scalable,\\nand robust foundation model for analyzing time series data\\nacross various applications. It has the potential to impact\\nvarious sectors, including healthcare, finance, and environ-\\nmental monitoring, by enhancing forecasting accuracy and\\nanomaly detection capabilities. Such improvements could\\nlead to better patient outcomes, more informed financial\\ndecisions, and greater preparedness for natural disasters.\\nReferences\\nAbdulaal, A., Liu, Z., and Lancewicki, T. Practical ap-\\nproach to asynchronous multivariate time series anomaly\\ndetection and localization. In SIGKDD , 2021.\\n9', metadata={'source': 'rpaper.pdf', 'page': 8}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nAnguita, D., Ghio, A., Oneto, L., Parra, X., and Reyes-\\nOrtiz, J. L. A public domain dataset for human activity\\nrecognition using smartphones. In European Symposium\\non Artificial Neural Networks , 2013.\\nBagnall, A., Dau, H. A., Lines, J., Flynn, M., Large, J.,\\nBostrom, A., Southam, P., and Keogh, E. The uea multi-\\nvariate time series classification archive, 2018, 2018.\\nD’Ascoli, S., Touvron, H., Leavitt, M. L., Morcos, A. S.,\\nBiroli, G., and Sagun, L. Convit: Improving vision trans-\\nformers with soft convolutional inductive biases. In ICML ,\\npp. 2286–2296, 2021.\\nDau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu,\\nY ., Gharghabi, S., Ratanamahatana, C. A., and Keogh, E.\\nThe ucr time series archive, 2019.\\nDempster, A., Petitjean, F., and Webb, G. I. ROCKET:\\nExceptionally fast and accurate time series classification\\nusing random convolutional kernels. Data Mining and\\nKnowledge Discovery , 34(5):1454–1495, 2020.', metadata={'source': 'rpaper.pdf', 'page': 9}),\n",
       " Document(page_content='Exceptionally fast and accurate time series classification\\nusing random convolutional kernels. Data Mining and\\nKnowledge Discovery , 34(5):1454–1495, 2020.\\nEkambaram, V ., Jati, A., Nguyen, N., Sinthong, P., and\\nKalagnanam, J. Tsmixer: Lightweight mlp-mixer model\\nfor multivariate time series forecasting. KDD , 2023.\\nEldele, E., Ragab, M., Chen, Z., Wu, M., Kwoh, C. K., Li,\\nX., and Guan, C. Time-series representation learning\\nvia temporal and contextual contrasting. In IJCAI , pp.\\n2352–2359, 2021.\\nGoldberger, A. L., Amaral, L. A. N., Glass, L., Hausdorff,\\nJ. M., Ivanov, P. C., Mark, R. G., Mietus, J. E., Moody,\\nG. B., Peng, C.-K., and Stanley, H. E. Physiobank, phys-\\niotoolkit, and physionet components of a new research\\nresource for complex physiologic signals. Circulation ,\\n2000.\\nHe, K., Chen, X., Xie, S., Li, Y ., Doll ´ar, P., and Girshick,\\nR. Masked autoencoders are scalable vision learners. In\\nCVPR , 2022.\\nHundman, K., Constantinou, V ., Laporte, C., Colwell, I.,', metadata={'source': 'rpaper.pdf', 'page': 9}),\n",
       " Document(page_content='2000.\\nHe, K., Chen, X., Xie, S., Li, Y ., Doll ´ar, P., and Girshick,\\nR. Masked autoencoders are scalable vision learners. In\\nCVPR , 2022.\\nHundman, K., Constantinou, V ., Laporte, C., Colwell, I.,\\nand Soderstrom, T. Detecting spacecraft anomalies us-\\ning lstms and nonparametric dynamic thresholding. In\\nSIGKDD , 2018.\\nJin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y ., Shi, X.,\\nChen, P.-Y ., Liang, Y ., Li, Y .-F., Pan, S., and Wen, Q.\\nTime-LLM: Time series forecasting by reprogramming\\nlarge language models. In International Conference on\\nLearning Representations (ICLR) , 2024.\\nKim, T., Kim, J., Tae, Y ., Park, C., Choi, J.-H., and Choo,\\nJ. Reversible instance normalization for accurate time-\\nseries forecasting against distribution shift. ICLR , 2021.Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The\\nefficient transformer. In ICLR , 2020.\\nKwapisz, J. R., Weiss, G. M., and Moore, S. A. Activ-\\nity recognition using cell phone accelerometers. Sigkdd\\nExplorations , 2011.', metadata={'source': 'rpaper.pdf', 'page': 9}),\n",
       " Document(page_content='efficient transformer. In ICLR , 2020.\\nKwapisz, J. R., Weiss, G. M., and Moore, S. A. Activ-\\nity recognition using cell phone accelerometers. Sigkdd\\nExplorations , 2011.\\nLi, J., Hui, X., and Zhang, W. Informer: Beyond efficient\\ntransformer for long sequence time-series forecasting. In\\nAAAI , 2021.\\nLi, K., Wang, Y ., Peng, G., Song, G., Liu, Y ., Li, H., and\\nQiao, Y . Uniformer: Unified transformer for efficient\\nspatial-temporal representation learning. In ICLR , 2022.\\nLi, S., Jin, X., Xuan, Y ., Zhou, X., Chen, W., Wang, Y .-X.,\\nand Yan, X. Enhancing the locality and breaking the mem-\\nory bottleneck of transformer on time series forecasting.\\nInNeurIPS , volume 32, 2019.\\nLi, Z., Qi, S., Li, Y ., and Xu, Z. Revisiting long-term time\\nseries forecasting: An investigation on linear mapping.\\narXiv preprint arXiv:2305.10721 , 2023.\\nLiu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L., and\\nXu, Q. Scinet: time series modeling and forecasting with', metadata={'source': 'rpaper.pdf', 'page': 9}),\n",
       " Document(page_content='arXiv preprint arXiv:2305.10721 , 2023.\\nLiu, M., Zeng, A., Chen, M., Xu, Z., Lai, Q., Ma, L., and\\nXu, Q. Scinet: time series modeling and forecasting with\\nsample convolution and interaction. NeurIPS , 2022.\\nLIU, M., Zeng, A., LAI, Q., Gao, R., Li, M., Qin, J., and Xu,\\nQ. T-wavenet: A tree-structured wavelet neural network\\nfor time series signal analysis. In ICLR , 2022.\\nLiu, P., Wu, B., Li, N., Dai, T., Lei, F., Bao, J., Jiang, Y ., and\\nXia, S.-T. Wftnet: Exploiting global and local periodicity\\nin long-term time series forecasting. ICASSP , 2023.\\nLiu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and\\nDustdar, S. Pyraformer: Low-complexity pyramidal atten-\\ntion for long-range time series modeling and forecasting.\\nICLR , 2021.\\nLiu, Y ., Wu, H., Wang, J., and Long, M. Non-stationary\\ntransformers: Rethinking the stationarity in time series\\nforecasting. NeurIPS , 2022.\\nLiu, Y ., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L.,\\nand Long, M. itransformer: Inverted transformers are', metadata={'source': 'rpaper.pdf', 'page': 9}),\n",
       " Document(page_content='transformers: Rethinking the stationarity in time series\\nforecasting. NeurIPS , 2022.\\nLiu, Y ., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L.,\\nand Long, M. itransformer: Inverted transformers are\\neffective for time series forecasting. In ICLR , 2024.\\nMathur, A. P. and Tippenhauer, N. O. Swat: A water treat-\\nment testbed for research and training on ics security.\\nIninternational workshop on cyber-physical systems for\\nsmart water networks (CySWater) , 2016.\\nMeng, Q., Qian, H., Liu, Y ., Cui, L., Xu, Y ., and Shen, Z.\\nMHCCL: masked hierarchical cluster-wise contrastive\\nlearning for multivariate time series. In AAAI , 2023.\\n10', metadata={'source': 'rpaper.pdf', 'page': 9}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nMoody, G. and Mark, R. The impact of the mit-bih arrhyth-\\nmia database. IEEE Engineering in Medicine and Biology\\nMagazine , 2001. doi: 10.1109/51.932724.\\nNie, Y ., Nguyen, N. H., Sinthong, P., and Kalagnanam, J.\\nA time series is worth 64 words: Long-term forecasting\\nwith transformers. ICLR , 2023.\\nRao, Y ., Zhao, W., Zhu, Z., Lu, J., and Zhou, J. Global\\nfilter networks for image classification. In Beygelzimer,\\nA., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.),\\nNeurIPS , 2021.\\nRhif, M., Ben Abbes, A., Farah, I. R., Mart ´ınez, B., and\\nSang, Y . Wavelet transform application for/in non-\\nstationary time-series analysis: A review. Applied Sci-\\nences , 2019.\\nStisen, A., Blunck, H., Bhattacharya, S., Prentow, T. S.,\\nKjærgaard, M. B., Dey, A., Sonne, T., and Jensen, M. M.\\nSmart devices are different: Assessing and mitigatingmo-\\nbile sensing heterogeneities for activity recognition. In', metadata={'source': 'rpaper.pdf', 'page': 10}),\n",
       " Document(page_content='Kjærgaard, M. B., Dey, A., Sonne, T., and Jensen, M. M.\\nSmart devices are different: Assessing and mitigatingmo-\\nbile sensing heterogeneities for activity recognition. In\\nProceedings of the 13th ACM Conference on Embedded\\nNetworked Sensor Systems , 2015.\\nSu, Y ., Zhao, Y ., Niu, C., Liu, R., Sun, W., and Pei, D.\\nRobust anomaly detection for multivariate time series\\nthrough stochastic recurrent neural network. In SIGKDD ,\\n2019.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels. arXiv preprint arXiv:2302.13971 , 2023.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\\nis all you need. NeurIPS , 2017.\\nWen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J.,\\nand Sun, L. Transformers in time series: A survey. In', metadata={'source': 'rpaper.pdf', 'page': 10}),\n",
       " Document(page_content='is all you need. NeurIPS , 2017.\\nWen, Q., Zhou, T., Zhang, C., Chen, W., Ma, Z., Yan, J.,\\nand Sun, L. Transformers in time series: A survey. In\\nIJCAI , pp. 6778–6786, 8 2023.\\nWoo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S. Ets-\\nformer: Exponential smoothing transformers for time-\\nseries forecasting. arXiv preprint arXiv:2202.01381 ,\\n2022.\\nWu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L.,\\nand Zhang, L. Cvt: Introducing convolutions to vision\\ntransformers. In ICCV , 2021a.\\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: De-\\ncomposition transformers with Auto-Correlation for long-\\nterm series forecasting. NeurIPS , 2021b.Wu, H., Hu, T., Liu, Y ., Zhou, H., Wang, J., and Long, M.\\nTimesnet: Temporal 2d-variation modeling for general\\ntime series analysis. ICLR , 2023.\\nXu, J., Wu, H., Wang, J., and Long, M. Anomaly trans-\\nformer: Time series anomaly detection with association\\ndiscrepancy. In ICLR , 2022.\\nYang, L. and Hong, S. Unsupervised time-series represen-', metadata={'source': 'rpaper.pdf', 'page': 10}),\n",
       " Document(page_content='Xu, J., Wu, H., Wang, J., and Long, M. Anomaly trans-\\nformer: Time series anomaly detection with association\\ndiscrepancy. In ICLR , 2022.\\nYang, L. and Hong, S. Unsupervised time-series represen-\\ntation learning with iterative bilinear temporal-spectral\\nfusion. In ICML , 2022.\\nYue, Z., Wang, Y ., Duan, J., Yang, T., Huang, C., Tong, Y .,\\nand Xu, B. Ts2vec: Towards universal representation of\\ntime series. In AAAI , 2022.\\nZeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers\\neffective for time series forecasting? AAAI , 2023.\\nZhang, J., Feng, L., He, Y ., Wu, Y ., and Dong, Y . Temporal\\nconvolutional explorer helps understand 1d-cnn’s learn-\\ning behavior in time series classification from frequency\\ndomain. In CIKM , 2023.\\nZhang, T., Zhang, Y ., Cao, W., Bian, J., Yi, X., Zheng, S.,\\nand Li, J. Less is more: Fast multivariate time series\\nforecasting with light sampling-oriented mlp structures.\\narXiv preprint arXiv:2207.01186 , 2022.', metadata={'source': 'rpaper.pdf', 'page': 10}),\n",
       " Document(page_content='and Li, J. Less is more: Fast multivariate time series\\nforecasting with light sampling-oriented mlp structures.\\narXiv preprint arXiv:2207.01186 , 2022.\\nZhang, Y . and Yan, J. Crossformer: Transformer utilizing\\ncross-dimension dependency for multivariate time series\\nforecasting. ICLR , 2023.\\nZhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin,\\nR. FEDformer: Frequency enhanced decomposed trans-\\nformer for long-term series forecasting. ICML , 2022.\\nZhou, T., Niu, P., Wang, X., Sun, L., and Jin, R. One fits all:\\nPower general time series analysis by pretrained LM. In\\nNeurIPS , 2023.\\n11', metadata={'source': 'rpaper.pdf', 'page': 10}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nA. Circular Convolutions\\nThe convolution theorem suggests that the multiplication in the frequency domain is equivalent to the circular convolution\\nprocess.\\nLetx[n]andh[n]be two length Nsequences. Their DFTs are X[k]andH[k], respectively. Consider the circular convolution\\ny[n] = (x⊛h)[n]. The DFT of y[n]isY[k].\\nFirst, the DFT of the Convolution can be formulated as:\\nY[k] =N−1X\\nn=0 N−1X\\nm=0x[m]·h[(n−m) mod N]!\\n·e−i2πkn/N\\nHowever, if we changed the order of summation, it becomes:\\nY[k] =N−1X\\nm=0x[m]·N−1X\\nn=0h[(n−m) mod N]·e−i2πkn/N\\nBy substituting n−mwithr:\\nY[k] =N−1X\\nm=0x[m]·e−i2πkm/N·N−1X\\nr=0h[r]·e−i2πkr/N\\nTherefore, we recognize the DFTs of x[n]andh[n]:\\nY[k] = N−1X\\nm=0x[m]·e−i2πkm/N!\\n· N−1X\\nr=0h[r]·e−i2πkr/N!\\nY[k] =X[k]·H[k]\\nThus, we have shown that the DFT of the circular convolution of two sequences x[n]andh[n]is the product of their\\nindividual DFTs, i.e., Y[k] =X[k]·H[k].', metadata={'source': 'rpaper.pdf', 'page': 11}),\n",
       " Document(page_content='· N−1X\\nr=0h[r]·e−i2πkr/N!\\nY[k] =X[k]·H[k]\\nThus, we have shown that the DFT of the circular convolution of two sequences x[n]andh[n]is the product of their\\nindividual DFTs, i.e., Y[k] =X[k]·H[k].\\nB. Frequency Domain Processing Role to Learn Long-Range Dependencies\\nFourier transforms, used in our Adaptive Spectral Block (ASB), can learn long-range and short-range dependencies in time\\nseries. The Fourier Transform (FT) of a time series x(t)is given by:\\nX(f) =Z∞\\n−∞x(t)e−j2πftdt\\nwhere X(f)represents the signal in the frequency domain, fis the frequency, and trepresents time.\\nThe FT decomposes x(t)into its constituent frequencies, where each frequency component represents a pattern in the time\\nseries. Low-frequency components correspond to long-range dependencies (slowly changing trends), and high-frequency\\ncomponents correspond to short-range dependencies (rapid fluctuations). Let’s consider a simplified model where the ASB', metadata={'source': 'rpaper.pdf', 'page': 11}),\n",
       " Document(page_content='components correspond to short-range dependencies (rapid fluctuations). Let’s consider a simplified model where the ASB\\napplies a filter H(f)to the Fourier transform X(f)of the input signal, enhancing certain frequencies while attenuating\\nothers:\\nY(f) =H(f)·X(f)\\nwhere Y(f)is the output signal in the frequency domain.\\nThe adaptiveness comes from adjusting H(f)based on the data, which can be modeled as a learning process where\\nH(f)is updated to minimize a loss function Lthat measures the discrepancy between the model output and the true data\\ncharacteristics:\\nmin\\nH(f)L(Y(f),True Data )\\n12', metadata={'source': 'rpaper.pdf', 'page': 11}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nThrough this process, H(f)learns to emphasize the frequency components that are most relevant for predicting the target,\\nwhether they capture long-range or short-range dependencies.\\nAfter filtering in the frequency domain, the inverse Fourier transform (IFT) is applied to convert Y(f)back into the time\\ndomain, yielding the modified signal y(t):\\ny(t) =Z∞\\n−∞Y(f)ej2πftd f\\nThis signal now encapsulates the learned dependencies, ready for further processing or as an input to subsequent model\\nlayers.\\nC. Algorithm of Adaptive Spectral Block\\nAlgorithm 1 Pseudocode of the Adaptive Spectral Block.\\ndef adaptive_high_freq_mask(x, threshold):\\n# Calculate energy\\nenergy = torch.abs(x_fft).pow(2).sum(dim=-1)\\n# Compute the adaptive threshold\\nthreshold = torch.quantile(energy, threshold)\\n# Identify the dominant frequencies\\ndominant_freq = normalized_energy > threshold\\n# Set adaptive mask values\\nadaptive_mask[dominant_freq] = 1', metadata={'source': 'rpaper.pdf', 'page': 12}),\n",
       " Document(page_content='threshold = torch.quantile(energy, threshold)\\n# Identify the dominant frequencies\\ndominant_freq = normalized_energy > threshold\\n# Set adaptive mask values\\nadaptive_mask[dominant_freq] = 1\\nreturn adaptive_mask\\n# Transform input x_in to frequency domain\\nX_fft = fft(x_in)\\n# Create an adaptive mask for high-freq. components\\nfreq_mask = adaptive_high_freq_mask(X_fft, threshold)\\n# Apply adaptive high-frequency mask\\nX_masked = X_fft *freq_mask\\n# Apply global and local learnable weights\\nX_L = X_masked *local_weight\\nX_G = X_fft *global_weight\\n# Transform data back into the time domain\\nx_out = ifft(X_L + X_G)\\nD. Experimental Setup\\nD.1. Training Protocol\\nTo train the classification experiments, we optimized TSLANet using AdamW with a learning rate of 1e-3 and a weight\\ndecay of 1e-4, applied during both training and pretraining phases. The experiments ran for 50 epochs for pretraining and', metadata={'source': 'rpaper.pdf', 'page': 12}),\n",
       " Document(page_content='decay of 1e-4, applied during both training and pretraining phases. The experiments ran for 50 epochs for pretraining and\\n100 epochs for fine-tuning. For the forecasting and anomaly detection experiments, we utilized a learning rate of 1e-4 and a\\nweight decay of 1e-6, with both phases running for 10 epochs.\\nFor all experiments, the patch size was set to 8 and the stride to 4. Each experiment was repeated three times, with the\\naverage performance reported. TSLANet was implemented using PyTorch and conducted on NVIDIA RTX A6000 GPUs.\\nD.2. Objective Functions\\nFor the classification task, we employ a categorical cross-entropy loss function with label smoothing, defined as Lclf=\\n−PC\\ni=1ysmooth\\ni·log(ˆyi). Here, ysmooth\\ni is the true class label in one-hot encoded form adjusted via label smoothing, ˆyiis the\\npredicted probability for each class, and Cis the total number of classes. Label smoothing reduces model confidence by', metadata={'source': 'rpaper.pdf', 'page': 12}),\n",
       " Document(page_content='predicted probability for each class, and Cis the total number of classes. Label smoothing reduces model confidence by\\nadjusting the true labels with a smoothing parameter ϵ, making the distribution more uniform, where each yiis transformed\\ntoysmooth\\ni = (1−ϵ)·yi+ϵ\\nC.\\nIn forecasting and anomaly detection, we use the Mean Squared Error (MSE) to measure discrepancies between predicted\\nvalues and actual observations, expressed as LMSE=1\\nNPN\\ni=1(yi−ˆyi)2. Here, yirepresents the actual value at time i,ˆyi\\n13', metadata={'source': 'rpaper.pdf', 'page': 12}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\ndenotes the forecasted value, and Nis the number of predictions. This MSE loss is also utilized in self-supervised learning\\ntasks to reconstruct masked patches.\\nD.3. Evaluation Metrics\\nModel performance was evaluated using standard metrics appropriate to each task. For classification, we reported accuracy;\\nfor forecasting, Mean Squared Error (MSE) and Mean Absolute Error (MAE) were used; for anomaly detection, the F1-score\\nwas our primary metric due to the imbalanced nature of the datasets.\\nE. Datasets Details\\nE.1. Data Preprocessing\\nFor the classification task, the UCR and UEA datasets are already split into train/test splits. A validation set was picked\\nfrom each dataset in the training set with a ratio of 80/20. The selection of the hyperparameters was based on the average\\nresults on the validation sets across each collection of datasets, i.e., UCR and UEA. For biomedical and human activity', metadata={'source': 'rpaper.pdf', 'page': 13}),\n",
       " Document(page_content='results on the validation sets across each collection of datasets, i.e., UCR and UEA. For biomedical and human activity\\nrecognition datasets, which are not split by default, we split the data into a 60/20/20 ratio for train/validation/test splits.\\nFor forecasting and anomaly detection datasets, these are split into a ratio of 70/10/20 following a line of previous works,\\ntowards a fair comparison with these works (Zhou et al., 2022; Kitaev et al., 2020; Li et al., 2021; Wu et al., 2023). All\\ndatasets are normalized during training.\\nFor the self-supervised task, we deploy the unlabeled version of the training set in each dataset for pretraining, then use the\\nsame set again with labels for fine-tuning.\\nE.2. Classification\\nIn our evaluation, we extensively utilize four categories of datasets:\\n•UCR datasets: The UCR Time Series Classification Archive is one of the most comprehensive collections of univariate', metadata={'source': 'rpaper.pdf', 'page': 13}),\n",
       " Document(page_content='In our evaluation, we extensively utilize four categories of datasets:\\n•UCR datasets: The UCR Time Series Classification Archive is one of the most comprehensive collections of univariate\\ndatasets tailored for time series analysis. This archive encompasses 85 diverse datasets, each presenting unique\\nchallenges and characteristics that span a wide array of domains, from healthcare and finance to environmental\\nmonitoring and beyond. The variety within the UCR archive allows for a robust assessment of TSLANet across\\ndifferent contexts, showcasing its versatility and performance.\\n•UEA datasets: We also incorporate datasets from the University of East Anglia (UEA) Time Series Classification\\nrepository, which is renowned for its rich collection of multivariate time series datasets. We were able to preprocess 26\\ndatasets, each offering a multidimensional perspective on time series analysis across various real-world scenarios, such', metadata={'source': 'rpaper.pdf', 'page': 13}),\n",
       " Document(page_content='datasets, each offering a multidimensional perspective on time series analysis across various real-world scenarios, such\\nas human activity recognition, sensor data interpretation, and complex system monitoring. More details about the UCR\\nand UEA datasets can be found in https://www.timeseriesclassification.com/ .\\n•Biomedical datasets: The biomedical domain presents unique challenges and opportunities for time series analysis. In\\nthis context, we utilized two pivotal datasets for our evaluation: the Sleep-EDF dataset and the MIT-BIH Arrhythmia\\ndataset.\\n–Sleep-EDF Dataset: This dataset consists of polysomnography recordings intended for sleep stage classification.\\nIt is part of the PhysioNet database and includes polysomnographic sleep recordings that have been widely used\\nto analyze sleep patterns and stages. For our analysis, we extracted the brain EEG signals.\\n–MIT-BIH Arrhythmia Dataset: Another significant dataset from PhysioNet, the MIT-BIH Arrhythmia Dataset, is', metadata={'source': 'rpaper.pdf', 'page': 13}),\n",
       " Document(page_content='to analyze sleep patterns and stages. For our analysis, we extracted the brain EEG signals.\\n–MIT-BIH Arrhythmia Dataset: Another significant dataset from PhysioNet, the MIT-BIH Arrhythmia Dataset, is\\ncomposed of electrocardiogram (ECG) recordings used primarily for arrhythmia detection and classification. It is\\none of the most extensively used datasets for validating arrhythmia detection algorithms, offering a comprehensive\\ncollection of annotated heartbeats and arrhythmia examples.\\nA summary of the characteristics of these two datasets is presented in Table 6.\\n•Human Activity Recognition datasets: Human activity recognition (HAR) using sensor data is a vital application of\\ntime series analysis, with implications for health monitoring, elder care, and fitness tracking. In this study, we evaluate\\nour model using three prominent HAR datasets: UCIHAR, WISDM, and HHAR.\\n14', metadata={'source': 'rpaper.pdf', 'page': 13}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\n–UCI Human Activity Recognition Using Smartphones (UCIHAR): This dataset is collected from experiments\\nthat were carried out with a group of 30 volunteers performing six activities (walking, walking upstairs, walking\\ndownstairs, sitting, standing, and laying) while wearing a smartphone on the waist. The smartphone’s embedded\\naccelerometer and gyroscope captured 3-axial linear acceleration and 3-axial angular velocity, respectively.\\n–Wireless Sensor Data Mining (WISDM): The WISDM dataset includes time series data from smartphone\\nsensors and wearable devices, capturing various human activities such as walking, jogging, sitting, and standing.\\nIt provides a diverse set of user-generated activity data, making it suitable for testing the robustness of HAR\\nmodels across different motion patterns and sensor placements.\\n–Heterogeneity Human Activity Recognition (HHAR): HHAR dataset stands out due to its collection from', metadata={'source': 'rpaper.pdf', 'page': 14}),\n",
       " Document(page_content='models across different motion patterns and sensor placements.\\n–Heterogeneity Human Activity Recognition (HHAR): HHAR dataset stands out due to its collection from\\nmultiple device types, including smartphones and smartwatches, across different individuals performing activities\\nlike biking, sitting, standing, walking, stair climbing, and more. Its heterogeneity in terms of device types and\\npositions offers a challenging benchmark for assessing a model’s ability to generalize across various sensor\\nconfigurations and activity types. Here, we utilized the data from the Samsung devices.\\nA summary of the characteristics of these three datasets is presented in Table 7.\\nTable 6: A description of characteristics of the biomedical datasets used in our experiments.\\nDataset # Train # Test Length # Channel # Class\\nSleep EEG 25,612 8,910 3,000 2 5\\nArrhythmia ECG 70,043 21,892 187 1 2\\nTable 7: A description of characteristics of the Human Activity Recognition datasets used in our experiments.', metadata={'source': 'rpaper.pdf', 'page': 14}),\n",
       " Document(page_content='Sleep EEG 25,612 8,910 3,000 2 5\\nArrhythmia ECG 70,043 21,892 187 1 2\\nTable 7: A description of characteristics of the Human Activity Recognition datasets used in our experiments.\\nDataset # Train # Test Length # Channel # Class\\nUCIHAR 7,352 2,947 128 9 6\\nWISDM 4,731 2,561 128 3 6\\nHHAR 10,336 4,436 128 3 6\\nE.3. Forecasting\\nOur study leverages a diverse set of forecasting datasets to evaluate the effectiveness of our model across various domains:\\n•Electricity: This dataset contains electricity consumption records from 321 clients, offering insights into usage patterns\\nand enabling demand forecasting, crucial for optimizing power generation and distribution.\\n•ETT (Electricity Transformer Temperature) datasets: The ETTh1, ETTh2, ETTm1, and ETTm2 datasets provide\\ndata on the temperature of electricity transformers and the load, facilitating the prediction of future temperatures and', metadata={'source': 'rpaper.pdf', 'page': 14}),\n",
       " Document(page_content='data on the temperature of electricity transformers and the load, facilitating the prediction of future temperatures and\\nloads based on past patterns. These datasets vary in granularity, with ”h” indicating hourly data and ”m” indicating\\n15-minute intervals, offering a range of temporal resolutions for forecasting challenges.\\n•Exchange Rate: Featuring daily exchange rates of different currencies against the US dollar, this dataset is vital for\\nfinancial forecasting, enabling models to anticipate currency fluctuations based on historical data.\\n•Traffic: Traffic dataset consists of hourly interstate 94 Westbound traffic volume for the Twin Cities (Minneapolis-St.\\nPaul) metropolitan area, allowing for the prediction of traffic flow patterns, essential for urban planning and congestion\\nmanagement.\\n•Weather: This dataset includes hourly weather conditions and atmospheric measurements from a weather station,', metadata={'source': 'rpaper.pdf', 'page': 14}),\n",
       " Document(page_content='management.\\n•Weather: This dataset includes hourly weather conditions and atmospheric measurements from a weather station,\\nsupporting forecasts of various weather phenomena, crucial for agriculture, transportation, and daily life planning.\\nWe describe the characteristics of these datasets in Table 8.\\n15', metadata={'source': 'rpaper.pdf', 'page': 14}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 8: Descriptions of the forecasting datasets. Dim shows the variate number of each dataset. Dataset Size indicates the\\nsize of the (Train, Validation, Test) split respectively. Frequency denotes the sampling interval of time points.\\nDataset Dim Dataset Size Frequency Information\\nECL 321 (18317, 2633, 5261) Hourly Electricity\\nETTh1, ETTh2 7 (8545, 2881, 2881) Hourly Electricity\\nETTm1, ETTm2 7 (34465, 11521, 11521) 15min Electricity\\nExchange 8 (5120, 665, 1422) Daily Economy\\nTraffic 862 (12185, 1757, 3509) Hourly Transportation\\nWeather 21 (36792, 5271, 10540) 10min Weather\\nE.4. Anomaly Detection\\nAnomaly detection plays a pivotal role across various domains, enabling the identification of unusual patterns that may\\nindicate critical incidents, such as system failures, security breaches, or environmental changes. In our study, we assess the', metadata={'source': 'rpaper.pdf', 'page': 15}),\n",
       " Document(page_content='indicate critical incidents, such as system failures, security breaches, or environmental changes. In our study, we assess the\\nperformance of our model using five benchmark datasets, each representing a distinct application area, to demonstrate its\\neffectiveness in detecting anomalies in diverse settings:\\n•SMD (Server Machine Dataset): Utilized for server monitoring, the SMD dataset comprises multivariate time series\\ndata collected from servers and aims to identify unusual server behaviors that could indicate failures or security issues.\\n•MSL (Mars Science Laboratory): This dataset contains telemetry data from the Mars Science Laboratory rover,\\nfocusing on space exploration applications. Anomaly detection in this context is crucial for identifying potential issues\\nwith spacecraft systems based on their operational data.\\n•SMAP (Soil Moisture Active Passive): Related to earth observations, the SMAP dataset includes soil moisture', metadata={'source': 'rpaper.pdf', 'page': 15}),\n",
       " Document(page_content='with spacecraft systems based on their operational data.\\n•SMAP (Soil Moisture Active Passive): Related to earth observations, the SMAP dataset includes soil moisture\\nmeasurements intended for environmental monitoring. Detecting anomalies in soil moisture can provide insights into\\nenvironmental conditions and potential agricultural impacts.\\n•SWaT (Secure Water Treatment): In the domain of water treatment security, the SWaT dataset consists of data from\\na water treatment testbed, simulating the operational data of water treatment plants. Anomaly detection here is vital for\\nensuring the safety and security of water treatment processes.\\n•PSM (Pump Sensor Monitoring): Focused on industrial pump sensors, the PSM dataset gathers sensor data from\\npumps in industrial settings. Anomalies in this dataset can indicate equipment malfunctions or the need for maintenance,\\ncritical for preventing industrial accidents.\\nThe detailed characteristics of these datasets is presented in Table 9.', metadata={'source': 'rpaper.pdf', 'page': 15}),\n",
       " Document(page_content='critical for preventing industrial accidents.\\nThe detailed characteristics of these datasets is presented in Table 9.\\nTable 9: Descriptions of the Anomaly detection datasets. Dim shows the variate number of each dataset. Dataset Size\\nindicates the size of the (Train, Validation, Test) split respectively. Frequency denotes the sampling interval of time points.\\nDataset Dim Length Dataset Size Information\\nSMD 38 100 (566724, 141681, 708420) Server Machine\\nMSL 55 100 (44653, 11664, 73729) Spacecraft\\nSMAP 25 100 (108146, 27037, 427617) Spacecraft\\nSWaT 51 100 (396000, 99000, 449919) Infrastructure\\nPSM 25 100 (105984, 26497, 87841) Server Machine\\nF. Full Results\\nF.1. Classification\\n16', metadata={'source': 'rpaper.pdf', 'page': 15}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nDataset TSLANet GPT4TS TimesNet ROCKET CrossF. Pat.TST MLP TS-TCC TS2VEC\\nAdiac 80.56 52.69 24.04 78.52 58.31 34.78 61.38 76.57 72.89\\nArrowHead 80.57 66.29 49.71 77.31 73.71 72.57 75.43 62.20 77.71\\nBeef 90.00 66.67 60.00 67.33 73.33 76.67 73.33 47.32 76.67\\nBeetleFly 90.00 85.00 80.00 88.00 85.00 80.00 80.00 31.25 85.00\\nBirdChicken 100.00 85.00 60.00 84.50 85.00 80.00 75.00 75.00 80.00\\nCBF 97.56 92.00 92.22 89.67 89.56 85.11 83.44 90.79 88.33\\nCar 88.33 76.67 30.00 99.52 86.67 75.00 86.67 71.88 99.22\\nChlorineConcentration 85.94 61.25 55.21 69.40 61.72 56.56 61.72 57.40 71.85\\nCinC ECG torso 85.51 23.99 51.74 84.96 84.93 66.88 46.67 95.55 79.28\\nCoffee 100.00 100.00 53.57 100.00 100.00 100.00 100.00 95.83 100.00\\nComputers 68.40 52.00 62.40 66.48 63.20 69.60 58.00 61.95 60.40\\nCricket X 76.15 6.41 55.64 77.31 41.79 45.38 32.56 77.25 76.15\\nCricket Y 78.72 49.74 55.90 79.15 47.69 43.59 42.31 75.75 73.08', metadata={'source': 'rpaper.pdf', 'page': 16}),\n",
       " Document(page_content='Computers 68.40 52.00 62.40 66.48 63.20 69.60 58.00 61.95 60.40\\nCricket X 76.15 6.41 55.64 77.31 41.79 45.38 32.56 77.25 76.15\\nCricket Y 78.72 49.74 55.90 79.15 47.69 43.59 42.31 75.75 73.08\\nCricket Z 80.00 8.21 57.44 79.33 41.79 47.95 32.56 75.83 76.92\\nDiatomSizeReduction 92.16 88.89 48.69 97.68 95.75 91.18 93.14 95.94 97.71\\nDistalPhalanxOutlineAgeGroup 86.50 86.50 80.25 76.12 80.75 82.00 80.50 85.25 81.25\\nDistalPhalanxOutlineCorrect 80.67 75.67 73.67 75.68 76.17 78.50 75.50 80.76 81.17\\nDistalPhalanxTW 80.50 78.25 77.25 70.07 79.25 79.00 79.50 79.50 78.00\\nEarthquakes 82.30 38.82 23.60 75.32 82.30 80.75 59.94 75.89 72.36\\nECG200 88.00 85.00 90.00 84.90 86.00 89.00 84.00 87.50 87.00\\nECG5000 94.62 93.40 93.47 94.72 94.36 93.87 94.18 94.19 93.33\\nECGFiveDays 99.30 94.77 83.74 100.00 98.49 86.41 96.63 90.71 100.00\\nElectricDevices 68.28 56.36 68.58 66.84 61.87 74.66 48.22 69.31 68.10\\nFaceAll 82.31 37.22 73.61 93.33 90.53 79.94 78.64 76.99 79.17', metadata={'source': 'rpaper.pdf', 'page': 16}),\n",
       " Document(page_content='ECGFiveDays 99.30 94.77 83.74 100.00 98.49 86.41 96.63 90.71 100.00\\nElectricDevices 68.28 56.36 68.58 66.84 61.87 74.66 48.22 69.31 68.10\\nFaceAll 82.31 37.22 73.61 93.33 90.53 79.94 78.64 76.99 79.17\\nFaceFour 94.32 7.95 52.27 77.39 93.18 86.36 82.95 85.42 94.32\\nFacesUCR 92.39 82.88 46.00 94.81 83.07 77.46 74.39 92.93 94.24\\nFiftyWords 80.00 36.48 61.32 76.92 62.86 55.16 58.90 77.62 79.12\\nFISH 94.29 71.43 59.43 96.86 84.57 71.43 87.43 61.29 93.14\\nFordA 93.06 50.49 66.20 90.61 70.62 50.90 51.32 92.35 89.28\\nFordB 91.39 61.99 54.43 77.53 52.70 52.20 51.16 91.72 83.50\\nGun Point 99.33 90.00 87.33 99.33 89.33 94.00 85.33 93.33 98.00\\nHam 80.00 51.43 65.71 69.43 78.10 73.33 77.14 75.00 72.38\\nHandOutlines 88.90 36.20 86.30 94.35 86.00 85.20 86.40 85.81 85.70\\nHaptics 47.73 26.95 37.01 50.84 43.83 41.23 46.10 44.06 43.51\\nHerring 67.19 40.63 59.38 64.38 68.75 64.06 70.31 60.94 64.06\\nInlineSkate 36.73 18.91 25.82 39.64 30.91 29.45 27.82 29.76 38.55', metadata={'source': 'rpaper.pdf', 'page': 16}),\n",
       " Document(page_content='Haptics 47.73 26.95 37.01 50.84 43.83 41.23 46.10 44.06 43.51\\nHerring 67.19 40.63 59.38 64.38 68.75 64.06 70.31 60.94 64.06\\nInlineSkate 36.73 18.91 25.82 39.64 30.91 29.45 27.82 29.76 38.55\\nInsectWingbeatSound 66.36 63.23 60.00 63.92 64.29 57.83 64.75 66.52 63.79\\nItalyPowerDemand 97.08 96.89 97.08 97.17 97.28 96.60 96.89 96.44 95.63\\nLargeKitchenAppliances 81.87 33.33 47.20 81.47 53.87 63.20 42.13 76.08 86.40\\nLighting2 83.61 54.10 72.13 73.61 75.41 75.41 67.21 73.56 86.89\\nLighting7 83.56 53.42 72.60 68.63 72.60 67.12 64.38 81.53 83.56\\nMALLAT 94.71 91.86 54.50 94.12 93.48 84.01 95.05 91.11 89.13\\nMeat 93.33 50.00 33.33 93.33 88.33 91.67 80.00 31.25 91.67\\nMedicalImages 72.76 61.18 58.95 75.42 65.79 63.03 59.61 74.35 75.79\\nMiddlePhalanxOutlineAgeGroup 81.25 74.50 78.75 83.64 80.75 79.75 80.75 78.25 75.25\\nMiddlePhalanxOutlineCorrect 84.00 64.67 64.67 61.36 64.50 64.83 64.50 52.47 71.67\\nMiddlePhalanxTW 65.91 64.91 64.66 53.77 64.66 64.16 65.16 56.10 61.65', metadata={'source': 'rpaper.pdf', 'page': 16}),\n",
       " Document(page_content='MiddlePhalanxOutlineCorrect 84.00 64.67 64.67 61.36 64.50 64.83 64.50 52.47 71.67\\nMiddlePhalanxTW 65.91 64.91 64.66 53.77 64.66 64.16 65.16 56.10 61.65\\nMoteStrain 93.13 87.14 88.34 83.49 87.22 89.54 86.74 85.28 87.86\\nNonInvasiveFatalECG Thorax1 93.44 72.98 81.58 95.65 86.97 78.73 92.98 84.58 90.48\\nNonInvasiveFatalECG Thorax2 93.74 88.04 84.38 95.59 90.53 85.24 93.49 82.50 93.74\\nOliveOil 40.00 40.00 40.00 80.33 60.00 83.33 70.00 42.86 90.00\\nOSULeaf 74.79 9.50 43.39 82.89 49.59 42.15 45.04 63.28 76.86\\nPhalangesOutlinesCorrect 82.40 77.04 68.30 83.11 69.35 65.97 66.90 78.73 80.77\\nPhoneme 27.27 3.22 9.70 20.92 11.23 9.12 8.60 30.04 26.79\\nPlane 100.00 97.14 98.10 100.00 98.10 99.05 97.14 96.43 100.00\\nProximalPhalanxOutlineAgeGroup 88.29 83.90 86.34 90.17 86.34 86.34 85.85 73.34 81.95\\nProximalPhalanxOutlineCorrect 91.75 81.79 77.66 86.59 84.54 78.01 81.79 87.17 87.29\\nProximalPhalanxTW 83.00 81.50 81.75 78.98 80.00 80.25 82.75 72.75 79.00', metadata={'source': 'rpaper.pdf', 'page': 16}),\n",
       " Document(page_content='ProximalPhalanxOutlineCorrect 91.75 81.79 77.66 86.59 84.54 78.01 81.79 87.17 87.29\\nProximalPhalanxTW 83.00 81.50 81.75 78.98 80.00 80.25 82.75 72.75 79.00\\nRefrigerationDevices 55.47 33.60 33.60 50.40 42.40 45.87 38.67 49.74 51.20\\nScreenType 44.80 37.07 44.00 41.55 45.07 44.80 40.27 39.99 40.00\\nShapeletSim 90.00 49.44 50.00 65.72 57.22 56.67 56.67 61.98 87.78\\nShapesAll 85.17 61.17 64.33 86.63 68.17 61.00 61.83 79.11 88.00\\n17', metadata={'source': 'rpaper.pdf', 'page': 16}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nSmallKitchenAppliances 76.27 33.33 45.60 62.13 55.47 61.60 41.33 74.74 71.20\\nSonyAIBORobotSurface 85.86 42.93 70.55 93.16 81.03 83.69 70.38 68.46 89.18\\nSonyAIBORobotSurfaceII 92.44 70.30 85.62 91.26 85.73 85.73 85.10 86.15 90.66\\nStarLightCurves 97.41 92.70 89.22 97.63 92.74 86.09 91.96 96.80 96.28\\nStrawberry 98.37 94.94 93.15 97.84 94.45 93.15 95.76 93.59 96.57\\nSwedishLeaf 96.16 88.32 83.40 96.10 82.08 76.64 80.96 92.31 93.60\\nSymbols 94.07 16.98 86.43 96.71 86.13 82.21 84.72 86.08 96.58\\nSynthetic control 100.00 97.67 99.67 99.53 93.67 99.67 87.00 99.67 99.67\\nToeSegmentation1 87.72 52.63 61.40 94.21 62.28 66.23 60.09 78.75 92.11\\nToeSegmentation2 90.00 75.38 86.15 91.00 81.54 76.92 58.46 59.72 87.69\\nTrace 100.00 68.00 66.00 100.00 74.00 100.00 67.00 97.32 100.00\\nTwoLeadECG 93.85 76.56 68.74 100.00 86.65 84.55 91.48 81.63 99.78\\nTwo Patterns 100.00 99.58 98.00 100.00 79.90 93.23 84.13 100.00 99.21', metadata={'source': 'rpaper.pdf', 'page': 17}),\n",
       " Document(page_content='Trace 100.00 68.00 66.00 100.00 74.00 100.00 67.00 97.32 100.00\\nTwoLeadECG 93.85 76.56 68.74 100.00 86.65 84.55 91.48 81.63 99.78\\nTwo Patterns 100.00 99.58 98.00 100.00 79.90 93.23 84.13 100.00 99.21\\nuWaveGestureLibrary X 82.80 69.65 69.37 82.64 66.78 65.02 64.77 80.97 77.89\\nuWaveGestureLibrary Y 73.53 54.22 62.67 73.83 61.33 55.11 60.19 71.22 67.87\\nuWaveGestureLibrary Z 75.15 59.27 60.44 75.05 59.35 55.05 56.98 72.92 72.67\\nuWaveGestureLibraryAll 97.57 85.54 90.28 97.20 88.02 87.58 88.22 96.54 91.99\\nwafer 99.81 99.58 99.71 99.84 98.39 99.63 94.78 99.69 99.85\\nWine 66.67 53.70 50.00 71.30 68.52 77.78 72.22 57.81 85.19\\nWordsSynonyms 69.28 7.37 50.16 71.30 56.90 49.06 44.83 66.12 68.81\\nWorms 60.77 17.68 43.65 65.97 34.25 34.81 31.49 51.98 55.80\\nWormsTwoClass 77.35 58.01 62.98 76.62 62.43 60.77 58.01 64.48 69.61\\nyoga 85.83 71.83 67.77 90.49 73.87 68.43 65.13 77.46 84.23\\nAverage 83.18 61.58 65.27 81.42 73.47 71.84 69.68 75.07 81.42\\n1st count 38 0 1 27 2 2 2 4 9', metadata={'source': 'rpaper.pdf', 'page': 17}),\n",
       " Document(page_content='yoga 85.83 71.83 67.77 90.49 73.87 68.43 65.13 77.46 84.23\\nAverage 83.18 61.58 65.27 81.42 73.47 71.84 69.68 75.07 81.42\\n1st count 38 0 1 27 2 2 2 4 9\\nTable 10: Full classification results on the UCR datasets in terms of accuracy (as %).\\nTable 11: Full classification results on the UEA datasets in terms of accuracy (as %).\\nDataset TSLANet GPT4TS TimesNet ROCKET CrossF. PatchTST MLP TS-TCC TS2VEC\\nArticularyWordRecognition 99.00 93.33 96.18 99.33 98.00 97.67 97.33 98.00 87.33\\nAtrialFibrillation 40.00 33.33 33.33 20.00 46.66 53.33 46.66 33.33 53.33\\nBasicMotions 100.00 92.50 100.00 100.00 90.00 92.50 85.00 100.00 92.50\\nCricket 98.61 8.33 87.50 98.61 84.72 84.72 91.67 93.06 65.28\\nEpilepsy 98.55 85.51 78.13 98.55 73.19 65.94 60.14 97.10 62.32\\nEthanolConcentration 30.42 25.48 27.73 42.58 34.98 28.90 33.46 32.32 40.68\\nFaceDetection 66.77 65.58 67.47 64.70 66.17 68.96 67.42 63.05 50.96\\nFingerMovements 61.00 57.00 59.38 61.00 64.00 62.00 64.00 44.00 51.00', metadata={'source': 'rpaper.pdf', 'page': 17}),\n",
       " Document(page_content='FaceDetection 66.77 65.58 67.47 64.70 66.17 68.96 67.42 63.05 50.96\\nFingerMovements 61.00 57.00 59.38 61.00 64.00 62.00 64.00 44.00 51.00\\nHandMovementDirection 52.70 18.92 50.00 50.00 58.11 58.11 58.11 64.86 32.43\\nHandwriting 57.88 3.76 26.18 48.47 26.24 26.00 22.47 47.76 15.53\\nHeartbeat 77.56 36.59 74.48 69.76 76.59 76.59 73.17 77.07 69.76\\nInsectWingbeat 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00 10.00\\nJapaneseV owels 99.19 98.11 97.83 95.68 98.92 98.65 97.84 97.30 90.00\\nLibras 92.78 79.44 77.84 83.89 76.11 81.11 73.33 86.67 85.56\\nLSST 66.34 46.39 59.21 54.10 42.82 67.80 35.77 49.23 39.01\\nMotorImagery 62.00 50.00 51.04 53.00 61.00 61.00 61.00 47.00 47.00\\nNATOPS 95.56 91.67 81.82 83.33 88.33 96.67 93.89 96.11 82.22\\nPEMS-SF 83.82 87.28 88.13 75.10 82.08 88.44 82.08 86.71 72.25\\nPenDigits 98.94 97.74 98.19 97.34 93.65 99.23 92.94 98.51 97.40\\nPhonemeSpectra 17.75 3.01 18.24 17.60 7.55 11.69 7.10 25.92 8.23\\nRacketSports 90.79 76.97 82.64 86.18 81.58 84.21 78.95 84.87 74.34', metadata={'source': 'rpaper.pdf', 'page': 17}),\n",
       " Document(page_content='PenDigits 98.94 97.74 98.19 97.34 93.65 99.23 92.94 98.51 97.40\\nPhonemeSpectra 17.75 3.01 18.24 17.60 7.55 11.69 7.10 25.92 8.23\\nRacketSports 90.79 76.97 82.64 86.18 81.58 84.21 78.95 84.87 74.34\\nSelfRegulationSCP1 91.81 91.47 77.43 84.64 92.49 89.76 88.40 91.13 77.13\\nSelfRegulationSCP2 61.67 51.67 52.84 54.44 53.33 54.44 51.67 53.89 51.11\\nSpokenArabicDigits 99.91 99.36 98.36 99.20 96.41 99.68 96.68 99.77 85.27\\nStandWalkJump 46.67 33.33 53.33 46.67 53.33 60.00 60.00 40.00 46.67\\nUWaveGestureLibrary 91.25 84.38 83.13 94.40 81.56 80.00 81.88 86.25 62.81\\nAverage 72.73 58.51 66.55 68.79 66.84 69.13 65.81 69.38 59.62\\n1st count 12 0 0 3 2 7 0 2 0\\n18', metadata={'source': 'rpaper.pdf', 'page': 17}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 12: Full classification results on the human activity recognition and biomedical signal datasets in terms of accuracy\\n(as %).\\nDataset TSLANet GPT4TS TimesNet ROCKET CrossF. PatchTST MLP TS-TCC TS2VEC\\nUCIHAR 96.06 91.24 91.34 94.37 76.59 92.70 63.49 95.95 96.19\\nWISDM 97.77 89.49 89.61 97.03 77.31 95.94 58.88 97.05 93.87\\nHHAR 98.53 97.40 93.59 97.93 78.74 95.96 47.70 98.49 97.05\\nAverage 97.46 92.71 91.51 96.44 77.55 94.87 56.69 97.16 95.70\\nEEG 82.10 76.37 75.86 76.69 53.30 69.69 49.70 86.06 75.13\\nECG 98.37 97.70 98.33 97.72 88.33 98.06 91.57 98.44 97.48\\nAverage 90.24 87.04 87.10 87.20 70.82 83.87 70.63 92.25 86.31\\nF.2. Forecasting\\nTable 13: Full forecasting results on different prediction lengths ∈ {96,192,336,720}. Lower MSE indicates better\\nperformance.\\nMethods TSLANet Time-LLM iTransformer PatchTST Crossformer FEDformer Autoformer RLinear Dlinear TimesNet GPT4TS SCINet', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='performance.\\nMethods TSLANet Time-LLM iTransformer PatchTST Crossformer FEDformer Autoformer RLinear Dlinear TimesNet GPT4TS SCINet\\nMetric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAEECL96 0.136 0.229 0.131 0.224 0.148 0.240 0.138 0.230 0.219 0.314 0.193 0.308 0.201 0.317 0.201 0.281 0.140 0.237 0.168 0.272 0.139 0.238 0.247 0.345\\n192 0.152 0.244 0.152 0.241 0.162 0.253 0.149 0.243 0.231 0.322 0.201 0.315 0.222 0.334 0.201 0.283 0.153 0.249 0.184 0.289 0.153 0.251 0.257 0.355\\n336 0.168 0.262 0.160 0.248 0.178 0.269 0.169 0.262 0.246 0.337 0.214 0.329 0.231 0.338 0.215 0.298 0.169 0.267 0.198 0.300 0.169 0.266 0.269 0.369\\n720 0.205 0.293 0.192 0.298 0.225 0.317 0.211 0.299 0.280 0.363 0.246 0.355 0.254 0.361 0.257 0.331 0.203 0.301 0.220 0.320 0.206 0.297 0.299 0.390', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='720 0.205 0.293 0.192 0.298 0.225 0.317 0.211 0.299 0.280 0.363 0.246 0.355 0.254 0.361 0.257 0.331 0.203 0.301 0.220 0.320 0.206 0.297 0.299 0.390\\nAvg 0.165 0.257 0.158 0.252 0.178 0.270 0.167 0.259 0.244 0.334 0.214 0.327 0.227 0.338 0.219 0.298 0.166 0.264 0.193 0.295 0.167 0.263 0.268 0.365ETTh 196 0.370 0.394 0.362 0.392 0.386 0.405 0.382 0.401 0.423 0.448 0.376 0.419 0.449 0.459 0.386 0.395 0.375 0.399 0.384 0.402 0.376 0.397 0.654 0.599\\n192 0.412 0.417 0.398 0.418 0.441 0.436 0.428 0.425 0.471 0.474 0.420 0.448 0.500 0.482 0.437 0.424 0.405 0.416 0.436 0.429 0.416 0.418 0.719 0.631\\n336 0.399 0.416 0.430 0.427 0.487 0.458 0.451 0.436 0.570 0.546 0.459 0.465 0.521 0.496 0.479 0.446 0.439 0.443 0.491 0.469 0.442 0.433 0.778 0.659\\n720 0.472 0.475 0.442 0.457 0.503 0.491 0.452 0.459 0.653 0.621 0.506 0.507 0.514 0.512 0.481 0.470 0.472 0.490 0.521 0.500 0.477 0.456 0.836 0.699', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='720 0.472 0.475 0.442 0.457 0.503 0.491 0.452 0.459 0.653 0.621 0.506 0.507 0.514 0.512 0.481 0.470 0.472 0.490 0.521 0.500 0.477 0.456 0.836 0.699\\nAvg 0.413 0.426 0.408 0.423 0.454 0.448 0.428 0.430 0.529 0.522 0.440 0.460 0.496 0.487 0.446 0.434 0.423 0.437 0.458 0.450 0.428 0.426 0.747 0.647ETTh 296 0.280 0.341 0.268 0.328 0.297 0.349 0.285 0.340 0.745 0.584 0.358 0.397 0.346 0.388 0.288 0.338 0.289 0.353 0.340 0.374 0.285 0.342 0.707 0.621\\n192 0.330 0.375 0.329 0.375 0.380 0.400 0.356 0.386 0.877 0.656 0.429 0.439 0.456 0.452 0.374 0.390 0.383 0.418 0.402 0.414 0.354 0.389 0.860 0.689\\n336 0.317 0.374 0.368 0.409 0.428 0.432 0.350 0.395 1.043 0.731 0.496 0.487 0.482 0.486 0.415 0.426 0.448 0.465 0.452 0.452 0.373 0.407 1.000 0.744\\n720 0.404 0.440 0.372 0.420 0.427 0.445 0.395 0.427 1.104 0.763 0.463 0.474 0.515 0.511 0.420 0.440 0.605 0.551 0.462 0.468 0.406 0.441 1.249 0.838', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='720 0.404 0.440 0.372 0.420 0.427 0.445 0.395 0.427 1.104 0.763 0.463 0.474 0.515 0.511 0.420 0.440 0.605 0.551 0.462 0.468 0.406 0.441 1.249 0.838\\nAvg 0.333 0.383 0.334 0.383 0.383 0.407 0.347 0.387 0.942 0.684 0.437 0.449 0.450 0.459 0.374 0.399 0.431 0.447 0.414 0.427 0.355 0.395 0.954 0.723ETTm 196 0.289 0.349 0.272 0.334 0.334 0.368 0.291 0.340 0.404 0.426 0.379 0.419 0.505 0.475 0.355 0.376 0.299 0.343 0.338 0.375 0.292 0.346 0.418 0.438\\n192 0.328 0.370 0.310 0.358 0.377 0.391 0.328 0.365 0.450 0.451 0.426 0.441 0.553 0.496 0.391 0.392 0.335 0.365 0.374 0.387 0.332 0.372 0.439 0.450\\n336 0.355 0.389 0.352 0.384 0.426 0.420 0.365 0.389 0.532 0.515 0.445 0.459 0.621 0.537 0.424 0.415 0.369 0.386 0.410 0.411 0.366 0.394 0.490 0.485\\n720 0.421 0.425 0.383 0.411 0.491 0.459 0.422 0.423 0.666 0.589 0.543 0.490 0.671 0.561 0.487 0.450 0.425 0.421 0.478 0.450 0.417 0.421 0.595 0.550', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='720 0.421 0.425 0.383 0.411 0.491 0.459 0.422 0.423 0.666 0.589 0.543 0.490 0.671 0.561 0.487 0.450 0.425 0.421 0.478 0.450 0.417 0.421 0.595 0.550\\nAvg 0.348 0.383 0.329 0.372 0.407 0.410 0.352 0.379 0.513 0.495 0.448 0.452 0.588 0.517 0.414 0.408 0.357 0.379 0.400 0.406 0.352 0.383 0.486 0.481ETTm 296 0.169 0.259 0.161 0.253 0.180 0.264 0.169 0.254 0.287 0.366 0.203 0.287 0.255 0.339 0.182 0.265 0.167 0.260 0.187 0.267 0.173 0.262 0.286 0.377\\n192 0.224 0.297 0.219 0.293 0.250 0.309 0.230 0.294 0.414 0.492 0.269 0.328 0.281 0.340 0.246 0.304 0.224 0.303 0.249 0.309 0.229 0.301 0.399 0.445\\n336 0.275 0.329 0.271 0.329 0.311 0.348 0.280 0.329 0.597 0.542 0.325 0.366 0.339 0.372 0.307 0.342 0.281 0.342 0.321 0.351 0.286 0.341 0.637 0.591\\n720 0.354 0.380 0.352 0.379 0.412 0.407 0.378 0.386 1.730 1.042 0.421 0.415 0.433 0.432 0.407 0.398 0.397 0.421 0.408 0.403 0.378 0.401 0.960 0.735', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='720 0.354 0.380 0.352 0.379 0.412 0.407 0.378 0.386 1.730 1.042 0.421 0.415 0.433 0.432 0.407 0.398 0.397 0.421 0.408 0.403 0.378 0.401 0.960 0.735\\nAvg 0.256 0.316 0.251 0.313 0.288 0.332 0.264 0.316 0.757 0.611 0.305 0.349 0.327 0.371 0.286 0.327 0.267 0.332 0.291 0.333 0.267 0.326 0.571 0.537Exchange96 0.083 0.201 - - 0.086 0.206 0.088 0.205 0.256 0.367 0.148 0.278 0.197 0.323 0.093 0.217 0.081 0.203 0.107 0.234 0.082 0.199 0.267 0.396\\n192 0.177 0.299 - - 0.177 0.299 0.176 0.299 0.470 0.509 0.271 0.315 0.300 0.369 0.184 0.307 0.157 0.293 0.226 0.344 0.171 0.293 0.351 0.459\\n336 0.331 0.417 - - 0.331 0.417 0.301 0.397 1.268 0.883 0.460 0.427 0.509 0.524 0.351 0.432 0.305 0.414 0.367 0.448 0.354 0.428 1.324 0.853\\n720 0.888 0.739 - - 0.847 0.691 0.901 0.714 1.767 1.068 1.195 0.695 1.447 0.941 0.886 0.714 0.643 0.601 0.964 0.746 0.877 0.704 1.058 0.797', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='720 0.888 0.739 - - 0.847 0.691 0.901 0.714 1.767 1.068 1.195 0.695 1.447 0.941 0.886 0.714 0.643 0.601 0.964 0.746 0.877 0.704 1.058 0.797\\nAvg 0.370 0.414 - - 0.360 0.403 0.367 0.404 0.940 0.707 0.519 0.429 0.613 0.539 0.379 0.418 0.297 0.378 0.416 0.443 0.371 0.406 0.750 0.626Traffic96 0.372 0.261 0.362 0.248 0.395 0.268 0.401 0.267 0.522 0.290 0.587 0.366 0.613 0.388 0.649 0.389 0.410 0.282 0.593 0.321 0.388 0.282 0.788 0.499\\n192 0.388 0.266 0.374 0.247 0.417 0.276 0.406 0.268 0.530 0.293 0.604 0.373 0.616 0.382 0.601 0.366 0.423 0.287 0.617 0.336 0.407 0.290 0.789 0.505\\n336 0.394 0.269 0.385 0.271 0.433 0.283 0.421 0.277 0.558 0.305 0.621 0.383 0.622 0.337 0.609 0.369 0.436 0.296 0.629 0.336 0.412 0.294 0.797 0.508\\n720 0.430 0.289 0.43 0.288 0.467 0.302 0.452 0.297 0.589 0.328 0.626 0.382 0.660 0.408 0.647 0.387 0.466 0.315 0.640 0.350 0.450 0.312 0.841 0.523', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='720 0.430 0.289 0.43 0.288 0.467 0.302 0.452 0.297 0.589 0.328 0.626 0.382 0.660 0.408 0.647 0.387 0.466 0.315 0.640 0.350 0.450 0.312 0.841 0.523\\nAvg 0.396 0.271 0.388 0.264 0.428 0.282 0.420 0.277 0.550 0.304 0.610 0.376 0.628 0.379 0.627 0.378 0.434 0.295 0.620 0.336 0.414 0.295 0.804 0.509Weather96 0.148 0.197 0.147 0.201 0.174 0.214 0.160 0.204 0.158 0.230 0.217 0.296 0.266 0.336 0.192 0.232 0.176 0.237 0.172 0.220 0.162 0.212 0.221 0.306\\n192 0.193 0.241 0.189 0.234 0.221 0.254 0.204 0.245 0.206 0.277 0.276 0.336 0.307 0.367 0.240 0.271 0.220 0.282 0.219 0.261 0.204 0.248 0.261 0.340\\n336 0.245 0.282 0.262 0.279 0.278 0.296 0.257 0.285 0.272 0.335 0.339 0.380 0.359 0.395 0.292 0.307 0.265 0.319 0.280 0.306 0.254 0.286 0.309 0.378\\n720 0.325 0.337 0.304 0.316 0.358 0.349 0.329 0.338 0.398 0.418 0.403 0.428 0.419 0.428 0.364 0.353 0.323 0.362 0.365 0.359 0.326 0.337 0.377 0.427', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='720 0.325 0.337 0.304 0.316 0.358 0.349 0.329 0.338 0.398 0.418 0.403 0.428 0.419 0.428 0.364 0.353 0.323 0.362 0.365 0.359 0.326 0.337 0.377 0.427\\nAvg 0.228 0.264 0.225 0.257 0.258 0.278 0.238 0.268 0.259 0.315 0.309 0.360 0.338 0.382 0.272 0.291 0.246 0.300 0.259 0.287 0.237 0.271 0.292 0.363\\n19', metadata={'source': 'rpaper.pdf', 'page': 18}),\n",
       " Document(page_content='TSLANet: Rethinking Transformers for Time Series Representation Learning\\nTable 14: Full results for the anomaly detection.\\nMethods SMD MSL SMAP SWaT PSM Avg F1\\nMetrics P R F1 P R F1 P R F1 P R F1 P R F1 %\\nTSLANet (Ours) 85.58 90.37 87.91 77.46 90.12 83.32 92.45 64.47 75.96 91.50 94.14 92.80 98.36 98.55 97.73 87.54\\nGPT4TS 88.89 84.98 86.89 82.00 82.91 82.45 90.60 60.95 72.88 92.20 96.34 94.23 98.62 95.68 97.13 86.72\\nTimesNet 87.91 81.54 84.61 89.54 75.36 81.84 90.14 56.40 69.39 90.75 95.40 93.02 98.51 96.20 97.34 85.24\\nPatchTST 87.26 82.14 84.62 88.34 70.96 78.70 90.64 55.46 68.82 91.10 80.94 85.72 98.84 93.47 96.08 82.79\\nETSformer 87.44 79.23 83.13 85.13 84.93 85.03 92.25 55.75 69.50 90.02 80.36 84.91 99.31 85.28 91.76 82.87\\nFEDformer 87.95 82.39 85.08 77.14 80.07 78.57 90.47 58.10 70.76 90.17 96.42 93.19 97.31 97.16 97.23 84.97\\nLightTS 87.10 78.42 82.53 82.40 75.78 78.95 92.58 55.27 69.21 91.98 94.72 93.33 98.37 95.97 97.15 84.23', metadata={'source': 'rpaper.pdf', 'page': 19}),\n",
       " Document(page_content='LightTS 87.10 78.42 82.53 82.40 75.78 78.95 92.58 55.27 69.21 91.98 94.72 93.33 98.37 95.97 97.15 84.23\\nDLinear 83.62 71.52 77.10 84.34 85.42 84.88 92.32 55.41 69.26 80.91 95.30 87.52 98.28 89.26 93.55 82.46\\nStationary 88.33 81.21 84.62 68.55 89.14 77.50 89.37 59.02 71.09 68.03 96.75 79.88 97.82 96.76 97.29 82.08\\nAutoformer 88.06 82.35 85.11 77.27 80.92 79.05 90.40 58.62 71.12 89.85 95.81 92.74 99.08 88.15 93.29 84.26\\nPyraformer 85.61 80.61 83.04 83.81 85.93 84.86 92.34 57.71 71.09 87.92 96.00 91.78 71.67 96.02 82.08 82.57\\nAnomaly Transformer 88.91 82.23 85.49 79.61 87.37 83.31 91.85 58.11 71.18 72.51 97.32 83.10 68.35 94.72 79.40 80.50\\nInformer 86.60 77.23 81.65 81.77 86.48 84.06 90.11 57.13 69.92 70.29 96.75 81.43 64.27 96.33 77.10 78.83\\nReformer 82.58 69.24 75.32 85.51 83.31 84.40 90.91 57.44 70.40 72.50 96.53 82.80 59.93 95.38 73.61 77.31\\nLogTransformer 83.46 70.13 76.21 73.05 87.37 79.57 89.15 57.59 69.97 68.67 97.31 80.52 63.06 98.00 76.74 76.60', metadata={'source': 'rpaper.pdf', 'page': 19}),\n",
       " Document(page_content='LogTransformer 83.46 70.13 76.21 73.05 87.37 79.57 89.15 57.59 69.97 68.67 97.31 80.52 63.06 98.00 76.74 76.60\\nTransformer 83.58 76.13 79.56 71.57 87.37 78.68 89.37 57.12 69.70 68.84 96.53 80.37 62.75 96.56 76.07 76.88\\nF.3. Anomaly Detection\\nG. Future Work\\nTSLANet is aimed to be a foundation model for time series analysis. Therefore, we have some future directions toward\\nachieving this goal. These are summarized as follows.\\nLarge-Scale Pretraining We aim to explore the potential of TSLANet when pretrained on a diverse and large cohort\\nof datasets. This would enable us to assess the model’s generalization capabilities and its performance on few-shot and\\nzero-shot learning tasks. In addition, this would give our model an advantage in competing against LLM-pretrained models\\nin time series analysis.\\nBetter Pretraining Task We aim to develop other pretraining tasks beyond the current masking approach, which, while', metadata={'source': 'rpaper.pdf', 'page': 19}),\n",
       " Document(page_content='in time series analysis.\\nBetter Pretraining Task We aim to develop other pretraining tasks beyond the current masking approach, which, while\\nstraightforward and effective for initial learning, presents limitations in fully capturing the complexity of time series data.\\nMasking may not adequately challenge the model to learn the intricate temporal dependencies and patterns essential for\\nadvanced classification and forecasting. This exploration will contribute to evolving TSLANet into a more refined and\\ncapable foundation model for time series analysis.\\nEnhanced Noise Reduction Techniques Building upon the adaptive spectral filtering capabilities of TSLANet, future\\nwork could explore more sophisticated noise reduction techniques that can adapt to a wider variety of noise patterns and\\ndistributions, as well as be adept to the quick fluctuations in short-term forecasting problems.\\n20', metadata={'source': 'rpaper.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Janith\\Desktop\\LLM-demo\\venvllmdemo\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Vector Embedding and Vector Store\n",
    "\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(documents[:10], OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'term and short-term interactions while mitigat-\\ning noise via adaptive thresholding. Additionally,\\nwe introduce an Interactive Convolution Block\\nand leverage self-supervised learning to refine\\nthe capacity of TSLANet for decoding complex\\ntemporal patterns and improve its robustness on\\ndifferent datasets. Our comprehensive experi-\\nments demonstrate that TSLANet outperforms\\nstate-of-the-art models in various tasks spanning\\nclassification, forecasting, and anomaly detection,\\nshowcasing its resilience and adaptability across\\na spectrum of noise levels and data sizes. The\\ncode is available at https://github.com/\\nemadeldeen24/TSLANet\\n1. Introduction\\nTime series data, known for its sequential nature and tempo-\\nral dependencies, is ubiquitous across numerous domains,\\nincluding finance, healthcare, and environmental monitor-\\ning. Recently, the Transformer model (Vaswani et al., 2017),\\noriginally renowned for its breakthroughs in natural lan-'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector databases\n",
    "\n",
    "query = \"What is Time series data\"\n",
    "\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faiss vector db\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db1 = FAISS.from_documents(documents[:5], OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'term and short-term interactions while mitigat-\\ning noise via adaptive thresholding. Additionally,\\nwe introduce an Interactive Convolution Block\\nand leverage self-supervised learning to refine\\nthe capacity of TSLANet for decoding complex\\ntemporal patterns and improve its robustness on\\ndifferent datasets. Our comprehensive experi-\\nments demonstrate that TSLANet outperforms\\nstate-of-the-art models in various tasks spanning\\nclassification, forecasting, and anomaly detection,\\nshowcasing its resilience and adaptability across\\na spectrum of noise levels and data sizes. The\\ncode is available at https://github.com/\\nemadeldeen24/TSLANet\\n1. Introduction\\nTime series data, known for its sequential nature and tempo-\\nral dependencies, is ubiquitous across numerous domains,\\nincluding finance, healthcare, and environmental monitor-\\ning. Recently, the Transformer model (Vaswani et al., 2017),\\noriginally renowned for its breakthroughs in natural lan-'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Time series data\"\n",
    "\n",
    "result = db1.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model= \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "user_prompt =  \"\"\" \n",
    "    Answer the following question based only on the provided context.\n",
    "    Think step by step before providing a detailed answer.\n",
    "    <context> {context} </context>\n",
    "    Question: {input}\n",
    "    \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Please response to the user queries.\"),\n",
    "    (\"user\",user_prompt)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001E5919FC1C0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"What are the datasets used in this research paper?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The datasets used in this research paper are referred to as the UEA datasets. They were utilized for classification tasks and the forecasting Mean Squared Error (MSE) results were averaged over different lengths, specifically {96, 192, 336, 720}.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
